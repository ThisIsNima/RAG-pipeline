

----- Page 1 (native) -----
Machine Learning, 29, 131–163 (1997)
c⃝1997 Kluwer Academic Publishers. Manufactured in The Netherlands.
Bayesian Network Classiﬁers*
NIR FRIEDMAN
nir@cs.berkeley.edu
Computer Science Division, 387 Soda Hall, University of California, Berkeley, CA 94720
DAN GEIGER
dang@cs.technion.ac.il
Computer Science Department, Technion, Haifa, Israel, 32000
MOISES GOLDSZMIDT
moises@erg.sri.com
SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025
Editor: G. Provan, P. Langley, and P. Smyth
Abstract. Recent work in supervised learning has shown that a surprisingly simple Bayesian classiﬁer with strong
assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classiﬁers
such as C4.5. This fact raises the question of whether a classiﬁer with less restrictive assumptions can perform
even better. In this paper we evaluate approaches for inducing classiﬁers from data, based on the theory of learning
Bayesian networks. These networks are factored representations of probability distributions that generalize the
naive Bayesian classiﬁer and explicitly represent statements about independence. Among these approaches we
single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same
time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes.
We experimentally tested these approaches, using problems from the University of California at Irvine repository,
and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.
Keywords: Bayesian networks, classiﬁcation
1.
Introduction
Classiﬁcation is a basic task in data analysis and pattern recognition that requires the
construction of a classiﬁer, that is, a function that assigns a class label to instances described
by a set of attributes. The induction of classiﬁers from data sets of preclassiﬁed instances
is a central problem in machine learning. Numerous approaches to this problem are based
on various functional representations such as decision trees, decision lists, neural networks,
decision graphs, and rules.
One of the most effective classiﬁers, in the sense that its predictive performance is com-
petitive with state-of-the-art classiﬁers, is the so-called naive Bayesian classiﬁer described,
for example, by Duda and Hart (1973) and by Langley et al. (1992). This classiﬁer learns
from training data the conditional probability of each attribute Ai given the class label C.
Classiﬁcation is then done by applying Bayes rule to compute the probability of C given
the particular instance of A1, . . . , An, and then predicting the class with the highest poste-
rior probability. This computation is rendered feasible by making a strong independence
assumption: all the attributes Ai are conditionally independent given the value of the class
C. By independence we mean probabilistic independence, that is, A is independent of B
*
This paper is an extended version of Geiger (1992) and Friedman and Goldszmidt (1996a).

----- Page 2 (native) -----
132
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
C
A
A
A
2
n
1
Figure 1. The structure of the naive Bayes network.
given C whenever Pr(A|B, C) = Pr(A|C) for all possible values of A, B and C, whenever
Pr(C) > 0.
The performance of naive Bayes is somewhat surprising, since the above assumption is
clearly unrealistic. Consider, for example, a classiﬁer for assessing the risk in loan appli-
cations: it seems counterintuitive to ignore the correlations between age, education level,
and income. This example raises the following question: can we improve the performance
of naive Bayesian classiﬁers by avoiding unwarranted (by the data) assumptions about
independence?
In order to tackle this problem effectively, we need an appropriate language and efﬁcient
machinery to represent and manipulate independence assertions. Both are provided by
Bayesian networks (Pearl, 1988). These networks are directed acyclic graphs that allow
efﬁcient and effective representation of the joint probability distribution over a set of random
variables. Each vertex in the graph represents a random variable, and edges represent direct
correlations between the variables. More precisely, the network encodes the following
conditional independence statements: each variable is independent of its nondescendants
in the graph given the state of its parents. These independencies are then exploited to reduce
the number of parameters needed to characterize a probability distribution, and to efﬁciently
compute posterior probabilities given evidence. Probabilistic parameters are encoded in
a set of tables, one for each variable, in the form of local conditional distributions of a
variable given its parents. Using the independence statements encoded in the network, the
joint distribution is uniquely determined by these local conditional distributions.
When represented as a Bayesian network, a naive Bayesian classiﬁer has the simple
structure depicted in Figure 1. This network captures the main assumption behind the naive
Bayesian classiﬁer, namely, that every attribute (every leaf in the network) is independent
from the rest of the attributes, given the state of the class variable (the root in the network).
Since we have the means to represent and manipulate independence assertions, the obvi-
ous question follows: can we induce better classiﬁers by learning unrestricted Bayesian
networks?
Learning Bayesian networks from data is a rapidly growing ﬁeld of research that has seen
a great deal of activity in recent years, including work by Buntine (1991, 1996), Cooper and
Herskovits (1992), Friedman and Goldszmidt (1996c), Lam and Bacchus (1994), Hecker-
man (1995), and Heckerman, Geiger, and Chickering (1995). This is a form of unsupervised
learning, in the sense that the learner does not distinguish the class variable from the attribute

----- Page 3 (native) -----
BAYESIAN NETWORK CLASSIFIERS
133
variables in the data. The objective is to induce a network (or a set of networks) that “best
describes” the probability distribution over the training data. This optimization process is
implemented in practice by using heuristic search techniques to ﬁnd the best candidate over
the space of possible networks. The search process relies on a scoring function that assesses
the merits of each candidate network.
We start by examining a straightforward application of current Bayesian networks tech-
niques. We learn networks using the score based on the minimum description length (MDL)
principle (Lam & Bacchus, 1994; Suzuki, 1993), and use them for classiﬁcation. The re-
sults, which are analyzed in Section 3, are mixed: although the learned networks perform
signiﬁcantly better than naive Bayes on some data sets, they perform worse on others. We
trace the reasons for these results to the deﬁnition of the MDL scoring function. Roughly
speaking, the problem is that the MDL score measures the error of the learned Bayesian
network over all the variables in the domain. Minimizing this error, however, does not
necessarily minimize the local error in predicting the class variable given the attributes. We
argue that similar problems will occur with other scoring functions in the literature.
Accordingly, we limit our attention to a class of network structures that are based on the
structure of naive Bayes, requiring that the class variable be a parent of every attribute.
This ensures that, in the learned network, the probability Pr(C|A1, . . . , An), the main
term determining the classiﬁcation, will take every attribute into account. Unlike the naive
Bayesian classiﬁer, however, our classiﬁer allows additional edges between attributes that
capture correlations among them. This extension incurs additional computational costs.
While the induction of the naive Bayesian classiﬁer requires only simple bookkeeping, the
induction of Bayesian networks requires searching the space of all possible networks—
that is, the space of all possible combinations of edges.
To address this problem, we
examine a restricted form of correlation edges. The resulting method, which we call Tree
Augmented Naive Bayes (TAN), approximates the interactions between attributes by using
a tree structure imposed on the naive Bayesian structure. As we show, this approximation
is optimal, in a precise sense; moreover, we can learn TAN classiﬁers in polynomial time.
This result extends a well-known result by Chow and Liu (1968) (see also Pearl (1988))
for learning tree-structured Bayesian networks. Finally, we also examine a generalization
of these models based on the idea that correlations among attributes may vary according
to the speciﬁc instance of the class variable. Thus, instead of one TAN model we have
a collection of networks as the classiﬁer. Interestingly enough, Chow and Liu already
investigated classiﬁers of this type for recognition of handwritten characters.
After describing these methods, we report the results of an empirical evaluation comparing
them with state-of-the-art machine learning methods. Our experiments show that TAN
maintains the robustness and computational complexity of naive Bayes, and at the same
time displays better accuracy. We compared TAN with C4.5, naive Bayes, and selective
naive Bayes (a wrapper approach to feature subset selection method combined with naive
Bayes), on a set of problems from the University of California at Irvine (UCI) repository
(see Section 4.1). These experiments show that TAN is a signiﬁcant improvement over
the other three approaches. We obtained similar results with a modiﬁed version of Chow’s
and Liu’s original method, which eliminates errors due to variance in the parameters. It is

----- Page 4 (native) -----
134
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
interesting that this method, originally proposed three decades ago, is still competitive with
state-of-the-art methods developed since then by the machine learning community.
This paper is organized as follows. In Section 2 we review Bayesian networks and how
to learn them. In Section 3 we examine a straightforward application of learning Bayesian
networks for classiﬁcation, and show why this approach might yield classiﬁers that exhibit
poor performance. In Section 4 we examine how to address this problem using extensions
to the naive Bayesian classiﬁer. In Section 5 we describe in detail the experimental setup
and results. In Section 6 we discuss related work and alternative solutions to the problems
we point out in previous sections. We conclude in Section 7. Finally, Appendix A reviews
several concepts from information theory that are relevant to the contents of this paper.
2.
Learning Bayesian networks
Consider a ﬁnite set U = {X1, . . . , Xn} of discrete random variables where each variable
Xi may take on values from a ﬁnite set, denoted by Val(Xi). We use capital letters such
as X, Y, Z for variable names, and lower-case letters such as x, y, z to denote speciﬁc
values taken by those variables. Sets of variables are denoted by boldface capital letters
such as X, Y, Z, and assignments of values to the variables in these sets are denoted by
boldface lowercase letters x, y, z (we use Val(X) in the obvious way). Finally, let P be a
joint probability distribution over the variables in U, and let X, Y, Z be subsets of U. We
say that X and Y are conditionally independent given Z, if for all x ∈Val(X), y ∈Val(Y),
z ∈Val(Z), P(x | z, y) = P(x | z) whenever P(y, z) > 0.
A Bayesian network is an annotated directed acyclic graph that encodes a joint probability
distribution over a set of random variables U. Formally, a Bayesian network for U is a pair
B = ⟨G, Θ⟩. The ﬁrst component, G, is a directed acyclic graph whose vertices corre-
spond to the random variables X1, . . . , Xn, and whose edges represent direct dependencies
between the variables. The graph G encodes independence assumptions: each variable Xi
is independent of its nondescendants given its parents in G. The second component of the
pair, namely Θ, represents the set of parameters that quantiﬁes the network. It contains a
parameter θxi|Πxi = PB(xi|Πxi) for each possible value xi of Xi, and Πxi of ΠXi, where
ΠXi denotes the set of parents of Xi in G. A Bayesian network B deﬁnes a unique joint
probability distribution over U given by
PB(X1, . . . , Xn) =
n
Y
i=1
PB(Xi|ΠXi) =
n
Y
i=1
θXi|ΠXi .
(1)
We note that one may associate a notion of minimality with the deﬁnition of a Bayesian
network, as done by Pearl (1988), yet this association is irrelevant to the material in this
paper.
As an example, let U∗= {A1, . . . , An, C}, where the variables A1, . . . , An are the
attributes and C is the class variable. Consider a graph structure where the class variable
is the root, that is, ΠC = ∅, and each attribute has the class variable as its unique parent,
namely, ΠAi = {C} for all 1 ≤i ≤n. This is the structure depicted in Figure 1. For this
type of graph structure, Equation 1 yields Pr(A1, . . . , An, C) = Pr(C) · Qn
i=1 Pr(Ai|C).

----- Page 5 (native) -----
BAYESIAN NETWORK CLASSIFIERS
135
From the deﬁnition of conditional probability, we get Pr(C|A1, . . . , An) = α · Pr(C) ·
Qn
i=1 Pr(Ai|C), where α is a normalization constant. This is in fact the deﬁnition of naive
Bayes commonly found in the literature (Langley et al., 1992).
The problem of learning a Bayesian network can be informally stated as: Given a training
set D = {u1, . . . , uN} of instances of U, ﬁnd a network B that best matches D. The
common approach to this problem is to introduce a scoring function that evaluates each
network with respect to the training data, and then to search for the best network according to
this function. In general, this optimization problem is intractable (Chickering, 1995). Yet,
for certain restricted classes of networks, there are efﬁcient algorithms requiring polynomial
time in the number of variables in the network. We indeed take advantage of these efﬁcient
algorithms in Section 4.1, where we propose a particular extension to naive Bayes.
The two main scoring functions commonly used to learn Bayesian networks are the
Bayesian scoring function (Cooper & Herskovits, 1992; Heckerman et al., 1995), and the
function based on the principle of minimal description length (MDL) (Lam & Bacchus,
1994; Suzuki, 1993); see also Friedman and Goldszmidt (1996c) for a more recent account
of this scoring function. These scoring functions are asymptotically equivalent as the sample
size increases; furthermore, they are both asymptotically correct: with probability equal
to one the learned distribution converges to the underlying distribution as the number of
samples increases (Heckerman, 1995; Bouckaert, 1994; Geiger et al., 1996). An in-depth
discussion of the pros and cons of each scoring function is beyond the scope of this paper.
Henceforth, we concentrate on the MDL scoring function.
TheMDLprinciple(Rissanen, 1978)castslearningintermsofdatacompression. Roughly
speaking, the goal of the learner is to ﬁnd a model that facilitates the shortest description
of the original data. The length of this description takes into account the description of
the model itself and the description of the data using the model. In the context of learning
Bayesian networks, the model is a network. Such a network B describes a probability dis-
tribution PB over the instances appearing in the data. Using this distribution, we can build
an encoding scheme that assigns shorter code words to more probable instances. According
to the MDL principle, we should choose a network B such that the combined length of the
network description and the encoded data (with respect to PB) is minimized.
Let B = ⟨G, Θ⟩be a Bayesian network, and let D = {u1, . . . , uN} be a training set,
where each ui assigns a value to all the variables in U. The MDL scoring function of a
network B given a training data set D, written MDL(B|D), is given by
MDL(B|D) = log N
2
|B| −LL(B|D) ,
(2)
where |B| is the number of parameters in the network. The ﬁrst term represents the length
of describing the network B, in that, it counts the bits needed to encode the speciﬁc network
B, where 1/2·log N bits are used for each parameter in Θ. The second term is the negation
of the log likelihood of B given D:
LL(B|D) =
N
X
i=1
log(PB(ui)) ,
(3)

----- Page 6 (native) -----
136
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
whichmeasureshowmanybitsareneededtodescribeD basedontheprobabilitydistribution
PB (see Appendix A). The log likelihood also has a statistical interpretation: the higher
the log likelihood, the closer B is to modeling the probability distribution in the data D.
Let ˆPD(·) be the empirical distribution deﬁned by frequencies of events in D, namely,
ˆPD(A) =
1
N
P
j 1A(uj) for each event A ⊆Val(U), where 1A(u) = 1 if u ∈A and
1A(u) = 0 if u ̸∈A. Applying Equation 1 to the log likelihood and changing the order
of summation yields the well-known decomposition of the log likelihood according to the
structure of B:
LL(B|D) = N
N
X
i=1
X
xi ∈Val(Xi)
Πxi ∈Val(ΠXi)
ˆPD(xi, Πxi) log(θxi|Πxi ) .
(4)
It is easy to show that this expression is maximized when
θxi|Πxi = ˆPD(xi|Πxi) .
(5)
Consequently, if we have two Bayesian networks, B = ⟨G, Θ⟩and B′ = ⟨G, Θ′⟩, that
share the same structure G, and if Θ satisﬁes Equation 5, then LL(B|D) ≥LL(B′|D).
Thus, given a network structure, there is a closed form solution for the parameters that
maximize the log likelihood score, namely, Equation 5. Moreover, since the ﬁrst term of
Equation 2 does not depend on the choice of parameters, this solution minimizes the MDL
score. This is a crucial observation since it relieves us of searching in the space of Bayesian
networks, and lets us search only in the smaller space of network structures, and then ﬁll in
the parameters by computing the appropriate frequencies from the data. Henceforth, unless
we state otherwise, we will assume that the choice of parameters satisﬁes Equation 5.
The log likelihood score by itself is not suitable for learning the structure of the network,
since it tends to favor complete graph structures in which every variable is connected to every
other variable. This is highly undesirable, since such networks do not provide any useful
representation of the independence assertions in the learned distributions. Moreover, these
networks require an exponential number of parameters, most of which will have extremely
high variance and will lead to poor predictions. Thus, the learned parameters in a maximal
network will perfectly match the training data, but will have poor performance on test data.
This problem, called overﬁtting, is avoided by the MDL score. The ﬁrst term of the MDL
score (Equation 2), regulates the complexity of networks by penalizing those that contain
many parameters. Thus, the MDL score of a larger network might be worse (larger) than
that of a smaller network, even though the former might match the data better. In practice,
the MDL score regulates the number of parameters learned and helps avoid overﬁtting of
the training data.
We stress that the MDL score is asymptotically correct. Given a sufﬁcient number of
independentsamples, thebestMDL-scoringnetworkswillbearbitrarilyclosetothesampled
distribution.
Regarding the search process, in this paper we will rely on a greedy strategy for the obvious
computational reasons. This procedure starts with the empty network and successively

----- Page 7 (native) -----
BAYESIAN NETWORK CLASSIFIERS
137
applies local operations that maximally improve the score until a local minima is found.
The operations applied by the search procedure include arc addition, arc deletion, and arc
reversal.
3.
Bayesian networks as classiﬁers
Using the method just described, one can induce a Bayesian network B, that encodes a
distribution PB(A1, . . . , An, C), from a given training set. We can then use the resulting
model so that given a set of attributes a1, . . . , an, the classiﬁer based on B returns the
label c that maximizes the posterior probability PB(c|a1, . . . , an). Note that, by inducing
classiﬁers in this manner, we are addressing the main concern expressed in the introduction:
we remove the bias introduced by the independence assumptions embedded in the naive
Bayesian classiﬁer.
This approach is justiﬁed by the asymptotic correctness of the Bayesian learning pro-
cedure. Given a large data set, the learned network will be a close approximation for the
probability distribution governing the domain (assuming that instances are sampled inde-
pendently from a ﬁxed distribution). Although this argument provides us with a sound
theoretical basis, in practice we may encounter cases where the learning process returns a
network with a relatively good MDL score that performs poorly as a classiﬁer.
To understand the possible discrepancy between good predictive accuracy and good MDL
score, we must re-examine the MDL score. Recall that the log likelihood term in Equation 2
istheonethatmeasuresthequalityofthelearnedmodel, andthatD = {u1, . . . , uN}denotes
the training set. In a classiﬁcation task, each ui is a tuple of the form ⟨ai
1, . . . , ai
n, ci⟩that
assigns values to the attributes A1, . . . , An and to the class variable C. We can rewrite the
log likelihood function (Equation 3) as
LL(B|D) =
N
X
i=1
log PB(ci|ai
1, . . . , ai
n) +
N
X
i=1
log PB(ai
1, . . . , ai
n) .
(6)
The ﬁrst term in this equation measures how well B estimates the probability of the class
given the attributes. The second term measures how well B estimates the joint distribution
of the attributes. Since the classiﬁcation is determined by PB(C|A1, . . . , An), only the
ﬁrst term is related to the score of the network as a classiﬁer (i.e., its predictive accuracy).
Unfortunately, this term is dominated by the second term when there are many attributes;
as n grows larger, the probability of each particular assignment to A1, . . . , An becomes
smaller, since the number of possible assignments grows exponentially in n. Thus, we ex-
pect the terms of the form PB(A1, . . . , An) to yield values closer to zero, and consequently
−log PB(A1, . . . , An) will grow larger. However, at the same time, the conditional prob-
ability of the class will remain more or less the same. This implies that a relatively large
error in the conditional term in Equation 6 may not be reﬂected in the MDL score. Thus,
using MDL (or other nonspecialized scoring functions) for learning unrestricted Bayesian
networks may result in a poor classiﬁer in cases where there are many attributes. We use the
phrase “unrestricted networks” in the sense that the structure of the graph is not constrained,
as in the case of a naive Bayesian classiﬁer.

----- Page 8 (native) -----
138
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
0
5
10
15
20
25
30
35
40
45
22 19 10 25 16 11 9 4 6 18 17 2 13 1 15 14 12 21 7 20 23 8 24 3 5
Percentage Classification Error
Data Set
Bayesian Network
Naive Bayes
 0
 5
10
15
20
25
30
35
40
45
 0
 5
10
15
20
25
30
35
40
45
Naive Bayes Error
Bayesian Network Error
Figure 2. Error curves (top) and scatter plot (bottom) comparing unsupervised Bayesian networks (solid line,
x axis) to naive Bayes (dashed line, y axis). In the error curves, the horizontal axis lists the data sets, which
are sorted so that the curves cross only once, and the vertical axis measures the percentage of test instances that
were misclassiﬁed (i.e., prediction errors). Thus, the smaller the value, the better the accuracy. Each data point
is annotated by a 90% conﬁdence interval. In the scatter plot, each point represents a data set, where the x
coordinate of a point is the percentage of misclassiﬁcations according to unsupervised Bayesian networks and the
y coordinate is the percentage of misclassiﬁcations according to naive Bayes. Thus, points above the diagonal line
correspond to data sets on which unrestricted Bayesian networks perform better, and points below the diagonal
line correspond to data sets on which naive Bayes performs better.
To conﬁrm this hypothesis, we conducted an experiment comparing the classiﬁcation
accuracy of Bayesian networks learned using the MDL score (i.e., classiﬁers based on
unrestricted networks) to that of the naive Bayesian classiﬁer. We ran this experiment on

----- Page 9 (native) -----
BAYESIAN NETWORK CLASSIFIERS
139
25 data sets, 23 of which were from the UCI repository (Murphy & Aha, 1995). Section 5
describes in detail the experimental setup, evaluation methods, and results. As the results in
Figure 2 show, the classiﬁer based on unrestricted networks performed signiﬁcantly better
than naive Bayes on six data sets, but performed signiﬁcantly worse on six data sets. A quick
examination of the data sets reveals that all the data sets on which unrestricted networks
performed poorly contain more than 15 attributes.
A closer inspection of the networks induced on the two data sets where the unrestricted net-
works performed substantially worse reveals that in these networks the number of relevant
attributes inﬂuencing the classiﬁcation is rather small. While these data sets (“soybean-
large” and “satimage”) contain 35 and 36 attributes, respectively, the classiﬁers induced
relied only on ﬁve attributes for the class prediction. We base our deﬁnition of relevant
attributes on the notion of a Markov blanket of a variable X, which consists of X’s parents,
X’s children, and the parents of X’s children in a given network structure G (Pearl, 1988).
This set has the property that, conditioned on X’s Markov blanket, X is independent of
all other variables in the network. In particular, given an assignment to all the attributes in
the Markov blanket of the class variable C, the class variable is independent of the rest of
the attributes. Hence, prediction using a classiﬁer based on a Bayesian network examines
only the values of attributes in the Markov blanket of C. (Note that in the naive Bayesian
classiﬁer, the Markov blanket of C includes all the attributes, since all of the attributes are
children of C in the graph.) Thus, in learning the structure of the network, the learning
algorithm chooses the attributes that are relevant for predicting the class. In other words,
the learning procedure performs a feature selection. Often, this selection is useful and
discards truly irrelevant attributes. However, as these two examples show, the procedure
might discard attributes that are crucial for classiﬁcation. The choices made by the learning
procedure reﬂect the bias of the MDL score, which penalizes the addition of these crucial
attributes to the class variable’s Markov blanket. As our analysis suggests, the root of the
problem is the scoring function—a network with a better score is not necessarily a better
classiﬁer.
A straightforward approach to this problem would be to specialize the scoring function
(MDL in this case) to the classiﬁcation task. We can do so by restricting the log likelihood
to the ﬁrst term of Equation 6. Formally, let the conditional log likelihood of a Bayesian
network B given data set D be CLL(B|D) = PN
i=1 log PB(Ci|Ai
1, . . . , Ai
n). The problem
associated with the application of this conditional scoring function in practice is of a com-
putational nature. The function does not decompose over the structure of the network; that
is, we do not have an analogue of Equation 4. As a consequence, setting the parameters
θxi|Πxi = ˆPD(xi|Πxi) no longer maximizes the score for a ﬁxed network structure. Thus,
we would need to implement, in addition, a procedure to maximize this new function over
the space of parameters. We discuss this issue further in Section 6.2. Alternative approaches
are discussed in the next section.
4.
Extensions to the naive Bayesian classiﬁer
In this section we examine approaches that maintain the basic structure of a naive Bayes
classiﬁer, and thus ensure that all attributes are part of the class variable Markov blanket.

----- Page 10 (native) -----
140
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
C
Pregnant
Insulin
Age
DPF
Glucose
Mass
Figure 3. A TAN model learned for the data set “pima.” The dashed lines are those edges required by the naive
Bayesian classiﬁer. The solid lines are correlation edges between attributes.
These approaches, however, remove the strong assumptions of independence in naive Bayes
by ﬁnding correlations among attributes that are warranted by the training data.
4.1.
Augmented naive Bayesian networks as classiﬁers
We argued above that the performance of a Bayesian network as a classiﬁer may improve
if the learning procedure takes into account the special status of the class variable. An easy
way to ensure this is to bias the structure of the network, as in the naive Bayesian classiﬁer,
such that there is an edge from the class variable to each attribute. This ensures that, in the
learned network, the probability P(C|A1, . . . , An) will take all attributes into account. In
order to improve the performance of a classiﬁer based on this bias, we propose to augment
the naive Bayes structure with edges among the attributes, when needed, thus dispensing
with its strong assumptions about independence. We call these structures augmented naive
Bayesian networks and these edges augmenting edges.
In an augmented structure, an edge from Ai to Aj implies that the inﬂuence of Ai on the
assessment of the class variable also depends on the value of Aj. For example, in Figure 3,
the inﬂuence of the attribute “Glucose” on the class C depends on the value of “Insulin,”
while in the naive Bayesian classiﬁer the inﬂuence of each attribute on the class variable
is independent of other attributes. These edges affect the classiﬁcation process in that a
value of “Glucose” that is typically surprising (i.e., P(g|c) is low) may be unsurprising if
the value of its correlated attribute, “Insulin,” is also unlikely (i.e., P(g|c, i) is high). In this
situation, the naive Bayesian classiﬁer will overpenalize the probability of the class variable
by considering two unlikely observations, while the augmented network of Figure 3 will
not.
Adding the best set of augmenting edges is an intractable problem, since it is equivalent
to learning the best Bayesian network among those in which C is a root. Thus, even if we
could improve the performance of a naive Bayes classiﬁer in this way, the computational
effort required may not be worthwhile. However, by imposing acceptable restrictions on
the form of the allowed interactions, we can actually learn the optimal set of augmenting
edges in polynomial time.

----- Page 11 (native) -----
BAYESIAN NETWORK CLASSIFIERS
141
Our proposal is to learn a tree-augmented naive Bayesian (TAN) network in which the
class variable has no parents and each attribute has as parents the class variable and at most
one other attribute.1 Thus, each attribute can have one augmenting edge pointing to it. The
network in Figure 3 is in fact an TAN model. As we now show, we can take advantage of
this restriction to learn a TAN model efﬁciently. The procedure for learning these edges is
based on a well-known method reported by Chow and Liu (CL from now on) (1968), for
learning tree-like Bayesian networks (see also (Pearl, 1988, pp. 387–390)). We start by
reviewing CL’s result.
A directed acyclic graph on {X1, . . . , Xn} is a tree if ΠXi contains exactly one parent for
all Xi, except for one variable that has no parents (this variable is referred to as the root).
A tree network can be described by identifying the parent of each variable. A function
π : {1, . . . , n} 7→{0, . . . , n} is said to deﬁne a tree over X1, . . . , Xn, if there is exactly
one i such that π(i) = 0 (namely the root of the tree), and there is no sequence i1, . . . , ik
such that π(ij) = ij+1 for i ≤j < k and π(ik) = i1 (i.e., no cycles). Such a function
deﬁnes a tree network where ΠXi = {Xπ(i)} if π(i) > 0, and ΠXi = ∅if π(i) = 0.
Chow and Liu (1968) describe a procedure for constructing a tree Bayesian network from
data. This procedure reduces the problem of constructing a maximum likelihood tree to
ﬁnding a maximal weighted spanning tree in a graph. The problem of ﬁnding such a tree is
to select a subset of arcs from a graph such that the selected arcs constitute a tree and the sum
of weights attached to the selected arcs is maximized. There are well-known algorithms for
solving this problem of time complexity O(n2 log n), where n is the number of vertices in
the graph (Cormen et al., 1990).
The Construct-Tree procedure of CL consists of four steps:
1.
Compute I ˆ
PD(Xi; Xj) between each pair of variables, i ̸= j, where
IP (X; Y) =
X
x,y
P(x, y) log
P(x, y)
P(x)P(y)
is the mutual information function. Roughly speaking, this function measures how
much information Y provides about X. See Appendix A for a more detailed description
of this function.
2.
Build a complete undirected graph in which the vertices are the variables in X. Annotate
the weight of an edge connecting Xi to Xj by I ˆ
PD(Xi; Xj).
3.
Build a maximum weighted spanning tree.
4.
Transform the resulting undirected tree to a directed one by choosing a root variable
and setting the direction of all edges to be outward from it.
CL prove that this procedure ﬁnds the tree that maximizes the likelihood given the data D.
Theorem 1 (Chow & Liu, 1968) Let D be a collection of N instances of X1, . . . , Xn.
The Construct-Tree procedure constructs a tree BT that maximizes LL(BT |D) and has
time complexity O(n2 · N).

----- Page 12 (native) -----
142
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
This result can now be adapted to learn the maximum likelihood TAN structure. Let
A1, . . . , An be a set of attribute variables and C be the class variable. We say that B is a
TAN model if ΠC = ∅and there is a function π that deﬁnes a tree over A1, . . . , An such
that ΠAi = {C, Aπ(i)} if π(i) > 0, and ΠAi = {C} if π(i) = 0. The optimization problem
consists on ﬁnding a tree deﬁning function π over A1, . . . , An such that the log likelihood
is maximized.
As we prove below, the procedure we call Construct-TAN solves this optimization
problem. ThisprocedurefollowsthegeneraloutlineofCL’sprocedure, exceptthatinsteadof
using the mutual information between two attributes, it uses conditional mutual information
between attributes given the class variable. This function is deﬁned as
IP (X; Y|Z) =
X
x,y,z
P(x, y, z) log
P(x, y|z)
P(x|z)P(y|z) .
Roughly speaking, this function measures the information that Y provides about X when the
value of Z is known. Again, Appendix A gives a more detailed description of this function.
The Construct-TAN procedure consists of ﬁve main steps:
1.
Compute I ˆ
PD(Ai; Aj | C) between each pair of attributes, i ̸= j.
2.
Build a complete undirected graph in which the vertices are the attributes A1, . . . , An.
Annotate the weight of an edge connecting Ai to Aj by I ˆ
PD(Ai; Aj | C).
3.
Build a maximum weighted spanning tree.
4.
Transform the resulting undirected tree to a directed one by choosing a root variable
and setting the direction of all edges to be outward from it.
5.
Construct a TAN model by adding a vertex labeled by C and adding an arc from C to
each Ai.
Theorem 2 Let D be a collection of N instances of C, A1, . . . , An. The procedure
Construct-TAN builds a TAN BT that maximizes LL(BT |D) and has time complexity
O(n2 · N).
Proof: We start with a reformulation of the log likelihood:
LL(BT |D) = N ·
X
Xi
I ˆ
PD(Xi; ΠXi) + constant term,
(7)
which we derive in Appendix A. Thus, maximizing the log likelihood is equivalent to
maximizing the term
X
Xi
I ˆ
PD(Xi; ΠXi) .
We now specialize this term for TAN models. Let BT be a TAN deﬁned by π(·). Since C
has no parents, we have I ˆ
PD(C; ΠC) = 0. Since the parents of Ai are deﬁned by π, we

----- Page 13 (native) -----
BAYESIAN NETWORK CLASSIFIERS
143
set I ˆ
PD(Ai; ΠAi) = I ˆ
PD(Ai; Aπ(i), C) if π(i) > 0 and I ˆ
PD(Ai; ΠAi) = I ˆ
PD(Ai; C) if
π(i) = 0. Hence, we need to maximize the term
X
i,π(i)>0
I ˆ
PD(Ai; Aπ(i), C) +
X
i,π(i)=0
I ˆ
PD(Ai; C) .
(8)
We simplify this term by using the identity known as the chain law for mutual information
(Cover & Thomas, 1991): IP (X; Y, Z) = IP (X; Z) + IP (X; Y|Z). Hence, we can rewrite
expression (8) as
X
i
I ˆ
PD(Ai; C) +
X
i,π(i)>0
I ˆ
PD(Ai; Aπ(i)|C)
Note that the ﬁrst term is not affected by the choice of π(i). Therefore, it sufﬁces to
maximize the second term. Note also that the TAN model found by Construct-TAN is
guaranteed to maximize this term, and thus maximizes the log likelihood.
The ﬁrst step of Construct-TAN has complexity of O(n2 · N) and the third step has
complexity of O(n2 log n). Since usually N > log n, we get the stated time complexity.
Our initial experiments showed that the TAN model works well in that it yields good
classiﬁerscomparedtonaiveBayes, asshowninTables2and3). Itsperformancewasfurther
improved by the introduction of an additional smoothing operation. Recall that to learn the
parameters of a network we estimate conditional frequencies of the form ˆPD(X|ΠX). We
do this by partitioning the training data according to the possible values of ΠX and then
computing the frequency of X in each partition. When some of these partitions contain
very few instances, however, the estimate of the conditional probability is unreliable. This
problem is not as acute in the case of a naive Bayesian classiﬁer, since it partitions the data
according to the class variable, and usually all values of the class variables are adequately
represented in the training data. In TAN networks, however, for each attribute we assess
the conditional probability given the class variable and another attribute. This means that
the number of partitions is at least twice as large. Thus, it is not surprising to encounter
unreliable estimates, especially in small data sets.
Todealwiththisproblem, weintroduceasmoothingoperationontheparameterslearnedin
TAN models that is motivated by Bayesian considerations. In Bayesian learning of a multi-
nomial distribution P(X = vi) for i = 1, . . . , k, we start with a prior probability measure
over the possible settings of parameters Θ = {θi : i = 1, . . . , k}, where θi = P(X = vi),
and then compute the posterior probability Pr(Θ | D). The predicted probability for a new
instance of X is the weighted average of the predictions of all possible setting of Θ, weighted
by their posterior probability. Thus, Pr(X = vi|D) =
R
Pr(Xi = vi | Θ)Pr(Θ | D)dΘ.
For a particular family of priors, called Dirichlet priors, there is a known closed-form solu-
tion for this integral. A Dirichlet prior is speciﬁed by two hyperparameters: Θ0, an initial
estimate of Θ, and N 0, a number that summarizes our conﬁdence in this initial estimate.
One can think of N 0 as the number of samples we have seen in our lifetime prior to making
the estimate Θ0. Given hyperparameters Θ0 = {θ0
i } and N 0, and a data set D of length

----- Page 14 (native) -----
144
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
0
5
10
15
20
25
30
35
40
45
1 10 9 11 4 6 2 13 18 25 14 22 7 21 12 8 24 20 17 3 16 19 5 15 23
Percentage Classification Error
Data Set
TAN
Naive Bayes
 0
 5
10
15
20
25
30
35
40
45
 0
 5
10
15
20
25
30
35
40
45
Naive Bayes Error
TAN Error
Figure 4. Error curves and scatter plot comparing smoothed TAN (solid, x axis) with naive Bayes (dashed, y axis).
In the error curves, the smaller the value, the better the accuracy. In the scatter plot, points above the diagonal
line correspond to data sets where smoothed TAN performs better, and points below the diagonal line correspond
to data sets where naive Bayes performs better. See the caption of Figure 2 for a more detailed description.
N, the prediction for P(X = vi) has the form
P(X = vi|D) =
N
N + N 0 ˆPD(X = vi) +
N 0
N + N 0 θ0
i .
We refer the interested reader to DeGroot (1970). It is easy to see that this prediction biases
the learned parameters in a manner that depends on the conﬁdence in the prior and the

----- Page 15 (native) -----
BAYESIAN NETWORK CLASSIFIERS
145
Figure 5. Error curves and scatter plot comparing smoothed TAN (solid, x axis) with selective naive Bayes
(dashed, y axis). In the error curves, the smaller the value, the better the accuracy. In the scatter plot, points above
the diagonal line correspond to data sets where smoothed TAN performs better, and points below the diagonal
line correspond to data sets where selective naive Bayes performs better. See the caption of Figure 2 for a more
detailed description.
number of new instances in the data: the more instances we have in the training data, the
less bias is applied. If the number of instances N is large relative to N 0, than the bias
essentially disappears. On the other hand, if the number of instances is small, then the prior
dominates.
In the context of learning Bayesian networks, we can use a different Dirichlet prior for
each distribution of Xi given a particular value of its parents (Heckerman, 1995). This

----- Page 16 (native) -----
146
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
0
5
10
15
20
25
30
35
3 24 10 5 1 7 6 8 23 14 22 21 18 9 11 20 2 12 25 19 13 17 16 15 4
Percentage Classification Error
Data Set
TAN
C4.5
 0
 5
10
15
20
25
30
35
 0
 5
10
15
20
25
30
35
C4.5 Error
TAN Error
Figure 6. Error curves and scatter plot comparing smoothed TAN (solid, x axis) with C4.5 (dashed, y axis). In
the error curves, the smaller the value, the better the accuracy. In the scatter plot, points above the diagonal line
correspond to data sets where smoothed TAN performs better, and points below the diagonal line correspond to
data set where C4.5 performs better. See the caption of Figure 2 for a more detailed description.
results in choosing the parameters
θs(x|Πx) =
N · ˆPD(Πx)
N · ˆPD(Πx) + N 0
x|Πx
· ˆPD(x|Πx) +
N 0
x|Πx
N · ˆPD(Πx) + N 0
x|Πx
θ0(x|Πx) ,

----- Page 17 (native) -----
BAYESIAN NETWORK CLASSIFIERS
147
where θ0(x|Πx) is the prior estimate of P(x|Πx) and N 0
x|Πx is the conﬁdence associated
with that prior. Note that this application of Dirichlet priors biases the estimation of the
parameters depending on the number of instances in the data with particular values of X’s
parents. Thus, it mainly affects the estimation in those parts of the conditional probability
table that are rarely seen in the training data.
To use this method, we must therefore choose the prior parameters. One reasonable
choice of prior is the uniform distribution with some small N 0. Another reasonable choice
uses the marginal probability of X in the data as the prior probability. This choice is based
on the assumption that most conditional probabilities are close to the observed marginal.
Thus, we set θ0(x | Πx) = ˆPD(x). After initial trials we choose the value of N 0 to be 5 in
all of our experiments. (More precisely, we tried the values 1, 5, and 10 on a few data sets,
and N 0 = 5 was slightly better than the others.) We note that this smoothing is performed
after determining the structure of the TAN model. Thus, the smoothed model has the same
qualitative structure as the original model but has different numerical parameters. This
form of smoothing is standard practice in Bayesian statistics.2
In our experiments comparing the prediction error of smoothed TAN to that of unsmoothed
TAN, we observed that smoothed TAN performs at least as well as TAN, and occasionally
outperforms TAN signiﬁcantly (e.g., see the results for “soybean-large,” “segment,” and
“lymphography” in Table 3). Henceforth, we will assume that the version of TAN uses the
smoothing operator, unless noted otherwise.
Figure 4 compares the prediction error of the TAN classiﬁer to that of naive Bayes. As can
be seen, the the TAN classiﬁer dominates naive Bayes. This result supports our hypothesis
that, by relaxing the strong independence assumptions made by naive Bayes, one can indeed
learn better classiﬁers. We also tried a smoothed version of naive Bayes. This, however, did
not lead to signiﬁcant improvement over the unsmoothed naive Bayes. The only data set
where there was a noticeable improvement is “lymphography,” where the smoothed version
had 81.73% accuracy compared to 79.72% without smoothing. Note that for this particular
data set, the smoothed version of TAN has 85.03% accuracy compared to 66.87% without
smoothing. The complete results for the smoothed version of naive Bayes are reported in
Table 3.
Given that TAN performs better than naive Bayes and that naive Bayes is comparable to
C4.5 (Quinlan, 1993), a state-of-the-art decision tree learner, we may infer that TAN should
perform rather well in comparison to C4.5. To conﬁrm this prediction, we performed
experiments comparing TAN to C4.5, and also to the selective naive Bayesian classiﬁer
(Langley & Sage, 1994; John & Kohavi, 1997). The latter approach searches for the subset
of attributes over which naive Bayes has the best performance. The results, displayed in
Figures 5 and 6 and in Table 2, show that TAN is competitive with both approaches and can
lead to signiﬁcant improvements in many cases.
4.2.
Bayesian multinets as classiﬁers
The TAN approach forces the relations among attributes to be the same for all the differ-
ent instances of the class variable C. An immediate generalization would have different

----- Page 18 (native) -----
148
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
augmenting edges (tree structures in the case of TAN) for each class, and a collection of
networks as the classiﬁer.
To implement this idea, we partition the training data set by classes. Then, for each class
ci in Val(C), we construct a Bayesian network Bi for the attribute variables {A1, . . . , An}.
The resulting probability distribution PBi(A1, . . . , An) approximates the joint distribution
of the attributes, given a speciﬁc class, that is, ˆPD(A1, . . . , An | C = ci). The Bayesian
network for ci is called a local network for ci. The set of local networks combined with a
prior on C, P(C), is called a Bayesian multinet (Heckerman, 1991; Geiger & Heckerman,
1996). Formally, a multinet is a tuple M = ⟨PC, B1, . . . , Bk⟩where PC is a distribution
on C, and Bi is a Bayesian network over A1, . . . , An for 1 ≤i ≤k = |Val(C)|. A multinet
M deﬁnes a joint distribution:
PM(C, A1, . . . , An) = PC(C) · PBi(A1, . . . , An) when C = ci.
When learning a multinet, we set PC(C) to be the frequency of the class variable in
the training data, that is, ˆPD(C), and learn the networks Bi in the manner just described.
Once again, we classify by choosing the class that maximizes the posterior probability
PM(C|A1, . . . , An). By partitioning the data according to the class variable, this method-
ology ensures that the interactions between the class variable and the attributes are taken into
account. The multinet proposal is strictly a generalization of the augmented naive Bayes,
in the sense that that an augmented naive Bayesian network can be easily simulated by a
multinet where all the local networks have the same structure. Note that the computational
complexity of ﬁnding unrestricted augmenting edges for the attributes is aggravated by the
need to learn a different network for each value of the class variable. Thus, the search for
learning the Bayesian network structure must be carried out several times, each time on a
different data set.
As in the case of augmented naive Bayes, we can address this problem by constraining
the class of local networks we might learn to be treelike. Indeed, the construction of a set
of trees that minimizes the log likelihood score was the original method used by Chow and
Liu (1968) to build classiﬁers for recognizing handwritten characters. They reported that,
in their experiments, the error rate of this method was less than half that of naive Bayes.
We can use the algorithm in Theorem 1 separately to the attributes that correspond to
each value of the class variable. This results in a multinet in which each network is a tree.
Corollary 1 (Chow&Liu, 1968)LetD beacollectionofN instancesofC, A1, . . . , An.
There is a procedure of time complexity O(n2 · N) which constructs a multinet consisting
of trees that maximizes log likelihood.
Proof: The procedure is as follows:
1.
Split D into k = |Val(C)| partitions, D1, . . . , Dk, such that Di contains all the instances
in D where C = ci.
2.
Set PC(ci) = ˆPD(ci).
3.
Apply the procedure Construct-Tree of Theorem 1 on Di to construct Bi.

----- Page 19 (native) -----
BAYESIAN NETWORK CLASSIFIERS
149
Steps 1 and 2 take linear time. Theorem 1 states that step 3 has time complexity O(n2|Di|)
for each i. Since P
i |Di| = N, we conclude that the whole procedure has time complexity
O(n2N).
As with TAN models, we apply smoothing to avoid unreliable estimation of parameters.
Note also that we partition the data further, and therefore run a higher risk of missing the
accurate weight of some edge (in contrast to TAN). On the other hand, TAN forces the model
to show the same augmenting edges for all classes. As can be expected, our experiments
(see Figure 7) show that Chow and Liu (CL) multinets perform as well as TAN, and that
neither approach clearly dominates.
4.3.
Beyond tree-like networks
In the previous two sections we concentrated our attention on tree-like augmented naive
Bayesian networks and Bayesian multinets, respectively. This restriction was motivated
mainly by computational considerations: these networks can be induced in a provably
effective manner. This raises the question whether we can achieve better performance at
the cost of computational efﬁciency. One straightforward approach to this question is to
search the space of all augmented naive Bayesian networks (or the larger space of Bayesian
multinets) and select the one that minimizes the MDL score.
This approach presents two problems. First, we cannot examine all possible network
structures; therefore we must resort to heuristic search. In this paper we have examined
a greedy search procedure. Such a procedure usually ﬁnds a good approximation to the
minimal MDL scoring network.
Occasionally, however, it will stop at a “poor” local
minimum. To illustrate this point, we ran this procedure on a data set generated from a
parity function. This concept can be captured by augmenting the naive Bayes structure with
a complete subgraph. However, the greedy procedure returned the naive Bayes structure,
which resulted in a poor classiﬁcation rate. The greedy procedure learns this network
because attributes are independent of each other given the class. As a consequence, the
addition of any single edge did not improve the score, and thus, the greedy procedure
terminated without adding any edges.
The second problem involves the MDL score. Recall that the MDL score penalizes larger
networks. The relative size of the penalty grows larger for smaller data sets, so that the
score is heavily biased for simple networks. As a result, the procedure we just described
might learn too few augmenting edges. This problem is especially acute when there are
many classes. In this case, the naive Bayesian structure by itself requires many parameters,
and the addition of an augmenting edge involves adding at least as many parameters as the
number of classes. In contrast, we note that both the TAN and CL multinet classiﬁer learn
a spanning tree over all attributes.
As shown by our experimental results, see Table 4, both unrestricted augmented naive
Bayesian networks and unrestricted multinets lead to improved performance over that of
the unrestricted Bayesian networks of Section 3. Moreover, on some data sets they have
better accuracy than TAN and CL multinets.

----- Page 20 (native) -----
150
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
Figure 7. Error curves and scatter plot comparing smoothed TAN (solid line, x axis) with smoothed CL multinet
classiﬁer (dashed line, y axis). In the error curves, the smaller the value, the better the accuracy. In the scatter plot,
points above the diagonal line corresponds to data sets where smoothed TAN performs better and points below the
diagonal line corresponds to data sets where smoothed CL multinets classiﬁer performs better. See the caption of
Figure 2 for a more detailed description.
5.
Experimental methodology and results
We ran our experiments on the 25 data sets listed in Table 1. All of the data sets come
from the UCI repository (Murphy & Aha, 1995), with the exception of “mofn-3-7-10” and
“corral”. These two artiﬁcial data sets were designed by John and Kohavi (1997) to evaluate
methods for feature subset selection.

----- Page 21 (native) -----
BAYESIAN NETWORK CLASSIFIERS
151
Table 1. Description of data sets used in the experiments.
Dataset
# Attributes
# Classes
# Instances
Train
Test
1
australian
14
2
690
CV-5
2
breast
10
2
683
CV-5
3
chess
36
2
2130
1066
4
cleve
13
2
296
CV-5
5
corral
6
2
128
CV-5
6
crx
15
2
653
CV-5
7
diabetes
8
2
768
CV-5
8
ﬂare
10
2
1066
CV-5
9
german
20
2
1000
CV-5
10
glass
9
7
214
CV-5
11
glass2
9
2
163
CV-5
12
heart
13
2
270
CV-5
13
hepatitis
19
2
80
CV-5
14
iris
4
3
150
CV-5
15
letter
16
26
15000
5000
16
lymphography
18
4
148
CV-5
17
mofn-3-7-10
10
2
300
1024
18
pima
8
2
768
CV-5
19
satimage
36
6
4435
2000
20
segment
19
7
1540
770
21
shuttle-small
9
7
3866
1934
22
soybean-large
35
19
562
CV-5
23
vehicle
18
4
846
CV-5
24
vote
16
2
435
CV-5
25
waveform-21
21
3
300
4700
The accuracy of each classiﬁer is based on the percentage of successful predictions on the
test sets of each data set. We used the MLC++ system (Kohavi et al., 1994) to estimate the
prediction accuracy for each classiﬁer, as well as the variance of this accuracy. Accuracy was
measuredviatheholdoutmethodforthelargerdatasets(thatis, thelearningprocedureswere
given a subset of the instances and were evaluated on the remaining instances), and via ﬁve-
fold cross validation, using the methods described by Kohavi (1995), for the smaller ones.3
Since we do not deal, at present, with missing data, we removed instances with missing
values from the data sets. Currently, we also do not handle continuous attributes. Instead,
we applied a pre-discretization step in the manner described by Dougherty et al. (1995).
This pre-discretization is based on a variant of Fayyad and Irani’s (1993) discretization
method. These preprocessing stages were carried out by the MLC++ system. Runs with
the various learning procedures were carried out on the same training sets and evaluated
on the same test sets. In particular, the cross-validation folds were the same for all the
experiments on each data set.
Table 2 displays the accuracies of the main classiﬁcation approaches we have discussed
throughout the paper using the abbreviations:
NB: the naive Bayesian classiﬁer
BN: unrestricted Bayesian networks learned with the MDL score
TANs: TAN networks learned according to Theorem 2, with smoothed parameters

----- Page 22 (native) -----
152
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
CLs: CL multinet classiﬁer—Bayesian multinets learned according to Theorem 1—with
smoothed parameters
C4.5: the decision-tree induction method developed by Quinlan (1993)
SNB: the selective naive Bayesian classiﬁer, a wrapper-based feature selection applied to
naive Bayes, using the implementation of John and Kohavi (1997)
In the previous sections we discussed these results in some detail. We now summarize
the highlights. The results displayed in Table 2 show that although unrestricted Bayesian
networks can often lead to signiﬁcant improvement over the naive Bayesian classiﬁer, they
can also result in poor classiﬁers in the presence of multiple attributes. These results also
show that both TAN and the CL multinet classiﬁer are roughly equivalent in terms of
accuracy, dominate the naive Bayesian classiﬁer, and compare favorably with both C4.5
and the selective naive Bayesian classiﬁer.
Table 3 displays the accuracies of the naive Bayesian classiﬁer, the TAN classiﬁer, and
CL multinet classiﬁer with and without smoothing. The columns labeled NB, TAN, and CL
present the accuracies without smoothing, and the columns labeled NBs, TANs, and CLs
describe the accuracies with smoothing. These results show that smoothing can signiﬁcantly
improve the accuracy both of TAN and of CL multinet classiﬁer and does not signiﬁcantly
degrade the accuracy of results from other data sets. Improvement is noticed mainly in
small data sets and in data sets with large numbers of classes. On the other hand, smoothing
does not signiﬁcantly improve the accuracy of the naive Bayesian classiﬁer.
Finally, in Table 4 we summarize the accuracies of learning unrestricted augmented naive
Bayes networks (ANB) and multinets (MN) using the MDL score. The table also contains
the corresponding tree-like classiﬁers for comparison. These results show that learning
unrestricted networks can improve the accuracy in data sets that contain strong interactions
between attributes and that are large enough for the MDL score to add edges. On other data
sets, the MDL score is reluctant to add edges giving structures that are similar to the naive
Bayesian classiﬁer. Consequently, in these data sets, the predictive accuracy will be poor
when compared with TAN and CL multinet classiﬁer.
6.
Discussion
Inthissection, wereviewrelatedworkandexpandontheissueofaconditionalloglikelihood
scoring function. Additionally, we discuss how to extend the methods presented here to
deal with complicating factors such as numeric attributes and missing values.
6.1.
Related work on naive Bayes
There has been recent interest in explaining the surprisingly good performance of the naive
Bayesian classiﬁer (Domingos & Pazzani, 1996; Friedman, 1997a). The analysis provided
by Friedman (1997a) is particularly illustrative, in that it focuses on characterizing how the
bias and variance components of the estimation error combine to inﬂuence classiﬁcation

----- Page 23 (native) -----
BAYESIAN NETWORK CLASSIFIERS
153
Table 2. Experimental results of the primary approaches discussed in this paper.
performance. For the naive Bayesian classiﬁer, he shows that, under certain conditions, the
low variance associated with this classiﬁer can dramatically mitigate the effect of the high
bias that results from the strong independence assumptions.
One goal of the work described in this paper has been to improve the performance of the
naive Bayesian classiﬁer by relaxing these independence assumptions. Indeed, our empir-
ical results indicate that a more accurate modeling of the dependencies amongst features
leads to improved classiﬁcation. Previous extensions to the naive Bayesian classiﬁer also
identiﬁed the strong independence assumptions as the source of classiﬁcation errors, but
differ in how they address this problem. These works fall into two categories.
Work in the ﬁrst category, such as that of Langley and Sage (1994) and of John and
Kohavi (1997), has attempted to improve prediction accuracy by rendering some of the
attributes irrelevant. The rationale is as follows. As we explained in Section 4.1, if two
attributes, say Ai and Aj, are correlated, then the naive Bayesian classiﬁer may overamplify
the weight of the evidence of these two attributes on the class. The proposed solution in
this category is simply to ignore one of these two attributes. (Removing attributes is also
useful if some attributes are irrelevant, since they only introduce noise in the classiﬁcation
problem.) This is a straightforward application of feature subset selection. The usual
approach to this problem is to search for a good subset of the attributes, using an estimation
scheme, such as cross validation, to repeatedly evaluate the predictive accuracy of the naive
Bayesian classiﬁer on various subsets. The resulting classiﬁer is called the selective naive
Bayesian classiﬁer, following Langley and Sage (1994).

----- Page 24 (native) -----
154
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
Table 3. Experimental results describing the effect of smoothing parameters.
It is clear that, if two attributes are perfectly correlated, then the removal of one can only
improve the performance of the naive Bayesian classiﬁer. Problems arise, however, if two
attributes are only partially correlated. In these cases the removal of an attribute may lead
to the loss of useful information, and the selective naive Bayesian classiﬁer may still retain
both attributes. In addition, this wrapper-based approach is, in general, computationally
expensive. Our experimental results (see Figure 6) show that the methods we examine here
are usually more accurate than the selective naive Bayesian classiﬁer as used by John and
Kohavi (1997).
Work in the second category (Kononenko, 1991; Pazzani, 1995; Ezawa & Schuermann,
1995) are closer in spirit to our proposal, since they attempt to improve the predictive
accuracy by removing some of the independence assumptions. The semi-naive Bayesian
classiﬁer (Kononenko, 1991) is a model of the form
P(C, A1, . . . , An) = P(C) · P(A1|C) · · · P(Ak|C)
(9)
where A1, . . . , Ak are pairwise disjoint groups of attributes. Such a model assumes that Ai
is conditionally independent of Aj if, and only if, they are in different groups. Thus, no as-
sumption of independence is made about attributes that are in the same group. Kononenko’s
method uses statistical tests of independence to partition the attributes into groups. This pro-
cedure, however, tends to select large groups, which can lead to overﬁtting problems. The
number of parameters needed to estimate P(Ai|C) is |Val(C)| · (Q
Aj∈Ai |Val(Aj)| −1),
which grows exponentially with the number of attributes in the group. Thus, the parameters

----- Page 25 (native) -----
BAYESIAN NETWORK CLASSIFIERS
155
Table 4. Experimental results of comparing tree-like networks with unrestricted augmented naive Bayes and
multinets.
assessed for P(Ai|C) may quickly become unreliable if Ai contains more than two or three
attributes.
Pazzani suggests that this problem can be solved by using a cross-validation scheme to
evaluate the accuracy of a classiﬁer. His procedure starts with singleton groups (i.e., A1 =
{A1}, . . . , An = {An}) and then combines, in a greedy manner, pairs of groups. (He
also examines a procedure that performs feature subset selection after the stage of joining
attributes.) This procedure does not, in general, select large groups, since these lead to poor
prediction accuracy in the cross-validation test. Thus, Pazzani’s procedure learns classiﬁers
that partition the attributes in to many small groups. Since each group of attributes is
considered independent of the rest given the class, these classiﬁers can capture only small
number of correlations among the attributes.
Both Kononenko and Pazzani essentially assume that all of the attributes in each group
Ai can be arbitrarily correlated. To understand the implications of this assumption, we use
a Bayesian network representation. If we let Ai = {Ai1, . . . , Ail}, then, using the chain
rule, we get
P(Ai|C) = P(Ai1|C) · P(Ai2|Ai,1, C) · · · P(Ail|Ai,1, . . . , Ail−1, C).
Applying this decomposition to each of the terms in Equation 9, we get a product form
from which a Bayesian network can be built. Indeed, this is an augmented naive Bayes
network, in which there is a complete subgraph—that is, one to which we cannot add arcs

----- Page 26 (native) -----
156
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
without introducing a cycle—on the variables of each group Ai. In contrast, in a TAN
network there is a tree that spans over all attributes; thus, these models retain conditional
independencies among correlated attributes. For example, consider a data set where the two
attributes, A1 and A2, are each correlated with another attribute, A3, but are independent of
each other given A3. These correlations are captured by the semi-naive Bayesian classiﬁer
only if all three attributes are in the same group. In contrast, a TAN classiﬁer can place
an edge from A3 to A1 and another to A2. These edges capture the correlations between
the attributes. Moreover, if the attributes and the class variable are boolean, then the TAN
model would require 11 parameters, while the semi-naive Bayesian classiﬁer would require
14 parameters.
Thus, the representational tools of Bayesian networks let us relax the
independence assumptions between attributes in a gradual and ﬂexible manner, and study
and characterize these tradeoffs with the possibility of selecting the right compromise for
the application at hand.
Ezawa and Schuermann (1995) describe a method that use correlations between attributes
in a different manner. First, it computes all pairwise mutual information between attributes
and sorts them in descending order. Then, the method adds edges among attributes going in
the computed order until it reaches some predeﬁned threshold T. This approach presents a
problem. Consider three attributes that are correlated as in the above example: A1 and A3
are correlated, A2 and A3 are correlated, but A1 is probabilistically independent of A2 given
A3. When this method is used, the pairwise mutual information of all combinations will
appear to be high, and the algorithm will propose an edge between every pair of attributes.
Nonetheless, the edge between A1 and A2 is superﬂuous, since their relation is mediated
through A3. This problem is aggravated if we consider a fourth attribute, A4, that is strongly
correlated to A2. Then, either more superﬂuous edges will be added, or, if the threshold T
is reached, this genuine edge will be ignored in favor of a superﬂuous one. Even though the
TAN approach also relies on pairwise computation of the mutual information, it avoids this
problem by restricting the types of interactions to the form of a tree. We reiterate, that under
this restriction, the TAN approach ﬁnds an optimal tree (see Theorem 2). This example
also shows why learning structures that are not trees—that is, where some attributes have
more than one parent—requires us to examine higher-order interactions such as the mutual
information of A1 with A2 given C and A3.
Finally, another related effort that is somewhere between the categories mentioned above
is reported by Singh and Provan (1995, 1996). They combine several feature subset selec-
tion strategies with an unsupervised Bayesian network learning routine. This procedure,
however, can be computationally intensive (e.g., some of their strategies (Singh & Provan,
1995) involve repeated calls to a the Bayesian network learning routine).
6.2.
The conditional log likelihood
Even though the use of log likelihood is warranted by an asymptotic argument, as we have
seen, it may not work well when we have a limited number of samples. In Section 3
we suggested an approach based on the decomposition of Equation 6 that evaluates the
predictive error of a model by restricting the log likelihood to the ﬁrst term of the equation.
This approach is an example of a node monitor, in the terminology of Spiegelhalter, Dawid,

----- Page 27 (native) -----
BAYESIAN NETWORK CLASSIFIERS
157
Lauritzen, and Cowell (1993). Let the conditional log likelihood of a Bayesian network
B, given data set D, be CLL(B|D) = PN
i=1 log PB(Ci|Ai
1, . . . , Ai
n). Maximizing this
term amounts to maximizing the ability to correctly predict C for each assignment to
A1, . . . , An. Using manipulations analogous to the one described in Appendix A, it is
easy to show that maximizing the conditional log likelihood is equivalent to minimizing the
conditional cross-entropy:
D( ˆPD(C|A1 . . . , An)||PB(C|A1 . . . , An)) =
X
a1,...,an
ˆPD(a1 . . . , an)D( ˆPD(C|a1 . . . , an)||PB(C|a1 . . . , an))
(10)
This equation shows that by maximizing the conditional log likelihood we are learning the
model that best approximates the conditional probability of C given the attribute values.
Consequently, themodelthatmaximizesthisscoringfunctionshouldyieldthebestclassiﬁer.
We can easily derive a conditional MDL score that is based on the conditional log like-
lihood. In this variant, the learning task is stated as an attempt to efﬁciently describe the
class values for a ﬁxed collection of attribute records. The term describing the length of
the encoding of the Bayesian network model remains as before, while the second term is
equal to N · CLL(B|D). Unfortunately, we do not have an effective way to maximize the
term CLL(B|D), and thus the computation of the network that minimizes the overall score
becomes infeasible.
Recall that, as discussed in Section 3, once the structure of the network is ﬁxed, the MDL
score is minimized by simply substituting the frequencies in the data as the parameters of
the network (see Equation 5). Once we change the score to the CLL(B|D), this is true
only for a very restricted class of structures. If C is a leaf in the network—that is, if C
does not have any outgoing arcs—then it is easy to prove that setting parameters according
to Equation 5 maximizes CLL(B|D) for a ﬁxed network structure. However, if C has
outgoing arcs, we cannot describe PB(C|A1, . . . , An) as a product of parameters in Θ,
since PB(C|A1, . . . , An) also involves a normalization constant that requires us to sum
over all values of C. As a consequence, CLL(B|D) does not decompose, and we cannot
maximize the choice of each conditional probability table independently of the others.
Hence, we do not have a closed-form equation for choosing the optimal parameters for the
conditional log likelihood score. This implies that, to maximize the choice of parameters
for a ﬁxed network structure, we must resort to search methods such as gradient descent over
the space of parameters (e.g., using the techniques of (Binder et al., 1997)). When learning
the network structure, this search must be repeated for each structure candidate, rendering
the method computationally expensive. Whether we can ﬁnd heuristic approaches that will
allow effective learning using the conditional log likelihood remains an open question.
The difference between procedures that maximize log likelihood and ones that maximize
conditional log likelihood is similar to a standard distinction made in the statistics literature.
Dawid (1976) describes two paradigms for estimating P(C, A1, . . . , An). These paradigms
differ in how they decompose P(C, A1, . . . , An). In the sampling paradigm, we assume that
P(C, A1, . . . , An) = P(C) · P(A1, . . . , An|C) and assess both terms. In the diagnostic
paradigm, we assume that P(C, A1, . . . , An) = P(A1, . . . , An) · P(C|A1, . . . , An) and

----- Page 28 (native) -----
158
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
assess only the second term, since it is the only one relevant to the classiﬁcation process.
In general, neither of these approaches dominates the other (Ripley, 1996).
The naive Bayesian classiﬁer and the extensions we have evaluated belong to the sampling
paradigm. Although the unrestricted Bayesian networks (described in Section 3) do not
strictly belong in either paradigm, they are closer in spirit to the sampling paradigm.
6.3.
Numerical attributes and missing values
Throughout this paper we have made two assumptions: that all attributes have ﬁnite numbers
of values, and that the training data are complete, in that each instance assigns values to all
the variables of interest. We now brieﬂy discuss how to move beyond both these restrictions.
One approach to dealing with numerical attributes is to discretize them prior to learning
a model.
This is done using a discretization procedure such as the one suggested by
Fayyad and Irani (1993), to partition the range of each numerical attribute. Then we can
invoke our learning method treating all variables as having discrete values. As shown
by Dougherty, et al. (1995), this approach is quite effective in practice. An alternative
is to discretize numerical attributes during the learning process, which lets the procedure
adjust the discretization of each variable so that it contains just enough partitions to capture
interactions with adjacent variables in the network. Friedman and Goldszmidt (1996b)
propose a principled method for performing such discretization.
Finally, there is no conceptual difﬁculty in representing hybrid Bayesian networks that
contain both discrete and continuous variables. This approach involves choosing an appro-
priate representation for the conditional density of a numerical variable given its parents;
for example, Heckerman and Geiger (1995) examine learning networks with Gaussian dis-
tributions. It is straightforward to combine such representations in the classes of Bayesian
networks described in this paper. For example, a Gaussian variant of the naive Bayesian
classiﬁer appears in Duda and Hart (1973) and a variant based on kernel estimators appears
in John and Langley (1995). We suspect that there exist analogues to Theorem 2 for such
hybrid networks but we leave this issue for future work.
Regarding the problem of missing values, in theory, probabilistic methods provide a
principled solution. If we assume that values are missing at random (Rubin, 1976), then
we can use the marginal likelihood (the probability assigned to the parts of the instance that
were observed) as the basis for scoring models. If the values are not missing at random, then
more careful modeling must be exercised in order to include the mechanism responsible for
the missing data.
The source of difﬁculty in learning from incomplete data, however, is that the marginal
likelihood does not decompose. That is, the score cannot be written as the sum of local terms
(as in Equation 4). Moreover, to evaluate the optimal choice of parameters for a candidate
network structure, we must perform nonlinear optimization using either EM (Lauritzen,
1995) or gradient descent (Binder et al., 1997).
The problem of selecting the best structure is usually intractable in the presence of missing
values. Several recent efforts (Geiger et al., 1996; Chickering & Heckerman, 1996) have
examined approximations to the marginal score that can be evaluated efﬁciently. Addition-
ally, Friedman (1997b) has proposed a variant of EM for selecting the graph structure that

----- Page 29 (native) -----
BAYESIAN NETWORK CLASSIFIERS
159
can efﬁciently search over many candidates. The computational cost associated with all of
these methods is directly related to the problem of inference in the learned networks. For-
tunately, inference in TAN models can be performed efﬁciently. For example, Friedman’s
method can be efﬁciently applied to learning TAN models in the presence of missing values.
We plan to examine the effectiveness of this and other methods for dealing with missing
values in future work.
7.
Conclusions
In this paper, we have analyzed the direct application of the MDL method to learning
unrestricted Bayesian networks for classiﬁcation tasks.
We showed that, although the
MDL method presents strong asymptotic guarantees, it does not necessarily optimize the
classiﬁcation accuracy of the learned networks. Our analysis suggests a class of scoring
functions that may be better suited to this task. These scoring functions appear to be
computationally intractable, and we therefore plan to explore effective approaches based
on approximations of these scoring functions.
The main contribution of our work is the experimental evaluation of the tree-augmented
naive Bayesian classiﬁers, TAN, and the Chow and Liu multinet classiﬁer. It is clear that
in some situations, it would be useful to model correlations among attributes that cannot be
captured by a tree structure (or collections of tree structures). Such models will be preferable
when there are enough training instances to robustly estimate higher-order conditional
probabilities. Still, both TAN and CL multinets embody a good tradeoff between the quality
of the approximation of correlations among attributes and the computational complexity in
the learning stage. The learning procedures are guaranteed to ﬁnd the optimal tree structure,
and, as our experimental results show, they perform well in practice against state-of-the-art
classiﬁcation methods in machine learning. We therefore propose them as worthwhile tools
for the machine learning community.
Acknowledgments
The authors are grateful to Denise Draper, Ken Fertig, Joe Halpern, David Heckerman, Ron
Kohavi, Pat Langley, Judea Pearl, and Lewis Stiller for comments on previous versions
of this paper and useful discussions relating to this work. We also thank Ron Kohavi for
technical help with the MLC++ library. Most of this work was done while Nir Friedman
and Moises Goldszmidt were at the Rockwell Science Center in Palo Alto, California. Nir
Friedman also acknowledges the support of an IBM graduate fellowship and NSF Grant
IRI-95-03109.

----- Page 30 (native) -----
160
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
Appendix A
Information-theoretic interpretation of the log likelihood
Here we review several information-theoretic notions and how they let us represent the log
likelihood score. We will concentrate on the essentials and refer the interested reader to
Cover and Thomas (1991) for a comprehensive treatment of these notions.
Let P be a joint probability distribution over U. The entropy of X (given P) is deﬁned
as HP (X) = −P
x P(x) log P(x). The function HP (X) is the optimal number of bits
needed to store the value of X, which roughly measures the amount of information carried
by X. More precisely, suppose that x1, . . . , xm is a sequence of independent samples of
X according to P(X), then we cannot represent x1, . . . , xm with less than m · HP (X) bits
(assuming that m is known). With this interpretation in mind, it is easy to understand the
properties of the entropy. First, the entropy is always nonnegative, since the encoding length
cannot be negative. Second, the entropy is zero if and only if X is perfectly predictable,
i.e., if one value of X has probability 1. In this case, we can reconstruct x1, . . . , xm without
lookingattheencoding. Finally, theentropyismaximalwhenP(X)istotallyuninformative,
i.e., assigns a uniform probability to X.
Suppose that B is a Bayesian network over U that was learned from D, i.e., Θ satisﬁes
Equation 5. The entropy associated with B is simply HPB(U). Applying Equation 1 to
log PB(u), moving the product out of the logarithm, and changing the order of summation,
we derive the equation
HPB(U) = −
X
i
X
xi,Πxi
ˆPD(xi, Πxi) log(θxi|Πxi ) ,
from which it immediately follows that HPB(U) = −1
N LL(B|D), using Equation 4. This
equality has several consequences. First, it implies that −LL(B|D) is the optimal number
of bits needed to describe D, assuming that PB is the distribution from which D is sampled.
This observation justiﬁes the use of the term −LL(B|D) for measuring the representation
of D in the MDL encoding scheme. Second, this equality implies that maximizing the log
likelihood is equivalent to searching for a model that minimizes the entropy, as shown by
Lewis (1959).
This reading suggests that by maximizing the log likelihood we are minimizing the
description of D. Another way of viewing this optimization process is to use cross entropy,
which is also known as the Kullback-Leibler divergence (Kullback & Leibler, 1951). Cross
entropy is a measure of distance between two probability distributions. Formally,
D(P(X)||Q(X)) =
X
x∈Val(X)
P(x) log P(x)
Q(x) .
(A.1)
One information-theoretic interpretation of cross entropy is the average redundancy in-
curred in encoding when we use a wrong probability measure. Roughly speaking, we will
incur an overhead of D(P(X)||Q(X)) per instance in the encoding of samples of P(X) when
we assume that X is distributed according to Q. That is, an encoding of x1, . . . , xm will be

----- Page 31 (native) -----
BAYESIAN NETWORK CLASSIFIERS
161
m(HP (x) + D(P(X)||Q(X))) bits long, mD(P(X)||Q(X)) more than the optimal. Given
this interpretation of cross entropy, it is not surprising that minimizing D( ˆPD(U)||PB(U))
is equivalent to minimizing HPB(U), and thus is also equivalent to maximizing LL(B|D).
We now turn our attention to the structure of the log likelihood term. A measure related
to entropy is the conditional entropy, which measures the entropy of X when we know
the value of Y: HP (X|Y) = −P
y P(y) P
x P(x|y) log P(x|y). In terms of encoding,
HP (X|Y) measures the optimal number of bits needed to encode the value X when the
value of Y is given. Intuitively, knowing the value of Y can only be useful for encoding X
more compactly. Indeed, HP (X|Y) ≤HP (X). The difference between these two values,
called the mutual information between X and Y, measures how much information Y bears
on X. Formally, the mutual information is deﬁned as
IP (X; Y) = HP (X) −HP (X|Y) =
X
x,y
P(x, y) log
P(x, y)
P(x)P(y) .
Applying these deﬁnitions to Equation 4 we immediately derive the equation
LL(B|D) = −N
X
i
H ˆ
PD(Xi|ΠXi) = N
X
i
I ˆ
PD(Xi; ΠXi) −N
X
i
H ˆ
PD(Xi) .
(A.2)
Several observations are in order. First, notice that H ˆ
PD(Xi) is independent of the choice of
B. Thus, to maximize LL(G|D) we must maximize only the ﬁrst term. This representation
providesanappealingintuition, sinceitamountstomaximizingthecorrelationbetweeneach
Xi and its parents. Second, the representation lets us easily prove that complete networks
maximize the log likelihood: if B′ has a superset of the arcs in B, then ΠXi ⊆Π′
Xi for all
i; since I(X; Y) ≤I(X; Y ∪Z), we immediately derive LL(B|D) ≤LL(B′|D).
Notes
1. TAN structures were called “Bayesian conditional trees” by Geiger (1992).
2. An alternative notion of smoothing was investigated by Cestnik (1990) in the context of learning naive Bayesian
classiﬁers.
3. The choice of k = 5 in our k-fold cross validation is based on the recommendations of Kohavi (1995).
References
Binder, J., D. Koller, S. Russell, & K. Kanazawa (1997). Adaptive probabilistic networks with hidden variables.
Machine Learning, this issue.
Bouckaert, R. R. (1994).
Properties of Bayesian network learning algorithms.
In R. L´opez de Mantar´as &
D. Poole (Eds.), Proceedings of the Tenth Conference on Uncertainty in Artiﬁcial Intelligence (pp. 102–109).
San Francisco, CA: Morgan Kaufmann.
Buntine, W. (1991). Theory reﬁnement on Bayesian networks. In B. D. D’Ambrosio, P. Smets, & P. P. Bonissone
(Eds.), Proceedings of the Seventh Annual Conference on Uncertainty Artiﬁcial Intelligence (pp. 52–60). San
Francisco, CA: Morgan Kaufmann.

----- Page 32 (native) -----
162
N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT
Buntine, W. (1996).
A guide to the literature on learning probabilistic networks from data.
IEEE Trans. on
Knowledge and Data Engineering, 8, 195–210.
Cestnik, B. (1990). Estimating probabilities: a crucial task in machine learning. In L. C. Aiello (Ed.), Proceedings
of the 9th European Conference on Artiﬁcial Intelligence (pp. 147–149). London: Pitman.
Chickering, D.M. (1995). Learning Bayesian networks is NP-complete. In D. Fisher & A. Lenz, Learning from
Data. Springer-Verlag.
Chickering, D. M. & D. Heckerman (1996). Efﬁcient approximations for the marginal likelihood of incomplete
data given a Bayesian network. In E. Horvits & F. Jensen (Eds.), Proceedings of the Twelfth Conference on
Uncertainty in Artiﬁcial Intelligence (pp. 158–168). San Francisco, CA: Morgan Kaufmann.
Chow, C. K. & C. N. Liu (1968). Approximating discrete probability distributions with dependence trees. IEEE
Trans. on Info. Theory, 14, 462–467.
Cooper, G. F. & E. Herskovits (1992). A Bayesian method for the induction of probabilistic networks from data.
Machine Learning, 9, 309–347.
Cormen, T. H., C. E. Leiserson, & R. L. Rivest (1990). Introduction to Algorithms. Cambridge, MA: MIT Press.
Cover, T. M. & J. A. Thomas (1991). Elements of Information Theory. New York: John Wiley & Sons.
Dawid, A. P. (1976). Properties of diagnostic data distributions. Biometrics, 32, 647–658.
DeGroot, M. H. (1970). Optimal Statistical Decisions. New York: McGraw-Hill.
Domingos, P. & M. Pazzani (1996). Beyond independence: Conditions for the optimality of the simple Bayesian
classiﬁer. In L. Saitta (Ed.), Proceedings of the Thirteenth International Conference on Machine Learning (pp.
105–112). San Francisco, CA: Morgan Kaufmann.
Dougherty, J., R. Kohavi, & M. Sahami (1995). Supervised and unsupervised discretization of continuous features.
In A. Prieditis & S. Russell (Eds.), Proceedings of the Twelfth International Conference on Machine Learning
(pp. 194–202). San Francisco, CA: Morgan Kaufmann.
Duda, R. O. & P. E. Hart (1973). Pattern Classiﬁcation and Scene Analysis. New York: John Wiley & Sons.
Ezawa, K. J. & T. Schuermann (1995). Fraud/uncollectable debt detection using a Bayesian network based learning
system: A rare binary outcome with mixed data structures. In P. Besnard & S. Hanks (Eds.), Proceedings of
the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence (pp. 157–166). San Francisco, CA: Morgan
Kaufmann.
Fayyad, U. M. & K. B. Irani (1993). Multi-interval discretization of continuous-valued attributes for classiﬁcation
learning. In Proceedings of the Thirteenth International Joint Conference on Artiﬁcial Intelligence (pp. 1022–
1027). San Francisco, CA: Morgan Kaufmann.
Friedman, J. (1997a). On bias, variance, 0/1 - loss, and the curse-of-dimensionality. Data Mining and Knowledge
Discovery, 1, 55–77.
Friedman, N. (1997b).
Learning belief networks in the presence of missing values and hidden variables.
In
D. Fisher (Ed.), Proceedings of the Fourteenth International Conference on Machine Learning (pp. 125–133).
San Francisco, CA: Morgan Kaufmann.
Friedman, N. & M. Goldszmidt (1996a). Building classiﬁers using Bayesian networks. In Proceedings of the
National Conference on Artiﬁcial Intelligence (pp. 1277–1284). Menlo Park, CA: AAAI Press.
Friedman, N. & M. Goldszmidt (1996b). Discretization of continuous attributes while learning Bayesian networks.
In L. Saitta (Ed.), Proceedings of the Thirteenth International Conference on Machine Learning (pp. 157–165).
San Francisco, CA: Morgan Kaufmann.
Friedman, N. & M. Goldszmidt (1996c).
Learning Bayesian networks with local structure.
In E. Horvits &
F. Jensen (Eds.), Proceedings of the Twelfth Conference on Uncertainty in Artiﬁcial Intelligence (pp. 252-262).
San Francisco, CA: Morgan Kaufmann.
Geiger, D. (1992).
An entropy-based learning algorithm of Bayesian conditional trees.
In D. Dubois, M. P.
Wellman, B. D. D’Ambrosio, & P. Smets (Eds.), Proceedings of the Eighth Annual Conference on Uncertainty
Artiﬁcial Intelligence (pp. 92–97). San Francisco, CA: Morgan Kaufmann.
Geiger, D. & D. Heckerman (1996). Knowledge representation and inference in similarity networks and Bayesian
multinets. Artiﬁcial Intelligence, 82, 45–74.
Geiger, D., D. Heckerman, & C. Meek (1996).
Asymptotic model selection for directed graphs with hidden
variables. In E. Horvits & F. Jensen (Eds.), Proceedings of the Twelfth Conference on Uncertainty in Artiﬁcial
Intelligence (pp. 283-290). San Francisco, CA: Morgan Kaufmann.
Heckerman, D. (1991). Probabilistic Similarity Networks. Cambridge, MA: MIT Press.
Heckerman, D. (1995). A tutorial on learning Bayesian networks. Technical Report MSR-TR-95-06, Microsoft
Research.

----- Page 33 (native) -----
BAYESIAN NETWORK CLASSIFIERS
163
Heckerman, D. & D. Geiger (1995).
Learning Bayesian networks: a uniﬁcation for discrete and Gaussian
domains. In P. Besnard & S. Hanks (Eds.), Proceedings of the Eleventh Conference on Uncertainty in Artiﬁcial
Intelligence (pp. 274–284). San Francisco, CA: Morgan Kaufmann.
Heckerman, D., D. Geiger, & D. M. Chickering (1995).
Learning Bayesian networks: The combination of
knowledge and statistical data. Machine Learning, 20, 197–243.
John, G. & R. Kohavi (1997).
Wrappers for feature subset selection.
Artiﬁcial Intelligence.
Accepted
for publication. A preliminary version appears in Proceedings of the Eleventh International Conference on
Machine Learning, 1994, pp. 121–129, under the title “Irrelevant features and the subset selection problem”.
John, G. H. & P. Langley (1995). Estimating continuous distributions in Bayesian classiﬁers. In P. Besnard &
S. Hanks (Eds.), Proceedings of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence (pp. 338–345).
San Francisco, CA: Morgan Kaufmann.
Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. In
Proceedings of the Fourteenth International Joint Conference on Artiﬁcial Intelligence (pp. 1137–1143). San
Francisco, CA: Morgan Kaufmann.
Kohavi, R., G. John, R. Long, D. Manley, & K. Pﬂeger (1994). MLC++: A machine learning library in C++.
In Proc. Sixth International Conference on Tools with Artiﬁcial Intelligence (pp. 740–743). IEEE Computer
Society Press.
Kononenko, I. (1991).
Semi-naive Bayesian classiﬁer.
In Y. Kodratoff (Ed.), Proc. Sixth European Working
Session on Learning (pp. 206–219). Berlin: Springer-Verlag.
Kullback, S. & R. A. Leibler (1951). On information and sufﬁciency. Annals of Mathematical Statistics, 22,
76–86.
Lam, W. & F. Bacchus (1994). Learning Bayesian belief networks. An approach based on the MDL principle.
Computational Intelligence, 10, 269–293.
Langley, P., W. Iba, & K. Thompson (1992). An analysis of Bayesian classiﬁers. In Proceedings, Tenth National
Conference on Artiﬁcial Intelligence (pp. 223–228). Menlo Park, CA: AAAI Press.
Langley, P. & S. Sage (1994). Induction of selective Bayesian classiﬁers. In R. L´opez de Mantar´as & D. Poole
(Eds.), Proceedings of the Tenth Conference on Uncertainty in Artiﬁcial Intelligence (pp. 399–406).
San
Francisco, CA: Morgan Kaufmann.
Lauritzen, S. L. (1995). The EM algorithm for graphical association models with missing data. Computational
Statistics and Data Analysis, 19, 191–201.
Lewis, P. M. (1959). Approximating probability distributions to reduce storage requirements. Information and
Control, 2, 214–225.
Murphy, P. M. & D. W. Aha (1995). UCI repository of machine learning databases. http://www.ics.uci.
edu/~mlearn/MLRepository.html.
Pazzani, M. J. (1995).
Searching for dependencies in Bayesian classiﬁers.
In D. Fisher & H. Lenz (Eds.),
Proceedings of the ﬁfth International Workshop on Artiﬁcial Intelligence and Statistics, Ft. Lauderdale, FL.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. San Francisco, CA: Morgan Kaufmann.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. San Francisco, CA: Morgan Kaufmann.
Ripley, B. D. (1996). Pattern recognition and neural networks. Cambridge: Cambridge University Press.
Rissanen, J. (1978). Modeling by shortest data description. Automatica, 14, 465–471.
Rubin, D. R. (1976). Inference and missing data. Biometrica, 63, 581–592.
Singh, M. & G. M. Provan (1995). A comparison of induction algorithms for selective and non-selective Bayesian
classiﬁers. In A. Prieditis & S. Russell (Eds.), Proceedings of the Twelfth International Conference on Machine
Learning (pp. 497–505). San Francisco, CA: Morgan Kaufmann.
Singh, M. & G. M. Provan (1996). Efﬁcient learning of selective Bayesian network classiﬁers. In L. Saitta (Ed.),
Proceedings of the Thirteenth International Conference on Machine Learning (pp. 453–461). San Francisco,
CA: Morgan Kaufmann.
Spiegelhalter, D. J., A. P. Dawid, S. L. Lauritzen, & R. G. Cowell (1993). Bayesian analysis in expert systems.
Statistical Science, 8, 219–283.
Suzuki, J. (1993). A construction of Bayesian networks from databases based on an MDL scheme. In D. Heck-
erman & A. Mamdani (Eds.), Proceedings of the Ninth Conference on Uncertainty in Artiﬁcial Intelligence (pp.
266–273). San Francisco, CA: Morgan Kaufmann.
Received July 10, 1996
Accepted July 28, 1997
Final Manuscript July 29, 1997