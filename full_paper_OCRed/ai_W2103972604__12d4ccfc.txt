

----- Page 1 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SIAM J. OPTIM.
c
âƒ2010 Society for Industrial and Applied Mathematics
Vol. 20, No. 4, pp. 1956â€“1982
A SINGULAR VALUE THRESHOLDING ALGORITHM FOR
MATRIX COMPLETIONâˆ—
JIAN-FENG CAIâ€ , EMMANUEL J. CAND`ESâ€¡, AND ZUOWEI SHENÂ§
Abstract. This paper introduces a novel algorithm to approximate the matrix with minimum
nuclear norm among all matrices obeying a set of convex constraints.
This problem may be un-
derstood as the convex relaxation of a rank minimization problem and arises in many important
applications as in the task of recovering a large matrix from a small subset of its entries (the famous
Netï¬‚ix problem). Oï¬€-the-shelf algorithms such as interior point methods are not directly amenable
to large problems of this kind with over a million unknown entries. This paper develops a simple
ï¬rst-order and easy-to-implement algorithm that is extremely eï¬ƒcient at addressing problems in
which the optimal solution has low rank. The algorithm is iterative, produces a sequence of matrices
{Xk, Y k}, and at each step mainly performs a soft-thresholding operation on the singular values
of the matrix Y k. There are two remarkable features making this attractive for low-rank matrix
completion problems. The ï¬rst is that the soft-thresholding operation is applied to a sparse matrix;
the second is that the rank of the iterates {Xk} is empirically nondecreasing. Both these facts allow
the algorithm to make use of very minimal storage space and keep the computational cost of each
iteration low. On the theoretical side, we provide a convergence analysis showing that the sequence
of iterates converges. On the practical side, we provide numerical examples in which 1, 000 Ã— 1, 000
matrices are recovered in less than a minute on a modest desktop computer. We also demonstrate
that our approach is amenable to very large scale problems by recovering matrices of rank about
10 with nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are
connected with the recent literature on linearized Bregman iterations for â„“1 minimization, and we
develop a framework in which one can understand these algorithms in terms of well-known Lagrange
multiplier algorithms.
Key words. nuclear norm minimization, matrix completion, singular value thresholding, La-
grange dual function, Uzawaâ€™s algorithm, linearized Bregman iteration
AMS subject classiï¬cations. 90C25, 15A83, 65K05
DOI. 10.1137/080738970
1. Introduction.
1.1. Motivation. There is a rapidly growing interest in the recovery of an un-
known low-rank or approximately low-rank matrix from very limited information.
This problem occurs in many areas of engineering and applied science such as ma-
chine learning [2, 3, 4], control [53], and computer vision; see [61]. As a motivating
example, consider the problem of recovering a data matrix from a sampling of its
entries. This routinely comes up whenever one collects partially ï¬lled out surveys,
and one would like to infer the many missing entries. In the area of recommender
systems, users submit ratings on a subset of entries in a database, and the vendor
provides recommendations based on the userâ€™s preferences. Because users only rate a
âˆ—Received by the editors October 23, 2008; accepted for publication (in revised form) January 5,
2010; published electronically March 3, 2010.
http://www.siam.org/journals/siopt/20-4/73897.html
â€ Department of Mathematics, University of California, Los Angeles, CA 90095 (cai@math.ucla.
edu). This author is supported by the Wavelets and Information Processing Programme under a
grant from DSTA, Singapore.
â€¡Applied and Computational Mathematics, Caltech, Pasadena, CA 91125 (candes@stanford.edu).
This author is partially supported by the Waterman Award from the National Science Foundation
and by ONR grant N00014-08-1-0749.
Â§Department of Mathematics, National University of Singapore, 117543 Singapore (matzuows@
nus.edu.sg). This author is supported in part by grant R-146-000-113-112 from the National Univer-
sity of Singapore.
1956

----- Page 2 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1957
few items, one would like to infer their preference for unrated items; this is the famous
Netï¬‚ix problem [1]. Recovering a rectangular matrix from a sampling of its entries is
known as the matrix completion problem. The issue is, of course, that this problem
is extraordinarily ill posed since with fewer samples than entries we have inï¬nitely
many completions. Therefore, it is apparently impossible to identify which of these
candidate solutions is indeed the â€œcorrectâ€ one without some additional information.
In many instances, however, the matrix we wish to recover has low rank or ap-
proximately low rank. For instance, the Netï¬‚ix data matrix of all user ratings may
be approximately low rank because it is commonly believed that only a few factors
contribute to anyoneâ€™s taste or preference. In computer vision, inferring scene geom-
etry and camera motion from a sequence of images is a well-studied problem known
as the structure-from-motion problem. This is an ill-conditioned problem for objects
that may be distant with respect to their size, or especially for â€œmissing dataâ€ which
occur because of occlusion or tracking failures. However, when properly stacked and
indexed, these images form a matrix which has very low rank (e.g., rank 3 under or-
thography) [24,61]. Other examples of low-rank matrix ï¬tting abound; e.g., in control
(system identiï¬cation), machine learning (multiclass learning), and so on. Having said
this, the premise that the unknown has (approximately) low rank radically changes
the problem, making the search for solutions feasible since the lowest-rank solution
now tends to be the right one.
In a recent paper [16], Cand`es and Recht showed that matrix completion is not
as ill posed as people thought. Indeed, they proved that most low-rank matrices can
be recovered exactly from most sets of sampled entries even though these sets have
surprisingly small cardinality, and more importantly, they proved that this can be
done by solving a simple convex optimization problem. To state their results, suppose
to simplify that the unknown matrix M âˆˆRnÃ—n is square and that one has available
m sampled entries {Mij : (i, j) âˆˆÎ©} where Î© is a random subset of cardinality m.
Then [16] proves that most matrices M of rank r can be perfectly recovered by solving
the optimization problem
(1.1)
minimize
âˆ¥Xâˆ¥âˆ—
subject to
Xij = Mij,
(i, j) âˆˆÎ©,
provided that the number of samples obeys m â‰¥Cn6/5r log n for some positive nu-
merical constant C.1 In (1.1), the functional âˆ¥Xâˆ¥âˆ—is the nuclear norm of the matrix
M, which is the sum of its singular values. The optimization problem (1.1) is convex
and can be recast as a semideï¬nite program [34,35]. In some sense, this is the tightest
convex relaxation of the NP-hard rank minimization problem
(1.2)
minimize
rank(X)
subject to
Xij = Mij,
(i, j) âˆˆÎ©,
since the nuclear ball {X : âˆ¥Xâˆ¥âˆ—â‰¤1} is the convex hull of the set of rank-one
matrices with spectral norm bounded by one. Another interpretation of Cand`es and
Rechtâ€™s result is that under suitable conditions, the rank minimization program (1.2)
and the convex program (1.1) are formally equivalent in the sense that they have
exactly the same unique solution.
1.2. Algorithm outline. Because minimizing the nuclear norm both provably
recovers the lowest-rank matrix subject to constraints (see [56] for related results) and
1Note that an n Ã— n matrix of rank r depends upon r(2n âˆ’r) degrees of freedom.

----- Page 3 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1958
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
gives generally good empirical results in a variety of situations, it is understandably of
great interest to develop numerical methods for solving (1.1). In [16], this optimization
problem was solved using one of the most advanced semideï¬nite programming solvers,
namely, SDPT3 [59]. This solver and others like SeDuMi are based on interior-point
methods and are problematic when the size of the matrix is large because they need
to solve huge systems of linear equations to compute the Newton direction. In fact,
SDPT3 can only handle n Ã— n matrices with n â‰¤100. Presumably, one could resort
to iterative solvers such as the method of conjugate gradients to solve for the Newton
step, but this is problematic as well since it is well known that the condition number
of the Newton system increases rapidly as one gets closer to the solution. In addition,
none of these general purpose solvers use the fact that the solution may have low
rank. We refer the reader to [50] for some recent progress on interior-point methods
concerning some special nuclear norm-minimization problems.
This paper develops the singular value thresholding algorithm for approximately
solving the nuclear norm minimization problem (1.1) and, by extension, problems of
the form
(1.3)
minimize
âˆ¥Xâˆ¥âˆ—
subject to
A(X) = b,
where A is a linear operator acting on the space of n1 Ã—n2 matrices and b âˆˆRm. This
algorithm is a simple ï¬rst-order method and is especially well suited for problems of
very large sizes in which the solution has low rank. We sketch this algorithm in the
special matrix completion setting and let PÎ© be the orthogonal projector onto the
span of matrices vanishing outside of Î© so that the (i, j)th component of PÎ©(X) is
equal to Xij if (i, j) âˆˆÎ© and zero otherwise. Our problem may be expressed as
(1.4)
minimize
âˆ¥Xâˆ¥âˆ—
subject to
PÎ©(X) = PÎ©(M),
with optimization variable X âˆˆRn1Ã—n2. Fix Ï„ > 0 and a sequence {Î´k}kâ‰¥1 of scalar
step sizes. Then starting with Y 0 = 0 âˆˆRn1Ã—n2, the algorithm inductively deï¬nes
(1.5)

Xk = shrink(Y kâˆ’1, Ï„),
Y k = Y kâˆ’1 + Î´kPÎ©(M âˆ’Xk)
until a stopping criterion is reached. In (1.5), shrink(Y , Ï„) is a nonlinear function
which applies a soft-thresholding rule at level Ï„ to the singular values of the input
matrix; see section 2 for details. The key property here is that for large values of Ï„,
the sequence {Xk} converges to a solution which very nearly minimizes (1.4). Hence,
at each step, one only needs to compute at most one singular value decomposition
and perform a few elementary matrix additions. Two important remarks are in order:
1. Sparsity. For each k â‰¥0, Y k vanishes outside of Î© and is, therefore, sparse,
a fact which can be used to evaluate the shrink function rapidly.
2. Low-rank property. The matrices Xk turn out to have low rank, and hence
the algorithm has a minimum storage requirement since we need to keep only
principal factors in memory.
Our numerical experiments demonstrate that the proposed algorithm can solve
problems, in Matlab, involving matrices of size 30,000 Ã— 30,000 having close to a
billion unknowns in 17 minutes on a standard desktop computer with a 1.86 GHz
CPU (dual core with Matlabâ€™s multithreading option enabled) and 3 GB of memory.
As a consequence, the singular value thresholding algorithm may become a rather
powerful computational tool for large scale matrix completion.

----- Page 4 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1959
1.3. General formulation. The singular value thresholding algorithm can be
adapted to deal with other types of convex constraints. For instance, it may address
problems of the form
(1.6)
minimize
âˆ¥Xâˆ¥âˆ—
subject to
fi(X) â‰¤0,
i = 1, . . . , m,
where each fi is a Lipschitz convex function (note that one can handle linear equality
constraints by considering pairs of aï¬ƒne functionals). In the simpler case where the
fiâ€™s are aï¬ƒne functionals, the general algorithm goes through a sequence of iterations
which greatly resemble (1.5). This is useful because this enables the development of
numerical algorithms which are eï¬€ective for recovering matrices from a small subset
of sampled entries possibly contaminated with noise.
1.4. Contents and notations. The rest of the paper is organized as follows.
In section 2, we derive the singular value thresholding (SVT) algorithm for the ma-
trix completion problem and recast it in terms of a well-known Lagrange multiplier
algorithm. In section 3, we extend the SVT algorithm and formulate a general itera-
tion which is applicable to general convex constraints. In section 4, we establish the
convergence results for the iterations given in sections 2 and 3. We demonstrate the
performance and eï¬€ectiveness of the algorithm through numerical examples in sec-
tion 5, and review additional implementation details. Finally, we conclude the paper
with a short discussion in section 6.
Before continuing, we provide here a brief summary of the notations used through-
out the paper. Matrices are bold capital, vectors are bold lowercase, and scalars or
entries are not bold. For instance, X is a matrix and Xij its (i, j)th entry. Likewise,
x is a vector and xi its ith component. The nuclear norm of a matrix is denoted
by âˆ¥Xâˆ¥âˆ—, the Frobenius norm by âˆ¥Xâˆ¥F , and the spectral norm by âˆ¥Xâˆ¥2; note that
these are, respectively, the 1-norm, the 2-norm, and the sup-norm of the vector of
singular values. The adjoint of a matrix X is Xâˆ—and similarly for vectors. The
notation diag(x), where x is a vector, stands for the diagonal matrix with {xi} as
diagonal elements. We denote by âŸ¨X, Y âŸ©= trace(Xâˆ—Y ) the standard inner prod-
uct between two matrices (âˆ¥Xâˆ¥2
F = âŸ¨X, XâŸ©). The Cauchyâ€“Schwarz inequality gives
âŸ¨X, Y âŸ©â‰¤âˆ¥Xâˆ¥Fâˆ¥Y âˆ¥F , and it is well known that we also have âŸ¨X, Y âŸ©â‰¤âˆ¥Xâˆ¥âˆ—âˆ¥Y âˆ¥2
(the spectral and nuclear norms are dual from one another); see, e.g., [16,56].
2. The singular value thresholding algorithm. This section introduces the
singular value thresholding algorithm and discusses some of its basic properties. We
begin with the deï¬nition of a key building block, namely, the singular value thresh-
olding operator.
2.1. The singular value shrinkage operator. Consider the singular value
decomposition (SVD) of a matrix X âˆˆRn1Ã—n2 of rank r
(2.1)
X = UÎ£V âˆ—,
Î£ = diag({Ïƒi}1â‰¤iâ‰¤r),
where U and V are, respectively, n1 Ã— r and n2 Ã— r matrices with orthonormal
columns, and the singular values Ïƒi are positive (unless speciï¬ed otherwise, we will
always assume that the SVD of a matrix is given in the reduced form above). For
each Ï„ â‰¥0, we introduce the soft-thresholding operator DÏ„ deï¬ned as follows:
(2.2)
DÏ„(X) := UDÏ„(Î£)V âˆ—,
DÏ„(Î£) = diag({Ïƒi âˆ’Ï„)+}),

----- Page 5 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1960
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
where t+ is the positive part of t, namely, t+ = max(0, t). In words, this operator
simply applies a soft-thresholding rule to the singular values of X, eï¬€ectively shrinking
these toward zero. This is the reason why we will also refer to this transformation
as the singular value shrinkage operator. Even though the SVD may not be unique,
it is easy to see that the singular value shrinkage operator is well deï¬ned, and we
do not elaborate further on this issue. In some sense, this shrinkage operator is a
straightforward extension of the soft-thresholding rule for scalars and vectors.
In
particular, note that if many of the singular values of X are below the threshold
Ï„, the rank of DÏ„(X) may be considerably lower than that of X, just like the soft-
thresholding rule applied to vectors leads to sparser outputs whenever some entries
of the input are below threshold.
The singular value thresholding operator is the proximity operator associated with
the nuclear norm. Details about the proximity operator can be found in, e.g., [42].
Theorem 2.1.2 For each Ï„ â‰¥0 and Y âˆˆRn1Ã—n2, the singular value shrinkage
operator (2.2) obeys
(2.3)
DÏ„(Y ) = arg min
X
1
2âˆ¥X âˆ’Y âˆ¥2
F + Ï„âˆ¥Xâˆ¥âˆ—.
Proof. Since the function h0(X) := Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥X âˆ’Y âˆ¥2
F is strictly convex, it is
easy to see that there exists a unique minimizer, and we thus need to prove that it is
equal to DÏ„(Y ). To do this, recall the deï¬nition of a subgradient of a convex function
f : Rn1Ã—n2 â†’R. We say that Z is a subgradient of f at X0, denoted Z âˆˆâˆ‚f(X0), if
(2.4)
f(X) â‰¥f(X0) + âŸ¨Z, X âˆ’X0âŸ©
for all X. Now Ë†
X minimizes h0 if and only if 0 is a subgradient of the functional h0
at the point Ë†
X, i.e.,
(2.5)
0 âˆˆË†
X âˆ’Y + Ï„âˆ‚âˆ¥Ë†
Xâˆ¥âˆ—,
where âˆ‚âˆ¥Ë†
Xâˆ¥âˆ—is the set of subgradients of the nuclear norm. Let X âˆˆRn1Ã—n2 be an
arbitrary matrix and UÎ£V âˆ—be its SVD. It is known [16,46,64] that
(2.6) âˆ‚âˆ¥Xâˆ¥âˆ—=

UV âˆ—+ W : W âˆˆRn1Ã—n2,
Uâˆ—W = 0,
W V = 0,
âˆ¥W âˆ¥2 â‰¤1

.
Set Ë†
X := DÏ„(Y ) for short. In order to show that Ë†
X obeys (2.5), decompose the
SVD of Y as Y = U0Î£0V âˆ—
0 +U1Î£1V âˆ—
1 , where U0, V0 (resp., U1, V1) are the singular
vectors associated with singular values greater than Ï„ (resp., smaller than or equal to
Ï„). With these notations, we have Ë†
X = U0(Î£0 âˆ’Ï„I)V âˆ—
0 and, therefore,
Y âˆ’Ë†
X = Ï„(U0V âˆ—
0 + W ),
W = Ï„ âˆ’1U1Î£1V âˆ—
1 .
By deï¬nition, Uâˆ—
0 W = 0, W V0 = 0, and since the diagonal elements of Î£1 have
magnitudes bounded by Ï„, we also have âˆ¥W âˆ¥2 â‰¤1. Hence Y âˆ’Ë†
X âˆˆÏ„âˆ‚âˆ¥Ë†
Xâˆ¥âˆ—, which
concludes the proof.
2One reviewer pointed out that a similar result had been mentioned in a talk given by Donald
Goldfarb at the Foundations of Computational Mathematics conference which took place in Hong
Kong in June 2008.

----- Page 6 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1961
2.2. Shrinkage iterations. We are now in the position to introduce the singular
value thresholding algorithm. Fix Ï„ > 0 and a sequence {Î´k} of positive step sizes.
Starting with Y0, inductively deï¬ne for k = 1, 2, . . . ,
(2.7)

Xk = DÏ„(Y kâˆ’1),
Y k = Y kâˆ’1 + Î´kPÎ©(M âˆ’Xk)
until a stopping criterion is reached (we postpone the discussion of this stopping
criterion and of the choice of step sizes). This shrinkage iteration is very simple to
implement. At each step, we only need to compute an SVD and perform elementary
matrix operations. With the help of a standard numerical linear algebra package, the
whole algorithm can be coded in just a few lines. As we will see later, the iteration (2.7)
is the linearized Bregman iteration, which is a special instance of Uzawaâ€™s algorithm.
Before addressing further computational issues, we would like to make explicit the
relationship between this iteration and the original problem (1.1). In section 4, we
will show that the sequence {Xk} converges to the unique solution of an optimization
problem closely related to (1.1), namely,
(2.8)
minimize
Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥Xâˆ¥2
F
subject to
PÎ©(X) = PÎ©(M).
Furthermore, it is intuitive that the solution to this modiï¬ed problem converges to
that of (1.4) as Ï„ â†’âˆas shown in section 3. Thus by selecting a large value of the
parameter Ï„, the sequence of iterates converges to a matrix which nearly minimizes
(1.1).
As mentioned earlier, there are two crucial properties which make this algorithm
ideally suited for matrix completion.
â€¢ Low-rank property. A remarkable empirical fact is that the matrices in the
sequence {Xk} have low rank (provided, of course, that the solution to (2.8)
has low rank). We use the word â€œempiricalâ€ because all of our numerical ex-
periments have produced low-rank sequences, but we cannot rigorously prove
that this is true in general. The reason for this phenomenon is, however, sim-
ple: Because we are interested in large values of Ï„ (as to better approximate
the solution to (1.1)), the thresholding step happens to â€œkillâ€ most of the
small singular values and produces a low-rank output. In fact, our numerical
results show that the rank of Xk is nondecreasing with k, and the maximum
rank is reached in the last steps of the algorithm; see section 5.
Thus, when the rank of the solution is substantially smaller than either di-
mension of the matrix, the storage requirement is low since we could store
each Xk in its SVD form (note that we need to keep only the current iterate
and may discard earlier values).
â€¢ Sparsity. Another important property of the SVT algorithm is that the it-
eration matrix Y k is sparse. Since Y 0 = 0, we have by induction that Y k
vanishes outside of Î©. The fewer entries available, the sparser Y k. Because
the sparsity pattern Î© is ï¬xed throughout, one can then apply sparse matrix
techniques to save storage. Also, if |Î©| = m, the computational cost of up-
dating Y k is of order m. Moreover, we can call subroutines supporting sparse
matrix computations, which can further reduce computational costs.
One such subroutine is the SVD. However, note that we do not need to com-
pute the entire SVD of Y k to apply the singular value thresholding operator.

----- Page 7 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1962
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
Only the part corresponding to singular values greater than Ï„ is needed.
Hence, a good strategy is to apply the iterative Lanczos algorithm to com-
pute the ï¬rst few singular values and singular vectors. Because Y k is sparse,
Y k can be applied to arbitrary vectors rapidly, and this procedure oï¬€ers a
considerable speedup over naive methods.
2.3. Relation with other works. Our algorithm is inspired by recent work
in the area of â„“1 minimization, and especially by the work on linearized Bregman
iterations for compressed sensing; see [11, 12, 13, 27, 55, 67] for linearized Bregman
iterations and [17, 19, 20, 21, 30] for some information about the ï¬eld of compressed
sensing. In this line of work, linearized Bregman iterations are used to ï¬nd the solution
to an underdetermined system of linear equations with minimum â„“1 norm. In fact,
Theorem 2.1 asserts that the singular value thresholding algorithm can be formulated
as a linearized Bregman iteration. Bregman iterations were ï¬rst introduced in [54]
as a convenient tool for solving computational problems in the imaging sciences, and
a later paper [67] showed that they were useful for solving â„“1-norm minimization
problems in the area of compressed sensing.
Linearized Bregman iterations were
proposed in [27] to improve performance of plain Bregman iterations; see also [67].
Additional details together with a technique for improving the speed of convergence
called kicking are described in [55].
On the practical side, the paper [13] applied
Bregman iterations to solve a deblurring problem, while on the theoretical side, the
references [11,12] gave a rigorous analysis of the convergence of such iterations. New
developments keep on coming out at a rapid pace, and recently, [39] introduced a
new iteration, the split Bregman iteration, to extend Bregman-type iterations (such
as linearized Bregman iterations) to problems involving the minimization of â„“1-like
functionals such as total-variation norms, Besov norms, and so forth.
When applied to â„“1-minimization problems, linearized Bregman iterations are se-
quences of soft-thresholding rules operating on vectors. Iterative soft-thresholding
algorithms in connection with â„“1 or total-variation minimization have quite a bit of
history in signal and image processing, and we would like to mention the works [14,48]
for total-variation minimization, [28,29,36] for â„“1 minimization, and [5,9,10,22,23,32,
33,58] for some recent applications in the area of image inpainting and image restora-
tion. Just as iterative soft-thresholding methods are designed to ï¬nd sparse solutions,
our iterative singular value thresholding scheme is designed to ï¬nd a sparse vector of
singular values. In classical problems arising in the areas of compressed sensing, and
signal or image processing, the sparsity is expressed in a known transformed domain
and soft-thresholding is applied to transformed coeï¬ƒcients. In contrast, the shrinkage
operator DÏ„ is adaptive. The SVT not only discovers a sparse singular vector but also
the bases in which we have a sparse representation. In this sense, the SVT algorithm
is an extension of earlier iterative soft-thresholding schemes.
Finally, we would like to contrast the SVT iteration (2.7) with the popular it-
erative soft-thresholding algorithm used in many papers in imaging processing and
perhaps best known under the name of proximal forward-backward splitting method
(PFBS); see [10, 26, 28, 36, 40, 62, 63], for example.
The constrained minimization
problem (1.4) may be relaxed into
(2.9)
minimize
Î»âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥PÎ©(X) âˆ’PÎ©(M)âˆ¥2
F
for some Î» > 0. Theorem 2.1 asserts that DÎ» is the proximity operator of Î»âˆ¥Xâˆ¥âˆ—
and Proposition 3.1(iii) in [26] gives that the solution to this unconstrained problem is

----- Page 8 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1963
characterized by the ï¬xed point equation X = DÎ»Î´(X +Î´PÎ©(M âˆ’X)) for each Î´ > 0.
One can then apply a simpliï¬ed version of the PFBS method (see (3.6) in [26]) to
obtain iterations of the form Xk = DÎ»Î´kâˆ’1(Xkâˆ’1+Î´kâˆ’1PÎ©(M âˆ’Xkâˆ’1)). Introducing
an intermediate matrix Y k, this algorithm may be expressed as
(2.10)

Xk = DÎ»Î´kâˆ’1(Y kâˆ’1),
Y k = Xk + Î´kPÎ©(M âˆ’Xk).
The diï¬€erence with (2.7) may seem subtle at ï¬rstâ€”replacing Xk in (2.10) with Y kâˆ’1
and setting Î´k = Î´ gives (2.7) with Ï„ = Î»Î´â€”but has enormous consequences as
this gives entirely diï¬€erent algorithms. First, they have diï¬€erent limits: While (2.7)
converges to the solution of the constrained minimization (2.8), (2.10) converges to
the solution of (2.9) provided that the sequence of step sizes is appropriately selected.
Second, selecting a large Î» (or a large value of Ï„ = Î»Î´) in (2.10) gives a low-rank
sequence of iterates and a limit with small nuclear norm. The limit, however, does
not ï¬t the data, and this is why one has to choose a small or moderate value of Î»
(or of Ï„ = Î»Î´). However, when Î» is not suï¬ƒciently large, the Xkâ€™s may not have
low rank even though the solution has low rank (and one may need to compute many
singular vectors), and thus applying the shrinkage operation accurately to Y k may be
computationally very expensive. Moreover, the limit does not necessary have a small
nuclear norm. These are some of the reasons why (2.10) does not seem to be very
suitable for very large scale matrix completion problems (in cases where computing
the SVD is prohibitive). Since the original submission of this paper, however, we note
that several papers proposed some working implementations [51,60].
2.4. Interpretation as a Lagrange multiplier method. In this section, we
recast the SVT algorithm as a type of Lagrange multiplier algorithm known as Uzawaâ€™s
algorithm. An important consequence is that this will allow us to extend the SVT
algorithm to other problems involving the minimization of the nuclear norm under
convex constraints; see section 3. Further, another contribution of this paper is that
this framework actually recasts linear Bregman iterations as a very special form of
Uzawaâ€™s algorithm, hence providing fresh and clear insights about these iterations.
In what follows, we set fÏ„(X) = Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥Xâˆ¥2
F for some ï¬xed Ï„ > 0 and recall
that we wish to solve (2.8)
minimize
fÏ„(X)
subject to
PÎ©(X) = PÎ©(M).
The Lagrangian for this problem is given by L(X, Y ) = fÏ„(X) + âŸ¨Y , PÎ©(M âˆ’X)âŸ©,
where Y âˆˆRn1Ã—n2. Strong duality holds and Xâ‹†and Y â‹†are primal-dual optimal if
(Xâ‹†, Y â‹†) is a saddlepoint of the Lagrangian L(X, Y ), i.e., a pair obeying
(2.11)
sup
Y
inf
X L(X, Y ) = L(Xâ‹†, Y â‹†) = inf
X sup
Y
L(X, Y ).
The function g0(Y ) = infX L(X, Y ) is called the dual function. Here, g0 is continu-
ously diï¬€erentiable and has a gradient which is Lipschitz with Lipschitz constant at
most one, as this is a consequence of well-known results concerning conjugate func-
tions. Uzawaâ€™s algorithm approaches the problem of ï¬nding a saddlepoint with an
iterative procedure. From Y0 = 0, say, inductively deï¬ne
(2.12)

L(Xk, Y kâˆ’1) = minX L(X, Y kâˆ’1),
Y k = Y kâˆ’1 + Î´kPÎ©(M âˆ’Xk),

----- Page 9 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1964
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
where {Î´k}kâ‰¥1 is a sequence of positive step sizes. Uzawaâ€™s algorithm is, in fact, a
subgradient method applied to the dual problem, where each step moves the current
iterate in the direction of the gradient or of a subgradient. Indeed, observe that the
gradient of g0(Y ) is given by
(2.13)
âˆ‚Y g0(Y ) = âˆ‚Y L( Ëœ
X, Y ) = PÎ©(M âˆ’Ëœ
X),
where Ëœ
X is the minimizer of the Lagrangian for that value of Y so that a gradient
descent update for Y is of the form
Y k = Y kâˆ’1 + Î´kâˆ‚Y g0(Y kâˆ’1) = Y kâˆ’1 + Î´kPÎ©(M âˆ’Xk).
It remains to compute the minimizer of the Lagrangian (2.12), and note that
(2.14)
arg min fÏ„(X) + âŸ¨Y , PÎ©(M âˆ’X)âŸ©= arg min Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥X âˆ’PÎ©Y âˆ¥2
F.
However, we know that the minimizer is given by DÏ„(PÎ©(Y )), and since Y k = PÎ©(Y k)
for all k â‰¥0, Uzawaâ€™s algorithm takes the form

Xk = DÏ„(Y kâˆ’1),
Y k = Y kâˆ’1 + Î´kPÎ©(M âˆ’Xk),
which is exactly the update (2.7). This point of view brings to bear many diï¬€erent
mathematical tools for proving the convergence of the singular value thresholding
iterations. For an early use of Uzawaâ€™s algorithm minimizing an â„“1-like functional,
the total-variation norm, under linear inequality constraints, see [14].
3. General formulation. This section presents a general formulation of the
SVT algorithm for approximately minimizing the nuclear norm of a matrix under
convex constraints.
3.1. Linear equality constraints. Set the objective functional fÏ„(X) =
Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥Xâˆ¥2
F for some ï¬xed Ï„ > 0, and consider the following optimization prob-
lem:
(3.1)
minimize
fÏ„(X)
subject to
A(X) = b,
where A is a linear transformation mapping n1 Ã— n2 matrices into Rm (Aâˆ—is the
adjoint of A).
This more general formulation is considered in [16] and [56] as an
extension of the matrix completion problem. Then the Lagrangian for this problem
is of the form
(3.2)
L(X, y) = fÏ„(X) + âŸ¨y, b âˆ’A(X)âŸ©,
where X âˆˆRn1Ã—n2 and y âˆˆRm, and starting with y0 = 0, Uzawaâ€™s iteration is given
by
(3.3)

Xk = DÏ„(Aâˆ—(ykâˆ’1)),
yk = ykâˆ’1 + Î´k(b âˆ’A(Xk)).
The iteration (3.3) is, of course, the same as (2.7) in the case where A is a sampling
operator extracting m entries with indices in Î© out of an n1Ã—n2 matrix. To verify this
claim, observe that in this situation, Aâˆ—A = PÎ©, and let M be any matrix obeying
A(M) = b. Then deï¬ning Y k = Aâˆ—(yk) and substituting this expression in (3.3)
gives (2.7).

----- Page 10 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1965
3.2. General convex constraints. One can also adapt the algorithm to handle
general convex constraints. Suppose we wish to minimize fÏ„(X) deï¬ned as before
over a convex set X âˆˆC. To simplify, we will assume that this convex set is given by
C = {X : fi(X) â‰¤0 âˆ€i = 1, . . . , m}, where the fiâ€™s are convex functionals (note that
one can handle linear equality constraints by considering pairs of aï¬ƒne functionals).
The problem of interest is then of the form
(3.4)
minimize
fÏ„(X)
subject to
fi(X) â‰¤0,
i = 1, . . . , m.
Just as before, it is intuitive that as Ï„ â†’âˆ, the solution to this problem converges
to a minimizer of the nuclear norm under the same constraints (1.6) as shown in
Theorem 3.1 at the end of this section.
Put F(X) := (f1(X), . . . , fm(X)) for short. Then the Lagrangian for (3.4) is
equal to L(X, y) = fÏ„(X) + âŸ¨y, F(X)âŸ©, where X âˆˆRn1Ã—n2 and y âˆˆRm is now a
vector with nonnegative components denoted, as usual, by y â‰¥0. One can apply
Uzawaâ€™s method just as before with the only modiï¬cation that we will use a subgra-
dient method with projection to maximize the dual function since we need to make
sure that the successive updates yk belong to the nonnegative orthant. This gives
(3.5)

Xk = arg min {fÏ„(X) + âŸ¨ykâˆ’1, F(X)âŸ©},
yk = [ykâˆ’1 + Î´kF(Xk)]+.
Above, x+ is of course the vector with entries equal to max(xi, 0). When F is an
aï¬ƒne mapping of the form b âˆ’A(X), this simpliï¬es to
(3.6)

Xk = DÏ„(Aâˆ—(ykâˆ’1)),
yk = [ykâˆ’1 + Î´k(b âˆ’A(Xk))]+,
and thus the extension to linear inequality constraints is straightforward.
3.3. Examples. Suppose we have available linear measurements b about a ma-
trix M, which take the form
(3.7)
b = A(M) + z,
where z âˆˆRm is a noise vector. Then under these circumstances, one might want
to ï¬nd the matrix which minimizes the nuclear norm among all matrices which are
consistent with the data b.
3.3.1. Linear inequality constraints. A possible approach to this problem
consists in solving
(3.8)
minimize
âˆ¥Xâˆ¥âˆ—
subject to
|vec(Aâˆ—(r))| â‰¤vec(E),
r := b âˆ’A(X),
where E is an array of tolerances, which is adjusted to ï¬t the noise statistics. Above,
vec(A) â‰¤vec(B), for any two matrices A and B, means componentwise inequalities;
that is, Aij â‰¤Bij for all indices i, j. We use this notation so as not to confuse the
reader with the positive semideï¬nite ordering. In the case of the matrix completion
problem where A extracts sampled entries indexed by Î©, one can always see the

----- Page 11 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1966
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
data vector as the sampled entries of some matrix B obeying PÎ©(B) = Aâˆ—(b); the
constraint is then natural for it may be expressed as
|Bij âˆ’Xij| â‰¤Eij,
(i, j) âˆˆÎ©.
If z is white noise with standard deviation Ïƒ, one may want to use a multiple of Ïƒ
for Eij. In words, we are looking for a matrix with minimum nuclear norm under the
constraint that all of its sampled entries do not deviate too much from what has been
observed.
Let Y+ âˆˆRn1Ã—n2 (resp., Yâˆ’âˆˆRn1Ã—n2) be the Lagrange multiplier associated
with the componentwise linear inequality constraints vec(Aâˆ—(r)) â‰¤vec(E) (resp.,
âˆ’vec(Aâˆ—(r)) â‰¤vec(E)). Then starting with Y 0
Â± = 0, the SVT iteration for this
problem is of the form
(3.9)

Xk = DÏ„(Aâˆ—A(Y kâˆ’1
+
âˆ’Y kâˆ’1
âˆ’
)),
Y k
Â± = [Y kâˆ’1
Â±
+ Î´k(Â±Aâˆ—(rk) âˆ’E)]+,
rk = bk âˆ’A(Xk),
where again [Â·]+ is applied componentwise (in the matrix completion problem, Aâˆ—A =
PÎ©).
3.3.2. Quadratic constraints. Another natural solution is to solve the quadra-
tically constrained nuclear norm minimization problem
(3.10)
minimize
âˆ¥Xâˆ¥âˆ—
subject to
âˆ¥b âˆ’A(X)âˆ¥â‰¤Ïµ.
When z is a stochastic error term, Ïµ would typically be adjusted to depend on the
noise power.
To see how we can adapt our ideas in this setting, we work with the approximate
objective functional Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥Xâˆ¥2
F as before, and rewrite our program in the conic
form
(3.11)
minimize
Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥Xâˆ¥2
F
subject to
b âˆ’A(X)
Ïµ

âˆˆK,
where K is the second-order cone K = {(x, t) âˆˆRm+1 : âˆ¥xâˆ¥â‰¤t}. This model has
also been considered in [49]. The cone K is self-dual. The Lagrangian is then given
by
L(X; y, s) = Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥Xâˆ¥2
F + âŸ¨y, b âˆ’A(X)âŸ©âˆ’sÏµ,
where (y, s) âˆˆRm+1 âˆˆKâˆ—= K; that is, âˆ¥yâˆ¥â‰¤s. Letting PK be the orthogonal
projection onto K, this leads to the simple iteration
(3.12)
â§
âª
â¨
âª
â©
Xk = DÏ„(Aâˆ—(yk)),

yk
sk

= PK

ykâˆ’1
skâˆ’1

+ Î´k

b âˆ’A(Xk)
âˆ’Ïµ

.
This is an explicit algorithm since the projection is given by (see also [37, Proposition
3.3])
PK : (x, t) â†’
â§
âª
â¨
âª
â©
(x, t),
âˆ¥xâˆ¥â‰¤t,
âˆ¥xâˆ¥+t
2âˆ¥xâˆ¥(x, âˆ¥xâˆ¥),
âˆ’âˆ¥xâˆ¥â‰¤t â‰¤âˆ¥xâˆ¥,
(0, 0),
t â‰¤âˆ’âˆ¥xâˆ¥.

----- Page 12 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1967
3.3.3. General conic constraints. Clearly, one could apply this methodology
with general cone constraints of the form F(X) + d âˆˆK, where K is some closed and
pointed convex cone. Inspired by the work on the Dantzig selector [18], which was
originally developed for estimating sparse parameter vectors from noisy data, another
approach is to set a constraint on the spectral norm of Aâˆ—(r)â€”recall that r is the
residual vector b âˆ’A(X)â€”and solve
(3.13)
minimize
âˆ¥Xâˆ¥âˆ—
subject to
âˆ¥Aâˆ—(r)âˆ¥â‰¤Ïµ.
Developing our approach in this setting is straightforward and involves projections of
the dual variable onto the positive semideï¬nite cone.
3.4. When the proximal problem gets close. We now show that minimizing
the proximal objective fÏ„(X) = Ï„âˆ¥Xâˆ¥âˆ—+ 1
2âˆ¥Xâˆ¥2
F is the same as minimizing the
nuclear norm in the limit of large Ï„â€™s. The theorem below is general and covers the
special case of linear equality constraints as in (2.8).
Theorem 3.1. Let Xâ‹†
Ï„ be the solution to (3.4) and Xâˆbe the minimum Frobe-
nius norm solution to (1.6) deï¬ned as
(3.14)
Xâˆ:= arg min
X {âˆ¥Xâˆ¥2
F : X is a solution of (1.6)}.
Assume that the fi(X)â€™s, 1 â‰¤i â‰¤m, are convex and lower semicontinuous. Then
(3.15)
lim
Ï„â†’âˆâˆ¥Xâ‹†
Ï„ âˆ’Xâˆâˆ¥F = 0.
Proof. It follows from the deï¬nition of Xâ‹†
Ï„ and Xâˆthat
(3.16)
âˆ¥Xâ‹†
Ï„ âˆ¥âˆ—+ 1
2Ï„ âˆ¥Xâ‹†
Ï„ âˆ¥2
F â‰¤âˆ¥Xâˆâˆ¥âˆ—+ 1
2Ï„ âˆ¥Xâˆâˆ¥2
F ,
and
âˆ¥Xâˆâˆ¥âˆ—â‰¤âˆ¥Xâ‹†
Ï„ âˆ¥âˆ—.
Summing these two inequalities gives
(3.17)
âˆ¥Xâ‹†
Ï„ âˆ¥2
F â‰¤âˆ¥Xâˆâˆ¥2
F ,
which implies that âˆ¥Xâ‹†
Ï„âˆ¥2
F is bounded uniformly in Ï„. Thus, we would prove the the-
orem if we could establish that any convergent subsequence {Xâ‹†
Ï„k}kâ‰¥1 must converge
to Xâˆ.
Consider an arbitrary converging subsequence {Xâ‹†
Ï„k} and set Xc := limkâ†’âˆXâ‹†
Ï„k.
Since for each 1 â‰¤i â‰¤m, fi(Xâ‹†
Ï„k) â‰¤0 and fi is lower semicontinuous, Xc obeys
fi(Xc) â‰¤0 for i = 1, . . . , m. Furthermore, since âˆ¥Xâ‹†
Ï„ âˆ¥2
F is bounded, (3.16) yields
lim sup
Ï„â†’âˆâˆ¥Xâ‹†
Ï„âˆ¥âˆ—â‰¤âˆ¥Xâˆâˆ¥âˆ—,
âˆ¥Xâˆâˆ¥âˆ—â‰¤lim inf
Ï„â†’âˆâˆ¥Xâ‹†
Ï„ âˆ¥âˆ—.
An immediate consequence is limÏ„â†’âˆâˆ¥Xâ‹†
Ï„ âˆ¥âˆ—= âˆ¥Xâˆâˆ¥âˆ—and, therefore, âˆ¥Xcâˆ¥âˆ—=
âˆ¥Xâˆâˆ¥âˆ—. This shows that Xc is a solution to (1.1). Now it follows from the deï¬ni-
tion of Xâˆthat âˆ¥Xcâˆ¥F â‰¥âˆ¥Xâˆâˆ¥F, while we also have âˆ¥Xcâˆ¥F â‰¤âˆ¥Xâˆâˆ¥F because
of (3.17). We conclude that âˆ¥Xcâˆ¥F = âˆ¥Xâˆâˆ¥F and thus Xc = Xâˆsince Xâˆis
unique.

----- Page 13 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1968
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
4. Convergence analysis. This section establishes the convergence of the SVT
iterations. We begin with the simpler proof of the convergence of (2.7) in the special
case of the matrix completion problem, and then present the argument for the more
general constraints (3.5). We hope that this progression will make the second and
more general proof more transparent. We have seen that SVT iterations are projected
gradient-descent algorithms applied to the dual problems. The convergence of pro-
jected gradient-descent algorithms has been well studied; see [6,25,38,43,45,52,68],
for example.
4.1. Convergence for matrix completion. We begin by recording a lemma
which establishes the strong convexity of the objective fÏ„.
Lemma 4.1. Let Z âˆˆâˆ‚fÏ„(X) and Zâ€² âˆˆâˆ‚fÏ„(Xâ€²). Then
(4.1)
âŸ¨Z âˆ’Zâ€², X âˆ’Xâ€²âŸ©â‰¥âˆ¥X âˆ’Xâ€²âˆ¥2
F.
The proof may be found in [57, page 240], but we sketch it for convenience. We
have Z âˆˆâˆ‚fÏ„(X) if and only if Z = Ï„Z0 + X, where Z0 âˆˆâˆ‚âˆ¥Xâˆ¥âˆ—. This gives
âŸ¨Z âˆ’Zâ€², X âˆ’Xâ€²âŸ©= Ï„ âŸ¨Z0 âˆ’Zâ€²
0, X âˆ’Xâ€²âŸ©+ âˆ¥X âˆ’Xâ€²âˆ¥2
F,
and it suï¬ƒces to show that âŸ¨Z0 âˆ’Zâ€²
0, X âˆ’Xâ€²âŸ©â‰¥0. From (2.6), we have that any
subgradient of the nuclear norm at X obeys âˆ¥Z0âˆ¥2 â‰¤1 and âŸ¨Z0, XâŸ©= âˆ¥Xâˆ¥âˆ—. In
particular, this gives |âŸ¨Z0, Xâ€²âŸ©| â‰¤âˆ¥Z0âˆ¥2âˆ¥Xâ€²âˆ¥âˆ—â‰¤âˆ¥Xâ€²âˆ¥âˆ—and, likewise, |âŸ¨Zâ€²
0, XâŸ©| â‰¤
âˆ¥Xâˆ¥âˆ—. Then the lemma follows from
âŸ¨Z0 âˆ’Zâ€²
0, X âˆ’Xâ€²âŸ©= âŸ¨Z0, XâŸ©+ âŸ¨Zâ€²
0, Xâ€²âŸ©âˆ’âŸ¨Z0, Xâ€²âŸ©âˆ’âŸ¨Zâ€²
0, XâŸ©
= âˆ¥Xâˆ¥âˆ—+ âˆ¥Xâ€²âˆ¥âˆ—âˆ’âŸ¨Z0, Xâ€²âŸ©âˆ’âŸ¨Zâ€²
0, XâŸ©â‰¥0.
This lemma is key in showing that the SVT algorithm (2.7) converges. Indeed,
applying [25, Theorem 2.1] gives the theorem below.
Theorem 4.2. Suppose the step sizes obey 0 < inf Î´k â‰¤sup Î´k < 2/âˆ¥Aâˆ¥2. Then
the sequence {Xk} obtained via (3.3) converges to the unique solution to (3.1). In
particular, the sequence {Xk} obtained via (2.7) converges to the unique solution of
(2.8) provided that 0 < inf Î´k â‰¤sup Î´k < 2.
4.2. General convergence theorem. Our second result is more general and
establishes the convergence of the SVT iterations to the solution of (3.4) under general
convex constraints. From now on, we will assume that the function F(X) is Lipschitz
only in the sense that
(4.2)
âˆ¥F(X) âˆ’F(Y âˆ¥â‰¤L(F)âˆ¥X âˆ’Y âˆ¥F ,
for some nonnegative constant L(F). Note that if F is aï¬ƒne, F(X) = b âˆ’A(X),
we have L(F) = âˆ¥Aâˆ¥2 where âˆ¥Aâˆ¥2 is the spectrum norm of the linear transformation
A deï¬ned as âˆ¥Aâˆ¥2 := sup{âˆ¥A(X)âˆ¥â„“2 : âˆ¥Xâˆ¥F = 1}. We also recall that F(X) =
(f1(X), . . . , fm(X)) where each fi is convex and that the Lagrangian for the problem
(3.4) is given by
L(X, y) = fÏ„(X) + âŸ¨y, F(X)âŸ©,
y â‰¥0.
To simplify, we will assume that strong duality holds which is automatically true if
the constraints obey constraint qualiï¬cations such as Slaterâ€™s condition [7].

----- Page 14 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1969
We ï¬rst establish a preparatory lemma, whose proof can be found in [31].
Lemma 4.3. Let (Xâ‹†, yâ‹†) be a primal-dual optimal pair for (3.4). Then for each
Î´ > 0, yâ‹†obeys
(4.3)
yâ‹†= [yâ‹†+ Î´F(Xâ‹†)]+.
We are now in the position to state our general convergence result; see also [25,
Theorem 2.1].
Theorem 4.4. Suppose that the step sizes obey 0 < inf Î´k â‰¤sup Î´k < 2/âˆ¥L(F)âˆ¥2,
where L(F) is the Lipschitz constant in (4.2).
Then assuming strong duality, the
sequence {Xk} obtained via (3.5) converges to the unique solution of (3.4).
Proof. Let (Xâ‹†, yâ‹†) be primal-dual optimal for the problem (3.4). We claim that
the optimality conditions give that for all X
âŸ¨Zk, X âˆ’XkâŸ©+ âŸ¨ykâˆ’1, F(X) âˆ’F(Xk)âŸ©â‰¥0,
âŸ¨Zâ‹†, X âˆ’Xâ‹†âŸ©+ âŸ¨yâ‹†, F(X) âˆ’F(Xâ‹†)âŸ©â‰¥0,
(4.4)
for some Zk âˆˆâˆ‚fÏ„(Xk) and some Zâ‹†âˆˆâˆ‚fÏ„(Xâ‹†).
We justify this assertion by
proving one of the two inequalities since the other is exactly similar. For the ï¬rst,
Xk minimizes L(X, ykâˆ’1) over all X and, therefore, there exist Zk âˆˆâˆ‚fÏ„(Xk) and
Zk
i âˆˆâˆ‚fi(Xk), 1 â‰¤i â‰¤m, such that
Zk +
m

i=1
ykâˆ’1
i
Zk
i = 0.
Now because each fi is convex,
fi(X) âˆ’fi(Xk) â‰¥âŸ¨Zk
i , X âˆ’XkâŸ©
and, therefore,
âŸ¨Zk, X âˆ’XkâŸ©+
m

i=1
ykâˆ’1
i
(fi(X) âˆ’fi(Xk)) â‰¥

Zk +
m

i=1
ykâˆ’1
i
Zk
i , X âˆ’Xk

= 0.
This is (4.4).
Now write the ï¬rst inequality in (4.4) for Xâ‹†, write the second for Xk, and sum
the two inequalities. This gives
âŸ¨Zk âˆ’Zâ‹†, Xk âˆ’Xâ‹†âŸ©+ âŸ¨ykâˆ’1 âˆ’yâ‹†, F(Xk) âˆ’F(Xâ‹†)âŸ©â‰¤0.
It follows from Lemma 4.1 that
(4.5)
âŸ¨ykâˆ’1 âˆ’yâ‹†, F(Xk) âˆ’F(Xâ‹†)âŸ©â‰¤âˆ’âŸ¨Zk âˆ’Zâ‹†, Xk âˆ’Xâ‹†âŸ©â‰¤âˆ’âˆ¥Xk âˆ’Xâ‹†âˆ¥2
F .
We continue and observe that because yâ‹†= [yâ‹†+ Î´kF(X)]+ by Lemma 4.3, we have
âˆ¥yk âˆ’yâ‹†âˆ¥= âˆ¥[ykâˆ’1 + Î´kF(Xk)]+ âˆ’[yâ‹†+ Î´kF(Xâ‹†)]+âˆ¥
â‰¤âˆ¥ykâˆ’1 âˆ’yâ‹†+ Î´k(F(Xk) âˆ’F(Xâ‹†))âˆ¥
since the projection onto the convex set Rm
+ is a contraction. Therefore,
âˆ¥yk âˆ’yâ‹†âˆ¥2 = âˆ¥ykâˆ’1 âˆ’yâ‹†âˆ¥2 + 2Î´k âŸ¨ykâˆ’1 âˆ’yâ‹†, F(Xk) âˆ’F(Xâ‹†)âŸ©
+ Î´2
kâˆ¥F(Xk) âˆ’F(Xâ‹†)âˆ¥2
â‰¤âˆ¥ykâˆ’1 âˆ’yâ‹†âˆ¥2 âˆ’2Î´kâˆ¥Xk âˆ’Xâ‹†âˆ¥2
F + Î´2
kL2 âˆ¥Xk âˆ’Xâ‹†âˆ¥2
F,

----- Page 15 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1970
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
where we have put L instead of L(F) for short. Under our assumptions about the
size of Î´k, we have 2Î´k âˆ’Î´2
kL2 â‰¥Î² for all k â‰¥1 and some Î² > 0. Then
(4.6)
âˆ¥yk âˆ’yâ‹†âˆ¥2 â‰¤âˆ¥ykâˆ’1 âˆ’yâ‹†âˆ¥2 âˆ’Î²âˆ¥Xk âˆ’Xâ‹†âˆ¥2
F ,
and the conclusion is as before.
5. Implementation and numerical results. This section provides implemen-
tation details of the SVT algorithmâ€”as to make it practically eï¬€ective for matrix
completionâ€”such as the numerical evaluation of the singular value thresholding oper-
ator, the selection of the step size Î´k, the selection of a stopping criterion, and so on.
This section also introduces several numerical simulation results which demonstrate
the performance and eï¬€ectiveness of the SVT algorithm. We show that 30,000Ã—30,000
matrices of rank 10 are recovered from just about 0.4% of their sampled entries in a
matter of a few minutes on a modest desktop computer with a 1.86 GHz CPU (dual
core with Matlabâ€™s multithreading option enabled) and 3 GB of memory.
5.1. Implementation details.
5.1.1. Evaluation of the singular value thresholding operator. To apply
the singular value thresholding operator at level Ï„ to an input matrix, it suï¬ƒces to
know those singular values and corresponding singular vectors above the threshold Ï„.
In the matrix completion problem, the singular value thresholding operator is applied
to sparse matrices {Y k} since the number of sampled entries is typically much lower
than the number of entries in the unknown matrix M, and we are hence interested in
numerical methods for computing the dominant singular values and singular vectors
of large sparse matrices. The development of such methods is a relatively mature
area in scientiï¬c computing and numerical linear algebra in particular. In fact, many
high-quality packages are readily available.
Our implementation uses PROPACK;
see [44] for documentation and availability. One reason for this choice is convenience:
PROPACK comes in a Matlab and a Fortran version, and we ï¬nd it convenient to use
the well-documented Matlab version. More importantly, PROPACK uses the iterative
Lanczos algorithm to compute the singular values and singular vectors directly, by
using the Lanczos bidiagonalization algorithm with partial reorthogonalization. In
particular, PROPACK does not compute the eigenvalues and eigenvectors of (Y k)âˆ—Y k
and Y k(Y k)âˆ—, or of an augmented matrix as in the Matlab built-in function â€œsvds,â€
for example.
Consequently, PROPACK is an eï¬ƒcientâ€”in terms of both number
of ï¬‚ops and storage requirementâ€”and stable package for computing the dominant
singular values and singular vectors of a large sparse matrix. For information, the
available documentation [44] reports a speedup factor of about ten over Matlabâ€™s
â€œsvds.â€ Furthermore, the Fortran version of PROPACK is about 3 to 4 times faster
than the Matlab version. Despite this signiï¬cant speedup, we have used only the
Matlab version, but since the singular value shrinkage operator is by and large the
dominant cost in the SVT algorithm, we expect that a Fortran implementation would
run about 3 to 4 times faster.
As for most SVD packages, though one can specify the number of singular values
to compute, PROPACK cannot automatically compute only those singular values ex-
ceeding the threshold Ï„. One must instead specify the number s of singular values
ahead of time, and the software will compute the s largest singular values and corre-
sponding singular vectors. To use this package, we must then determine the number
sk of singular values of Y kâˆ’1 to be computed at the kth iteration. We use the fol-
lowing simple method. Let rkâˆ’1 = rank(Xkâˆ’1) be the number of nonzero singular

----- Page 16 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1971
values of Xkâˆ’1 at the previous iteration. Set sk = rkâˆ’1 + 1 and compute the ï¬rst sk
singular values of Y kâˆ’1. If some of the computed singular values are already smaller
than Ï„, then sk is the right choice. Otherwise, increment sk by a predeï¬ned integer
â„“repeatedly until some of the singular values fall below Ï„. In the experiments, we
choose â„“= 5. Another rule might be to repeatedly multiply sk by a positive numberâ€”
e.g., 2â€”until our criterion is met. Incrementing sk by a ï¬xed integer works very well
in practice; in our experiments, we very rarely need more than one update.
We note that it is not necessary to rerun the Lanczos iterations for the ï¬rst
sk vectors since they have already been computed; only a few new singular values
(â„“of them) need to be numerically evaluated. This can be done by modifying the
PROPACK routines. We have not yet modiï¬ed PROPACK, however. Had we done
so, our run times would be decreased.
5.1.2. Step sizes. There is a large literature on ways of selecting a step size,
but for simplicity, we shall use step sizes that are independent of the iteration count;
that is, Î´k = Î´ for k = 1, 2, . . .. From Theorem 4.2, convergence for the completion
problem is guaranteed (2.7) provided that 0 < Î´ < 2. This choice is, however, too
conservative and the convergence is typically slow. In our experiments, we use instead
(5.1)
Î´ = 1.2 n1n2
m ,
i.e., 1.2 times the undersampling ratio. We give a heuristic justiï¬cation below.
Consider a ï¬xed matrix A âˆˆRn1Ã—n2. Under the assumption that the column and
row spaces of A are not well aligned with the vectors taken from the canonical basis of
Rn1 and Rn2, respectivelyâ€”the incoherence assumption in [16]â€”then with very large
probability over the choices of Î©, we have
(5.2)
(1 âˆ’Ïµ)p âˆ¥Aâˆ¥2
F â‰¤âˆ¥PÎ©(A)âˆ¥2
F â‰¤(1 + Ïµ)p âˆ¥Aâˆ¥2
F,
p := m/(n1n2),
provided that the rank of A is not too large. The probability model is that Î© is a set of
sampled entries of cardinality m sampled uniformly at random so that all the choices
are equally likely. In (5.2), we want to think of Ïµ as a small constant, e.g., smaller
than 1/2. In other words, the â€œenergyâ€ of A on Î© (the set of sampled entries) is
just about proportional to the size of Î©. The near isometry (5.2) is a consequence of
Theorem 4.1 in [16], and we omit the details.
Now returning to the proof of Theorem 4.2, one sees that a suï¬ƒcient condition
for the convergence of (2.7) is
âˆƒÎ² > 0,
âˆ’2Î´âˆ¥Xâ‹†âˆ’Xkâˆ¥2
F + Î´2âˆ¥PÎ©(Xâ‹†âˆ’Xk)âˆ¥2
F â‰¤âˆ’Î²âˆ¥Xâ‹†âˆ’Xkâˆ¥2
F ,
which is equivalent to
0 < Î´ < 2
âˆ¥Xâ‹†âˆ’Xkâˆ¥2
F
âˆ¥PÎ©(Xâ‹†âˆ’Xk)âˆ¥2
F
.
Since âˆ¥PÎ©(X)âˆ¥F â‰¤âˆ¥Xâˆ¥F for any matrix X âˆˆRn1Ã—n2, it is safe to select Î´ < 2. But
suppose that we could apply (5.2) to the matrix A = Xâ‹†âˆ’Xk. Then we could take
Î´ inversely proportional to p; e.g., with Ïµ = 1/4, we could take Î´ â‰¤1.6pâˆ’1. Below, we
shall use the value Î´ = 1.2pâˆ’1 which allows us to take large steps and still provides
convergence, at least empirically.
The reason why this is not a rigorous argument is that (5.2) cannot be applied
to A = Xâ‹†âˆ’Xk even though this matrix diï¬€erence may obey the incoherence
assumption. The issue here is that Xâ‹†âˆ’Xk is not a ï¬xed matrix, but rather depends
on Î© since the iterates {Xk} are computed with the knowledge of the sampled set.

----- Page 17 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1972
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
5.1.3. Initial steps. The SVT algorithm starts with Y 0 = 0, and we want to
choose a large Ï„ to make sure that the solution of (2.8) is close enough to a solution
of (1.1). Deï¬ne k0 as that integer obeying
(5.3)
Ï„
Î´âˆ¥PÎ©(M)âˆ¥2
âˆˆ(k0 âˆ’1, k0].
Since Y 0 = 0, it is not diï¬ƒcult to see that Xk = 0 and Y k = kÎ´ PÎ©(M) for
k = 1, . . . , k0. To save work, we may simply skip the computations of X1, . . . , Xk0
and start the iteration by computing Xk0+1 from Y k0.
This strategy is a special case of a kicking device introduced in [55]; the main idea
of such a kicking scheme is that one can â€œjump overâ€ a few steps whenever possible.
Just as in the aforementioned reference, we can develop similar kicking strategies here
as well. Because in our numerical experiments the kicking is rarely triggered, we forgo
the description of such strategies.
5.1.4. Stopping criteria. Here, we discuss stopping criteria for the sequence
of SVT iterations (2.7) and present two possibilities.
The ï¬rst is motivated by the ï¬rst-order optimality conditions or KKT conditions
tailored to the minimization problem (2.8). By (2.14) and letting âˆ‚Y g0(Y ) = 0 in
(2.13), we see that the solution Xâ‹†
Ï„ to (2.8) must also verify
(5.4)

X = DÏ„(Y ),
PÎ©(X âˆ’M) = 0,
where Y is a matrix vanishing outside of Î©c. Therefore, to make sure that Xk is
close to Xâ‹†
Ï„ , it is suï¬ƒcient to check how close (Xk, Y kâˆ’1) is to obeying (5.4). By
deï¬nition, the ï¬rst equation in (5.4) is always true. Therefore, it is natural to stop
(2.7) when the error in the second equation is below a speciï¬ed tolerance. We suggest
stopping the algorithm when
(5.5)
âˆ¥PÎ©(Xk âˆ’M)âˆ¥F
âˆ¥PÎ©(M)âˆ¥F
â‰¤Ïµ,
where Ïµ is a ï¬xed tolerance, e.g., 10âˆ’4. We provide a short heuristic argument justi-
fying this choice below.
In the matrix completion problem, we know that under suitable assumptions
âˆ¥PÎ©(M)âˆ¥2
F â‰p âˆ¥Mâˆ¥2
F ,
which is just (5.2) applied to the ï¬xed matrix M (the symbol â€œâ‰â€ here means that
there is a constant Ïµ as in (5.2)). Suppose we could also apply (5.2) to the matrix
Xk âˆ’M (which we rigorously cannot since Xk depends on Î©); then we would have
(5.6)
âˆ¥PÎ©(Xk âˆ’M)âˆ¥2
F â‰p âˆ¥Xk âˆ’Mâˆ¥2
F ,
and thus
âˆ¥PÎ©(Xk âˆ’M)âˆ¥F
âˆ¥PÎ©(M)âˆ¥F
â‰âˆ¥Xk âˆ’Mâˆ¥F
âˆ¥Mâˆ¥F
.
In words, one would control the relative reconstruction error by controlling the relative
error on the set of sampled locations.

----- Page 18 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1973
A second stopping criterion comes from duality theory. First, the iterates Xk
are generally not feasible for (2.8) although they become asymptotically feasible. One
can construct a feasible point from Xk by projecting it onto the aï¬ƒne space {X :
PÎ©(X) = PÎ©(M)} as Ëœ
Xk = Xk + PÎ©(M âˆ’Xk). As usual let fÏ„(X) = Ï„âˆ¥Xâˆ¥âˆ—+
1
2âˆ¥Xâˆ¥2
F and denote by pâ‹†the optimal value of (2.8). Since Ëœ
Xk is feasible, we have
pâ‹†â‰¤fÏ„( Ëœ
Xk) := bk. Second, using the notations of section 2.4, duality theory gives
that ak := g0(Y kâˆ’1) = L(Xk, Y kâˆ’1) â‰¤pâ‹†. Therefore, bk âˆ’ak is an upper bound
on the duality gap, and one can stop the algorithm when this quantity falls below a
given tolerance.
For very large problems in which one holds Xk in reduced SVD form, one may
not want to compute the projection Ëœ
Xk since this matrix would not have low rank
and would require signiï¬cant storage space (presumably, one would not want to spend
much time computing this projection either). Hence, the second method makes prac-
tical sense only when the dimensions are not prohibitively large, or when the iterates
do not have low rank.
Similarly, one can derive stopping criteria for all the iterations (3.3), (3.5), and
(3.6). For example, we can stop (3.3) for general linear constraints when âˆ¥A(Xk) âˆ’
bâˆ¥/âˆ¥bâˆ¥â‰¤Ïµ. We omit the detailed discussions here.
5.1.5. Algorithm. We conclude this section by summarizing the implementa-
tion details and give the SVT algorithm for matrix completion below (Algorithm 1).
Of course, one would obtain a very similar structure for the more general problems
of the form (3.1) and (3.4) with linear inequality constraints. For convenience, deï¬ne
for each nonnegative integer s â‰¤min{n1, n2},
[Uk, Î£k, V k]s,
k = 1, 2, . . .,
where Uk = [uk
1, . . . , uk
s] and V k = [vk
1, . . . , vk
s ] are the ï¬rst s singular vectors of the
matrix Y k, and Î£k is a diagonal matrix with the ï¬rst s singular values Ïƒk
1, . . . , Ïƒk
s on
the diagonal.
5.2. Numerical results.
5.2.1. Linear equality constraints. Our implementation is in Matlab, and
all the computational results we are about to report were obtained on a desktop
computer with a 1.86 GHz CPU (dual core with Matlabâ€™s multithreading option
enabled) and 3 GB of memory.
In our simulations, we generate n Ã— n matrices
of rank r by sampling two n Ã— r factors ML and MR independently, each having
independently and identically distributed Gaussian entries, and setting M = MLM âˆ—
R
as it is suggested in [16]. The set of observed entries Î© is sampled uniformly at random
among all sets of cardinality m.
The recovery is performed via the SVT algorithm (Algorithm 1), and we use
(5.7)
âˆ¥PÎ©(Xk âˆ’M)âˆ¥F /âˆ¥PÎ©Mâˆ¥F < 10âˆ’4
as a stopping criterion. As discussed earlier, the step sizes are constant and we set
Î´ = 1.2pâˆ’1. Throughout this section, we denote the output of the SVT algorithm by
Xopt. The parameter Ï„ is chosen empirically and set to Ï„ = 5n. A heuristic argument
is as follows. Clearly, we would like the term Ï„âˆ¥Mâˆ¥âˆ—to dominate the other, namely,
1
2âˆ¥Mâˆ¥2
F . For products of Gaussian matrices as above, standard random matrix theory
asserts that the Frobenius norm of M concentrates around nâˆšr and that the nuclear
norm concentrates around about nr (this should be clear in the simple case where

----- Page 19 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1974
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
Algorithm 1. Singular value thresholding (SVT) algorithm.
Input: sampled set Î© and sampled entries PÎ©(M), step size Î´, tolerance Ïµ,
parameter Ï„, increment â„“, and maximum iteration count kmax
Output: Xopt
Description: Recover a low-rank matrix M from a subset of sampled entries
1
Set Y 0 = k0Î´ PÎ©(M) (k0 is deï¬ned in (5.3))
2
Set r0 = 0
3
for k = 1 to kmax
4
Set sk = rkâˆ’1 + 1
5
repeat
6
Compute [Ukâˆ’1, Î£kâˆ’1, V kâˆ’1]sk
7
Set sk = sk + â„“
8
until Ïƒkâˆ’1
skâˆ’â„“â‰¤Ï„
9
Set rk = max{j : Ïƒkâˆ’1
j
> Ï„}
10
Set Xk = rk
j=1(Ïƒkâˆ’1
j
âˆ’Ï„)ukâˆ’1
j
vkâˆ’1
j
11
if âˆ¥PÎ©(Xk âˆ’M)âˆ¥F /âˆ¥PÎ©Mâˆ¥F â‰¤Ïµ then break
12
Set Y k
ij =

0
if (i, j) Ì¸âˆˆÎ©,
Y kâˆ’1
ij
+ Î´(Mij âˆ’Xk
ij)
if (i, j) âˆˆÎ©
13
end for k
14
Set Xopt = Xk
Table 5.1
Experimental results for matrix completion. The rank r is the rank of the unknown matrix M,
m/dr is the ratio between the number of sampled entries and the number of degrees of freedom in
an n Ã— n matrix of rank r (oversampling ratio), and m/n2 is the fraction of observed entries. All
the computational results on the right are averaged over ï¬ve runs.
Unknown M
Computational results
Size (n Ã— n)
Rank (r)
m/dr
m/n2
Time(s)
# Iters
Relative error
10
6
0.12
23
117
1.64 Ã— 10âˆ’4
1,000 Ã— 1,000
50
4
0.39
196
114
1.59 Ã— 10âˆ’4
100
3
0.57
501
129
1.68 Ã— 10âˆ’4
10
6
0.024
147
123
1.73 Ã— 10âˆ’4
5,000 Ã— 5,000
50
5
0.10
950
108
1.61 Ã— 10âˆ’4
100
4
0.158
3,339
123
1.72 Ã— 10âˆ’4
10
6
0.012
281
123
1.73 Ã— 10âˆ’4
10,000 Ã— 10,000
50
5
0.050
2,096
110
1.65 Ã— 10âˆ’4
100
4
0.080
7,059
127
1.79 Ã— 10âˆ’4
10
6
0.006
588
124
1.73 Ã— 10âˆ’4
20,000 Ã— 20,000
50
5
0.025
4,581
111
1.66 Ã— 10âˆ’4
30,000 Ã— 30,000
10
6
0.004
1,030
125
1.73 Ã— 10âˆ’4
r = 1 and is generally valid). The value Ï„ = 5n makes sure that on the average, the
value of Ï„âˆ¥Mâˆ¥âˆ—is about 10 times that of 1
2âˆ¥Mâˆ¥2
F as long as the rank is bounded
away from the dimension n.
Our computational results are displayed in Table 5.1. There, we report the run
time in seconds, the number of iterations it takes to reach convergence (5.7), and the

----- Page 20 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1975
0
50
100
150
0
1
2
3
4
5
6
7
8
9
10
11
Itertion Step k
Rank of Xk
(a) r = 10
0
20
40
60
80
100
120
140
0
10
20
30
40
50
60
Iteration step k
Rank of Xk
(b) r = 50
0
50
100
150
0
10
20
30
40
50
60
70
80
90
100
110
Iteration step k
Rank of Xk
(c) r = 100
Fig. 5.1. Rank of Xk as a function k when the unknown matrix M is of size 5,000 Ã— 5,000
and of rank r.
relative error of the reconstruction
(5.8)
relative error = âˆ¥Xopt âˆ’Mâˆ¥F /âˆ¥Mâˆ¥F,
where M is the real unknown matrix. All of these quantities are averaged over ï¬ve
runs. Table 5.1 also gives the percentage of entries that are observed, namely, m/n2
together with a quantity that we may want to think of the information oversampling
ratio. Recall that an n Ã— n matrix of rank r depends upon dr := r(2n âˆ’r) degrees
of freedom. Then m/dr is the ratio between the number of sampled entries and the
â€œtrue dimensionalityâ€ of an n Ã— n matrix of rank r.
The ï¬rst observation is that the SVT algorithm performs extremely well in these
experiments. In all of our experiments, it takes fewer than 200 SVT iterations to
reach convergence. As a consequence, the run times are short. As indicated in Table
5.1, we note that one recovers a 1,000 Ã— 1,000 matrix of rank 10 in less than a minute.
The algorithm also recovers 30,000 Ã— 30,000 matrices of rank 10 from about 0.4% of
their sampled entries in just about 17 minutes. In addition, higher-rank matrices are
also eï¬ƒciently completed: For example, it takes between one and two hours to recover
10,000 Ã— 10,000 matrices of rank 100 and 20,000 Ã— 20,000 matrices of rank 50. We
would like to stress that these numbers were obtained on a modest CPU (1.86GHz).
Furthermore, a Fortran implementation is likely to cut down on these numbers by a
multiplicative factor typically between three and four.
We also check the validity of the stopping criterion (5.7) by inspecting the relative
error deï¬ned in (5.8). Table 5.1 shows that the heuristic and nonrigorous analysis of
section 5.1 holds in practice since the relative reconstruction error is of the same order
as âˆ¥PÎ©(Xopt âˆ’M)âˆ¥F /âˆ¥PÎ©Mâˆ¥F âˆ¼10âˆ’4. Indeed, the overall relative errors reported
in Table 5.1 are all less than 2 Ã— 10âˆ’4.
We emphasized all along an important feature of the SVT algorithm, which is that
the matrices Xk have low rank. We demonstrate this fact empirically in Figure 5.1,
which plots the rank of Xk versus the iteration count k, and does this for unknown
matrices of size 5,000Ã—5,000 with diï¬€erent ranks. The plots reveal an interesting phe-
nomenon: In our experiments, the rank of Xk is nondecreasing so that the maximum
rank is reached in the ï¬nal steps of the algorithm. In fact, the rank of the iterates
quickly reaches the value r of the true rank. After these few initial steps, the SVT
iterations search for that matrix with rank r minimizing the objective functional. As
mentioned earlier, the low-rank property is crucial for making the algorithm run fast.
We now present a limited study examining the role of the parameters Î´ and Ï„ in
the convergence. We consider a square 1,000 Ã— 1,000 matrix of rank 10, and select

----- Page 21 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1976
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
Table 5.2
Mean and standard deviation over ï¬ve runs of the number of iterations needed to achieve
(5.7) for diï¬€erent values of the parameters Î´ and Ï„, together with the average ranks of Xk. The
test example is a random 1,000 Ã— 1,000 matrix of rank 10, and the number of sampled entries is
m = 6dr. We also report â€œDNCâ€ when none of the ï¬ve runs obeys (5.7) after 1,000 iterations.
Î´ = 0.8pâˆ’1
Î´ = 1.2pâˆ’1
Î´ = 1.6pâˆ’1
# of iters
Rank
# of iters
Rank
# of iters
Rank
Mean
Std
Mean
Mean
Std
Mean
Mean
Std
Mean
Ï„ = 2n
322
192
15.4
764
1246
11.9
DNC
DNC
DNC
Ï„ = 3n
117
2.6
10.0
77
1.8
10.0
1310
2194
10.0
Ï„ = 4n
146
3.1
10.0
97
2.0
9.9
266
435
10.0
Ï„ = 5n
177
4.1
10.0
117
2.8
10.0
87
2.3
10.0
Ï„ = 6n
207
6.2
10.0
136
2.7
10.0
102
1.9
10.0
a number m of entries equal to 6 times the number of degrees of freedom; that is,
m = 6dr. Numerical results are reported in Table 5.2, which gives the number of
iterations needed to achieve convergence (5.7) and the average rank of each iteration
for diï¬€erent values of Î´ and Ï„.
Table 5.2 suggests that for each value of Î´, there
exists an optimal Ï„ for which the SVT algorithm performs best. In more details,
when Ï„ is smaller than this optimal value, the number of iterations needed to achieve
convergence is larger (and also more variable). In addition, the average rank of each
iteration is also larger, and thus the computational cost is higher. When Ï„ is close to
the optimal value, the SVT algorithm exhibits a rapid convergence, and there is little
variability in the number of iterations needed to achieve convergence. When Ï„ is too
large, the SVT algorithm may overshrink Y k at each iterate which, in turn, leads to
slow convergence. Table 5.2 also indicates that the convergence of the SVT algorithm
depends on the step size Î´.
Finally, we demonstrate the results of the SVT algorithm for matrix completion
from noisy sampled entries. Suppose we observe data from the model
(5.9)
Bij = Mij + Zij,
(i, j) âˆˆÎ©,
where Z is a zero-mean Gaussian white noise with standard deviation Ïƒ. We run the
SVT algorithm but stop early, as soon as Xk is consistent with the data and obeys
(5.10)
âˆ¥PÎ©(Xk âˆ’B)âˆ¥2
F â‰¤(1 + Ïµ) mÏƒ2,
where Ïµ is a small parameter. Since âˆ¥PÎ©(M âˆ’B)âˆ¥2
F is very close to mÏƒ2 for large
values of m, we set Ïµ = 0. Our reconstruction Ë†
M is the ï¬rst Xk obeying (5.10). The
results are shown in Table 5.3 (the quantities are averages of 5 runs). Deï¬ne the noise
ratio as âˆ¥PÎ©(Z)âˆ¥F /âˆ¥PÎ©(M)âˆ¥F , and the relative error by (5.8). From Table 5.3, we
see that the SVT algorithm works well as the relative error between the recovered
and the true data matrix is just about equal to the noise ratio.
The theory of low-rank matrix recovery from noisy data is nonexistent at the
moment and is obviously beyond the scope of this paper. Having said this, we would
like to conclude this section with an intuitive and nonrigorous discussion, which may
explain why the observed recovery error is within the noise level. Suppose again that
Ë†
M obeys (5.6), namely,
(5.11)
âˆ¥PÎ©( Ë†
M âˆ’M)âˆ¥2
F â‰pâˆ¥Ë†
M âˆ’Mâˆ¥2
F .
As mentioned earlier, one condition for this to happen is that M and
Ë†
M have low
rank. This is the reason why it is important to stop the algorithm early as we hope to

----- Page 22 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1977
Table 5.3
Simulation results for noisy data. The computational results are averaged over ï¬ve runs. For
each test, the table shows the results of Algorithm 1 applied with an early stopping criterion.
Noise
Unknown matrix M
Computational results
ratio
Size (n Ã— n)
Rank (r)
m/dr
m/n2
Time(s)
# Iters
Relative error
10
6
0.12
10.8
51
0.78 Ã— 10âˆ’2
10âˆ’2
1,000 Ã— 1,000
50
4
0.39
87.7
48
0.95 Ã— 10âˆ’2
100
3
0.57
216
50
1.13 Ã— 10âˆ’2
10
6
0.12
4.0
19
0.72 Ã— 10âˆ’1
10âˆ’1
1,000 Ã— 1,000
50
4
0.39
33.2
17
0.89 Ã— 10âˆ’1
100
3
0.57
85.2
17
1.01 Ã— 10âˆ’1
10
6
0.12
0.9
3
0.52
1
1,000 Ã— 1,000
50
4
0.39
7.8
3
0.63
100
3
0.57
34.8
3
0.69
obtain a solution which is both consistent with the data and has low rank (the limit
of the SVT iterations, limkâ†’âˆXk, will not generally have low rank since there may
be no low-rank matrix matching the noisy data). From
âˆ¥PÎ©( Ë†
M âˆ’M)âˆ¥F â‰¤âˆ¥PÎ©( Ë†
M âˆ’B)âˆ¥F + âˆ¥PÎ©(B âˆ’M)âˆ¥F ,
and the fact that both terms on the right-hand side are on the order of
âˆš
mÏƒ2, we
would have pâˆ¥Ë†
M âˆ’Mâˆ¥2
F = O(mÏƒ2) by (5.11). In particular, this would give that
the relative reconstruction error is on the order of the noise ratio since âˆ¥PÎ©(M)âˆ¥2
F â‰
pâˆ¥Mâˆ¥2
Fâ€”as observed experimentally.
5.2.2. Inequality constraints. We now examine the speed at which one can
solve similar problems with inequality constraints instead of linear equality con-
straints.
We assume the model (5.9), where the matrix M of rank r is sampled
as before.
We use the noise-aware variant with quadratic constraints (3.10)â€“(3.11). We set
Ïµ to Ïµ2 = Ïƒ2(m+2
âˆš
2m) as this provides a likely upper bound on âˆ¥zâˆ¥so that the true
matrix M is in the feasible set with high probability. The step size is as before and
set to 1.2/p. As a stopping criterion, we stop the iterations (3.12) when the quadratic
constraint is very nearly satisï¬ed; in details, we terminate the algorithm when
âˆ¥b âˆ’A(Xk)âˆ¥F â‰¤(1 + tol) Ïµ
where tol is some small scalar, typically 0.05, so that the constraint is nearly enforced.
The experimental results are shown in Table 5.4. Our experiments suggest that
the algorithm (3.12) is fast and provides statistically accurate answers since it predicts
the unseen entries with an accuracy which is about equal to the standard deviation
of the noise. In fact, very recent work [15] performed after the original submission of
this paper suggests that even with considerable side information about the unknown
matrix, one would not be able to do much better.
As seen in the Table 5.4, although the reconstruction is accurate, the ranks of the
iterates Xk seem to increase with the iteration count k. This is unlike the case with
equality constraints, and we have witnessed this phenomenon in other settings as well
such as in the case of linear inequality constraints, e.g., with the iteration (3.9) for
solving (3.8). Because a higher rank slows down each iteration, it would be of interest
to ï¬nd methods which stabilize the rank and keep it low in general settings. We leave
this important issue for future research.

----- Page 23 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1978
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
Table 5.4
Simulation results for the noise-aware variant (3.12), which solves (3.11). The unknown matrix
M is 1,000Ã— 1,000 and of rank r = 10. We get to see 6 entries per degree of freedom; i.e., m = 6dr.
The noise ratio added is 0.1. The averaged true nuclear norm is 9961.
We choose Ï„ = 5n and
Î´ = 1.2pâˆ’1. The computational results are averaged over ï¬ve runs. The computer here is a quad-
core 2.30GHz AMD Phenom running Matlab 7.6.0 with 3 threads.
tol
Time(s)
# Iters
âˆ¥Ë†
M âˆ’Mâˆ¥F /(nÏƒ)
âˆ¥Ë†
Mâˆ¥âˆ—
Rank( Ë†
M)
0.25
32.8
126
1.11
9034
10
0.2
45.1
158
1.06
9119
15
0.15
94.2
192
1.04
9212
26
0.1
248
232
1.04
9308
39
0.05
447
257
1.03
9415
45
50
100
150
200
250
300
350
400
10
âˆ’1
10
0
 
 
Relative Error
Relative Residual Error
Best Possible Relative Error
(a) SVT
50
100
150
200
250
300
350
400
10
âˆ’1
10
0
 
 
Relative Error
Relative Residual Error
 Best Possible Relative Error Using the same rank
(b) Noise aware variant (3.6)
50
100
150
200
250
300
350
400
0
1
2
3
4
5
k
Rank
 
 
SVT
DS
(c) Rank vs. iteration count
Fig. 5.2. Computational results for the city-to-city distance data set. (a) Plot of the reconstruc-
tion errors from the SVT algorithm. The blue dashed line is the relative error âˆ¥Xk âˆ’Mâˆ¥F /âˆ¥Mâˆ¥F ,
the red dotted line is the relative residual error âˆ¥PÎ©(Xk âˆ’M)âˆ¥F /âˆ¥PÎ©(M)âˆ¥F , and the black line is
the best possible relative error achieved by truncating the SVD of M and keeping a number of terms
equal to the rank of Xk. (b) Same as (a) but with the iteration (3.6). (c) Rank of the successive
iterates Xk; the SVT algorithm is in blue and the noise aware variant (3.6) is in red.
5.3. An example with real data. We conclude the numerical section by ap-
plying our algorithms to a real data set. We downloaded from the Web site [8] a
matrix of geodesic distances (in miles) between 312 cities located in the United States
and Canada. The geodesic distances were computed from latitude and longitude in-
formation, and rounded to the nearest integer. It is well known that the squared
Euclidean distance matrix is a low-rank matrix. With geodesic distances, however,
a numerical test suggests that the geodesic-distance matrix M can be well approxi-
mated by a low-rank matrix. Indeed, letting M3 be the best rank-3 approximation, we
have âˆ¥M3âˆ¥F /âˆ¥Mâˆ¥F = 0.9933 or, equivalently, âˆ¥M3 âˆ’Mâˆ¥F /âˆ¥Mâˆ¥F = 0.1159. Now
sample 30% of the entries of M and obtain and estimate
Ë†
M by the SVT algorithm
and its noise aware variant (3.6). Here, we set Ï„ = 107 which happens to be about
100 times the largest singular value of M, and set Î´ = 2. For completion, we use
the SVT algorithm and the iteration (3.6), which solves (3.8) with Eij = 0.01|M|ij.
In Figure 5.2, we plot the relative error âˆ¥M âˆ’Xkâˆ¥F/âˆ¥Mâˆ¥F, the relative residual
error âˆ¥PÎ©(M âˆ’Xk)âˆ¥F /âˆ¥PÎ©(M)âˆ¥F , and the error of the best approximation with the
same rank. Let ki be the smallest integer such that the rank of Xki is i and the rank
of Xki+1 is i + 1. The computational times needed to reach the kith iteration are
shown in Table 5.5. Table 5.5 indicates that in a few seconds and in a few iterations,
both the SVT algorithm and the iteration (3.6) give a completion, which is nearly as
accurate as the best possible low-rank approximation to the unknown matrix M.

----- Page 24 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1979
Table 5.5
Speed and accuracy of the completion of the city-to-city distance matrix.
Here, âˆ¥M âˆ’
Miâˆ¥F /âˆ¥Mâˆ¥F is the best possible relative error achieved by a matrix of rank i.
Algorithm
Rank
ki
Time
âˆ¥M âˆ’Miâˆ¥F /âˆ¥Mâˆ¥F
âˆ¥M âˆ’Xkiâˆ¥F /âˆ¥Mâˆ¥F
1
58
1.4
0.4091
0.4170
SVT
2
190
4.8
0.1895
0.1980
3
343
8.9
0.1159
0.1252
1
47
2.6
0.4091
0.4234
(3.6)
2
166
7.2
0.1895
0.1998
3
310
13.3
0.1159
0.1270
6. Discussion. This paper introduced a novel algorithm, namely, the singu-
lar value thresholding algorithm for matrix completion and related nuclear norm
minimization problems.
This algorithm is easy to implement and surprisingly ef-
fective both in terms of computational cost and storage requirement when the min-
imum nuclear norm solution is also the lowest-rank solution. We would like to close
this paper by discussing a few open problems and research directions related to this
work.
Our algorithm exploits the fact that the sequence of iterates {Xk} has low rank
when the minimum nuclear solution has low rank. An interesting question is whether
one can prove (or disprove) that in a majority of the cases, this is indeed the case.
It would be interesting to explore other ways of computing DÏ„(Y )â€”in words,
the action of the singular value shrinkage operator. Our approach uses the Lanczos
bidiagonalization algorithm with partial reorthogonalization which takes advantage
of sparse inputs, but other approaches are possible. We mention two of them.
1. A series of papers have proposed the use of randomized procedures for the
approximation of a matrix Y with a matrix Z of rank r [47, 65].
When
this approximation consists of the truncated SVD retaining the part of the
expansion corresponding to singular values greater than Ï„, this can be used
to evaluate DÏ„(Y ). Some of these algorithms are eï¬ƒcient when the input Y
is sparse [65], and it would be interesting to know whether these methods are
fast and accurate enough to be used in the SVT iteration (2.7).
2. A wide range of iterative methods for computing matrix functions of the
general form f(Y ) are available today; see [41] for a survey.
A valuable
research direction is to investigate whether some of these iterative methods,
or others to be developed, would provide powerful ways for computing DÏ„(Y ).
In practice, one would like to solve (2.8) for large values of Ï„. However, a larger
value of Ï„ generally means a slower rate of convergence. A good strategy might be to
start with a value of Ï„, which is large enough so that (2.8) admits a low-rank solution,
and at the same time for which the algorithm converges rapidly. One could then use
a continuation method as in [66] to increase the value of Ï„ sequentially according to
a schedule Ï„0, Ï„1, . . . , and use the solution to the previous problem with Ï„ = Ï„iâˆ’1 as
an initial guess for the solution to the current problem with Ï„ = Ï„i (warm starting).
We hope to report on this in a separate paper.
Acknowledgments. The second author would like to thank Benjamin Recht
and Joel Tropp for fruitful conversations related to this project, and Stephen Becker
for his help in preparing the computational results of section 5.2.2.

----- Page 25 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1980
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
REFERENCES
[1] ACM SIGKDD and Netflix, Proceedings of kdd cup and workshop, http://www.cs.uic.
edu/Ëœliub/KDD-cup-2007/proceedings.html.
[2] J. Abernethy, F. Bach, T. Evgeniou, and J. P. Vert, Low-rank matrix factorization with
attributes, Arxiv preprint cs/0611124, 2006.
[3] Y. Amit, M. Fink, N. Srebro, and S. Ullman, Uncovering shared structures in multiclass
classiï¬cation, in Proceedings of the 24th International Conference on Machine Learning,
ACM, Providence, RI, 2007, pp. 17â€“24.
[4] A. Argyriou, T. Evgeniou, and M. Pontil, Multi-task feature learning, Adv. Neural Inform.
Process. Syst., 19 (2007), pp. 41â€“48.
[5] J. Bect, L. Blanc-Feraud, G. Aubert, and A. Chambolle, A-uniï¬ed variational framework
for image restoration, in Proc. ECCV, Vol. 3024, Springer, New York, 2004, pp. 1â€“13.
[6] D. Bertsekas, On the Goldstein-Levitin-Polyak gradient projection method, IEEE Trans. Au-
tomat. Control, 21 (1976), pp. 174â€“184.
[7] S. P. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, Lon-
don, 2004.
[8] J. Burkardt, Citiesâ€”City distance datasets, http://people.sc.fsu.edu/Ëœburkardt/datasets/
cities/cities.html.
[9] J.-F. Cai, R. H. Chan, L. Shen, and Z. Shen, Restoration of chopped and nodded images by
framelets, SIAM J. Sci. Comput., 30 (2008), pp. 1205â€“1227.
[10] J.-F. Cai, R. H. Chan, and Z. Shen, A framelet-based image inpainting algorithm, Appl.
Comput. Harmon. Anal., 24 (2008), pp. 131â€“149.
[11] J.-F. Cai, S. Osher, and Z. Shen, Convergence of the linearized Bregman iteration for â„“1-
norm minimization, Math. Comp., 78 (2009), pp. 2127â€“2136.
[12] J.-F. Cai, S. Osher, and Z. Shen, Linearized Bregman iterations for compressed sensing,
Math. Comp., 78 (2009), pp. 1515â€“1536.
[13] J.-F. Cai, S. Osher, and Z. Shen, Linearized Bregman iterations for frame-based image
deblurring, SIAM J. Imaging Sci., 2 (2009), pp. 226â€“252.
[14] E. J. Cand`es and F. Guo, New multiscale transforms, minimum total variation synthesis:
Applications to edge-preserving image reconstruction, Signal Process., 82 (2002), pp. 1519â€“
1543.
[15] E. J. Cand`es and Y. Plan, Matrix completion with noise, Proc. IEEE, to appear.
[16] E. J. Cand`es and B. Recht, Exact matrix completion via convex optimization, Found. Com-
put. Math., 9 (2009), pp. 717â€“772.
[17] E. Cand`es and J. Romberg, Sparsity and incoherence in compressive sampling, Inverse Prob-
lems, 23 (2007), pp. 969â€“985.
[18] E. Cand`es and T. Tao, The Dantzig selector: Statistical estimation when p is much larger
than n, Ann. Statist., 35 (2007), pp. 2313â€“2351.
[19] E. J. Cand`es, J. Romberg, and T. Tao, Robust uncertainty principles: Exact signal recon-
struction from highly incomplete frequency information, IEEE Trans. Inform. Theory, 52
(2006), pp. 489â€“509.
[20] E. J. Cand`es and T. Tao, Decoding by linear programming, IEEE Trans. Inform. Theory, 51
(2005), pp. 4203â€“4215.
[21] E. J. Cand`es and T. Tao, Near-optimal signal recovery from random projections: Universal
encoding strategies?, IEEE Trans. Inform. Theory, 52 (2006), pp. 5406â€“5425.
[22] A. Chai and Z. Shen, Deconvolution: A wavelet frame approach, Numer. Math., 106 (2007),
pp. 529â€“587.
[23] R. H. Chan, T. F. Chan, L. Shen, and Z. Shen, Wavelet algorithms for high-resolution image
reconstruction, SIAM J. Sci. Comput., 24 (2003), pp. 1408â€“1432.
[24] P. Chen and D. Suter, Recovering the missing components in a large noisy low-rank matrix:
Application to SFM, IEEE Trans. Pattern Anal. Machine Intelligence, 26 (2004), pp. 1051â€“
1063.
[25] Y. C. Cheng, On the gradient-projection method for solving the nonsymmetric linear comple-
mentarity problem, J. Optim. Theory Appl., 43 (1984), pp. 527â€“541.
[26] P. L. Combettes and V. R. Wajs, Signal recovery by proximal forward-backward splitting,
Multiscale Model. Simul., 4 (2005), pp. 1168â€“1200.
[27] J. Darbon and S. Osher, Fast discrete optimization for sparse approximations and deconvo-
lutions, Department of Mathematics, UCLA, preprint, 2007.
[28] I. Daubechies, M. Defrise, and C. De Mol, An iterative thresholding algorithm for linear
inverse problems with a sparsity constraint, Comm. Pure Appl. Math., 57 (2004), pp. 1413â€“
1457.

----- Page 26 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
A SINGULAR VALUE THRESHOLDING ALGORITHM
1981
[29] I. Daubechies, G. Teschke, and L. Vese, Iteratively solving linear inverse problems under
general convex constraints, Inverse Probl. Imaging, 1 (2007), pp. 29â€“46.
[30] D. L. Donoho, Compressed sensing, IEEE Trans. Inform. Theory, 52 (2006), pp. 1289â€“1306.
[31] B.C. Eaves, On the basic theorem of complementarity, Math. Program., 1 (1971), pp. 68â€“75.
[32] M. Elad, J.-L. Starck, P. Querre, and D. L. Donoho, Simultaneous cartoon and texture
image inpainting using morphological component analysis (MCA), Appl. Comput. Harmon.
Anal., 19 (2005), pp. 340â€“358.
[33] M. J. Fadili, J.-L. Starck, and F. Murtagh, Inpainting and zooming using sparse represen-
tations, Comput. J., 52 (2009), pp. 64â€“79.
[34] M. Fazel, Matrix rank minimization with applications, Ph.D. thesis, Stanford University, Stan-
ford, CA, 2002.
[35] M. Fazel, H. Hindi, and S. P. Boyd, Log-det heuristic for matrix rank minimization with
applications to Hankel and Euclidean distance matrices, Proc. Amer. Control Conf., 3
(2003), pp. 2156â€“2162.
[36] M. A. T. Figueiredo and R. D. Nowak, An EM algorithm for wavelet-based image restoration,
IEEE Trans. Image Process., 12 (2003), pp. 906â€“916.
[37] M. Fukushima, Z.-Q. Luo, and P. Tseng, Smoothing functions for second-order-cone com-
plementarity problems, SIAM J. Optim., 12 (2001/02), pp. 436â€“460.
[38] A. A. Goldstein, Convex programming in Hilbert space, Bull. Amer. Math. Soc., 70 (1964),
pp. 709â€“710.
[39] T. Goldstein and S. Osher, The split Bregman method for L1-regularized problems, SIAM
J. Imaging Sci., 2 (2009), pp. 323â€“343.
[40] E. T. Hale, W. Yin, and Y. Zhang, Fixed-point continuation for l1-minimization: Method-
ology and convergence, SIAM J. Optim., 19 (2008), pp. 1107â€“1130.
[41] N. J. Higham, Functions of matrices, Theory and computation, SIAM, Philadelphia, 2008.
[42] J.-B. Hiriart-Urruty and C. LemarÂ´echal, Convex analysis and minimization algorithms. I,
Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathemati-
cal Sciences], Vol. 305, Springer-Verlag, Berlin, 1993.
[43] A. N. Iusem, On the convergence properties of the projected gradient method for convex opti-
mization, Comput. Appl. Math., 22 (2003), pp. 37â€“52.
[44] R. M. Larsen, PROPACKâ€”Software for large and sparse SVD calculations, http://sun.
stanford.edu/Ëœrmunk/PROPACK/.
[45] E. S. Levitin and B. T. Poliak, Constrained minimization methods (Extremum problems
from functional-analytic point of view, discussing methods of solving and convergence con-
ditions), USSR Comput. Math. Math. Phys., 6 (1966), pp. 1â€“50.
[46] A. S. Lewis, The mathematics of eigenvalue optimization, Math. Program., 97 (2003), pp. 155â€“
176.
[47] E. Liberty, F. Woolfe, P. G. Martinsson, V. Rokhlin, and M. Tygert, Randomized
algorithms for the low-rank approximation of matrices, Proc. Nat. Acad. Sci. USA, 104
(2007), pp. 20167â€“20172.
[48] S. Lintner and F. Malgouyres, Solving a variational image restoration model which involves
L8 constraints, Inverse Prob., 20 (2004), pp. 815â€“831.
[49] Y.-J. Liu, D. F. Sun, and K. C. Toh, An Implementable Proximal Point Algorithmic Frame-
work for Nuclear Norm Minimization, Department of Mathematics, National University
of Singapore, preprint, 2009, http://www.optimization-online.org/DB HTML/2009/07/
2340.html.
[50] Z. Liu and L. Vandenberghe, Interior-point method for nuclear norm approximation with
application to system identiï¬cation, SIAM J. Matrix Anal. Appl., 31 (2009), pp. 1235â€“
1256.
[51] S. Ma, D. Goldfarb, and L. Chen, Fixed point and Bregman iterative methods for matrix
rank minimization, Math. Program., to appear.
[52] P. Marcotte and J. H. Wu, On the convergence of projection methods: Application to the de-
composition of aï¬ƒne variational inequalities, J. Optim. Theory Appl., 85 (1995), pp. 347â€“
362.
[53] M. Mesbahi and G. P. Papavassilopoulos, On the rank minimization problem over a positive
semideï¬nitelinear matrix inequality, IEEE Trans. Automat. Control, 42 (1997), pp. 239â€“
243.
[54] S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin, An iterative regularization method
for total variation-based image restoration, Multiscale Model. Simul., 4 (2005), pp. 460â€“
489.
[55] S. Osher, Y. Mao, B. Dong, and W. Yin, Fast linearized Bregman iteration for compressive
sensing and sparse denoising, Commun. Math. Sci., 8 (2010), pp. 93â€“111.

----- Page 27 (native) -----
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1982
JIAN-FENG CAI, EMMANUEL J. CAND`ES, AND ZUOWEI SHEN
[56] B. Recht, M. Fazel, and P. A. Parrilo, Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization, SIAM Rev., to appear.
[57] R. T. Rockafellar, Convex analysis, Princeton Mathematical Series 28, Princeton University
Press, Princeton, NJ, 1970.
[58] J. L. Starck, D. L. Donoho, and E. J. Cand`es, Astronomical image representation by the
curvelet transform, Astronom. Astrophys., 398 (2003), pp. 785â€“800.
[59] K. C. Toh, M. J. Todd, and R. H. TÂ¨utÂ¨uncÂ¨u, SDPT3â€”A Matlab software package for
semideï¬nite-quadratic-linear programming, version 3.0, 2001, http://www.math.nus.edu.
sg/mattohkc/sdpt3.html.
[60] K.-C. Toh and S. Yun, An accelerated proximal gradient algorithm for nuclear norm regular-
ized least squares problems, Paciï¬c J. Optim., to appear.
[61] C. Tomasi and T. Kanade, Shape and motion from image streams under orthography: A
factorization method, Int. J. Comput. Vision, 9 (1992), pp. 137â€“154.
[62] P. Tseng, Applications of a splitting algorithm to decomposition in convex programming and
variational inequalities., SIAM J. Control Optim., 29 (1991), pp. 119â€“138.
[63] P. Tseng, A modiï¬ed forward-backward splitting method for maximal monotone mappings,
SIAM J. Control Optim., 38 (2000), pp. 431â€“446.
[64] G.A. Watson, Characterization of the subdiï¬€erential of some matrix norms, Linear Algebra
Appl., 170 (1992), pp. 33â€“45.
[65] F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert, A fast randomized algorithm for the
approximation of matrices, Appl. Comput. Harmonic Anal., 25 (2008), pp. 335â€“366.
[66] S. J. Wright, R. Nowak, and M. Figueiredo, Sparse reconstruction by separable approxi-
mation, IEEE Trans. Signal Process., 57 (2009), pp. 2479â€“2493.
[67] W. Yin, S. Osher, D. Goldfarb, and J. Darbon, Bregman iterative algorithms for â„“1-
minimization with applications to compressed sensing, SIAM J. Imaging Sci., 1 (2008),
pp. 143â€“168.
[68] D. L. Zhu and P. Marcotte, Co-coercivity and its role in the convergence of iterative schemes
for solving variational inequalities, SIAM J. Optim., 6 (1996), pp. 714â€“726.