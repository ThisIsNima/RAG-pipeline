

----- Page 1 (native) -----
Journal
of
Articial
In
telligence
Researc
h

(		)
-
Submitted
	/	;
published
/	
Reinforcemen
t
Learning:
A
Surv
ey
Leslie
P
ac
k
Kaelbling
lpk@cs.br
o
wn.edu
Mic
hael
L.
Littman
mlittman@cs.br
o
wn.edu
Computer
Scienc
e
Dep
artment,
Box
	0,
Br
own
University
Pr
ovidenc
e,
RI
0	-	0
USA
Andrew
W.
Mo
ore
a
wm@cs.cmu.edu
Smith
Hal
l
,
Carne
gie
Mel
lon
University,
000
F
orb
es
A
venue
Pittsbur
gh,
P
A

USA
Abstract
This
pap
er
surv
eys
the
eld
of
reinforcemen
t
learning
from
a
computer-science
p
er-
sp
ectiv
e.
It
is
written
to
b
e
accessible
to
researc
hers
familia
r
with
mac
hine
learning.
Both
the
historical
basis
of
the
eld
and
a
broad
selection
of
curren
t
w
ork
are
summarized.
Reinforcemen
t
learning
is
the
problem
faced
b
y
an
agen
t
that
learns
b
eha
vior
through
trial-and-error
in
teractions
with
a
dynamic
en
vironmen
t.
The
w
ork
describ
ed
here
has
a
resem
blance
to
w
ork
in
psyc
hology
,
but
diers
considerably
in
the
details
and
in
the
use
of
the
w
ord
\reinforcemen
t."
The
pap
er
discusses
cen
tral
issues
of
reinforcemen
t
learning,
including
trading
o
exploration
and
exploitation,
establishing
the
foundations
of
the
eld
via
Mark
o
v
decision
theory
,
learning
from
dela
y
ed
reinforcemen
t,
constructing
empirical
mo
dels
to
accelerate
learning,
making
use
of
generalization
and
hierarc
h
y
,
and
coping
with
hidden
state.
It
concludes
with
a
surv
ey
of
some
implemen
ted
systems
and
an
assessmen
t
of
the
practical
utilit
y
of
curren
t
metho
ds
for
reinforcemen
t
learning.
.
In
tro
duction
Reinforcemen
t
learning
dates
bac
k
to
the
early
da
ys
of
cyb
ernetics
and
w
ork
in
statistics,
psyc
hology
,
neuroscience,
and
computer
science.
In
the
last
v
e
to
ten
y
ears,
it
has
attracted
rapidly
increasing
in
terest
in
the
mac
hine
learning
and
articial
in
telligence
comm
unities.
Its
promise
is
b
eguiling|a
w
a
y
of
programming
agen
ts
b
y
rew
ard
and
punishmen
t
without
needing
to
sp
ecify
how
the
task
is
to
b
e
ac
hiev
ed.
But
there
are
formidable
computational
obstacles
to
fullling
the
promise.
This
pap
er
surv
eys
the
historical
basis
of
reinforcemen
t
learning
and
some
of
the
curren
t
w
ork
from
a
computer
science
p
ersp
ectiv
e.
W
e
giv
e
a
high-lev
el
o
v
erview
of
the
eld
and
a
taste
of
some
sp
ecic
approac
hes.
It
is,
of
course,
imp
ossible
to
men
tion
all
of
the
imp
ortan
t
w
ork
in
the
eld;
this
should
not
b
e
tak
en
to
b
e
an
exhaustiv
e
accoun
t.
Reinforcemen
t
learning
is
the
problem
faced
b
y
an
agen
t
that
m
ust
learn
b
eha
vior
through
trial-and-error
in
teractions
with
a
dynamic
en
vironmen
t.
The
w
ork
describ
ed
here
has
a
strong
family
resem
blance
to
ep
on
ymous
w
ork
in
psyc
hology
,
but
diers
considerably
in
the
details
and
in
the
use
of
the
w
ord
\reinforcemen
t."
It
is
appropriately
though
t
of
as
a
class
of
problems,
rather
than
as
a
set
of
tec
hniques.
There
are
t
w
o
main
strategies
for
solving
reinforcemen
t-learning
problems.
The
rst
is
to
searc
h
in
the
space
of
b
eha
viors
in
order
to
nd
one
that
p
erforms
w
ell
in
the
en
vironmen
t.
This
approac
h
has
b
een
tak
en
b
y
w
ork
in
genetic
algorithms
and
genetic
programming,
c
		
AI
Access
F
oundation
and
Morgan
Kaufmann
Publishers.
All
righ
ts
reserv
ed.

----- Page 2 (native) -----
Kaelbling,
Littman,
&
Moore
a
T
s
i
r
B
I
R
Figure
:
The
standard
reinforcemen
t-learning
mo
del.
as
w
ell
as
some
more
no
v
el
searc
h
tec
hniques
(Sc
hmidh
ub
er,
		).
The
second
is
to
use
statistical
tec
hniques
and
dynamic
programming
metho
ds
to
estimate
the
utilit
y
of
taking
actions
in
states
of
the
w
orld.
This
pap
er
is
dev
oted
almost
en
tirely
to
the
second
set
of
tec
hniques
b
ecause
they
tak
e
adv
an
tage
of
the
sp
ecial
structure
of
reinforcemen
t-learning
problems
that
is
not
a
v
ailable
in
optimization
problems
in
general.
It
is
not
y
et
clear
whic
h
set
of
approac
hes
is
b
est
in
whic
h
circumstances.
The
rest
of
this
section
is
dev
oted
to
establishing
notation
and
describing
the
basic
reinforcemen
t-learning
mo
del.
Section

explains
the
trade-o
b
et
w
een
exploration
and
exploitation
and
presen
ts
some
solutions
to
the
most
basic
case
of
reinforcemen
t-learning
problems,
in
whic
h
w
e
w
an
t
to
maximize
the
immediate
rew
ard.
Section

considers
the
more
general
problem
in
whic
h
rew
ards
can
b
e
dela
y
ed
in
time
from
the
actions
that
w
ere
crucial
to
gaining
them.
Section

considers
some
classic
mo
del-free
algorithms
for
reinforcemen
t
learning
from
dela
y
ed
rew
ard:
adaptiv
e
heuristic
critic,
T
D
()
and
Q-learning.
Section

demonstrates
a
con
tin
uum
of
algorithms
that
are
sensitiv
e
to
the
amoun
t
of
computation
an
agen
t
can
p
erform
b
et
w
een
actual
steps
of
action
in
the
en
vironmen
t.
Generalization|the
cornerstone
of
mainstream
mac
hine
learning
researc
h|has
the
p
oten
tial
of
considerably
aiding
reinforcemen
t
learning,
as
describ
ed
in
Section
.
Section

considers
the
problems
that
arise
when
the
agen
t
do
es
not
ha
v
e
complete
p
erceptual
access
to
the
state
of
the
en
vironmen
t.
Section

catalogs
some
of
reinforcemen
t
learning's
successful
applications.
Finally
,
Section
	
concludes
with
some
sp
eculations
ab
out
imp
ortan
t
op
en
problems
and
the
future
of
reinforcemen
t
learning.
.
Reinforcemen
t-Learning
Mo
del
In
the
standard
reinforcemen
t-learning
mo
del,
an
agen
t
is
connected
to
its
en
vironmen
t
via
p
erception
and
action,
as
depicted
in
Figure
.
On
eac
h
step
of
in
teraction
the
agen
t
receiv
es
as
input,
i,
some
indication
of
the
curren
t
state,
s,
of
the
en
vironmen
t;
the
agen
t
then
c
ho
oses
an
action,
a,
to
generate
as
output.
The
action
c
hanges
the
state
of
the
en
vironmen
t,
and
the
v
alue
of
this
state
transition
is
comm
unicated
to
the
agen
t
through
a
scalar
r
einfor
c
ement
signal,
r
.
The
agen
t's
b
eha
vior,
B
,
should
c
ho
ose
actions
that
tend
to
increase
the
long-run
sum
of
v
alues
of
the
reinforcemen
t
signal.
It
can
learn
to
do
this
o
v
er
time
b
y
systematic
trial
and
error,
guided
b
y
a
wide
v
ariet
y
of
algorithms
that
are
the
sub
ject
of
later
sections
of
this
pap
er.


----- Page 3 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
F
ormally
,
the
mo
del
consists
of

a
discrete
set
of
en
vironmen
t
states,
S
;

a
discrete
set
of
agen
t
actions,
A;
and

a
set
of
scalar
reinforcemen
t
signals;
t
ypically
f0;
g,
or
the
real
n
um
b
ers.
The
gure
also
includes
an
input
function
I
,
whic
h
determines
ho
w
the
agen
t
views
the
en
vironmen
t
state;
w
e
will
assume
that
it
is
the
iden
tit
y
function
(that
is,
the
agen
t
p
erceiv
es
the
exact
state
of
the
en
vironmen
t)
un
til
w
e
consider
partial
observ
abilit
y
in
Section
.
An
in
tuitiv
e
w
a
y
to
understand
the
relation
b
et
w
een
the
agen
t
and
its
en
vironmen
t
is
with
the
follo
wing
example
dialogue.
En
vironmen
t:
Y
ou
are
in
state
.
Y
ou
ha
v
e

p
ossible
actions.
Agen
t:
I'll
tak
e
action
.
En
vironmen
t:
Y
ou
receiv
ed
a
reinforcemen
t
of

units.
Y
ou
are
no
w
in
state
.
Y
ou
ha
v
e

p
ossible
actions.
Agen
t:
I'll
tak
e
action
.
En
vironmen
t:
Y
ou
receiv
ed
a
reinforcemen
t
of
-
units.
Y
ou
are
no
w
in
state
.
Y
ou
ha
v
e

p
ossible
actions.
Agen
t:
I'll
tak
e
action
.
En
vironmen
t:
Y
ou
receiv
ed
a
reinforcemen
t
of

units.
Y
ou
are
no
w
in
state
.
Y
ou
ha
v
e

p
ossible
actions.
.
.
.
.
.
.
The
agen
t's
job
is
to
nd
a
p
olicy

,
mapping
states
to
actions,
that
maximizes
some
long-run
measure
of
reinforcemen
t.
W
e
exp
ect,
in
general,
that
the
en
vironmen
t
will
b
e
non-deterministic;
that
is,
that
taking
the
same
action
in
the
same
state
on
t
w
o
dieren
t
o
ccasions
ma
y
result
in
dieren
t
next
states
and/or
dieren
t
reinforcemen
t
v
alues.
This
happ
ens
in
our
example
ab
o
v
e:
from
state
,
applying
action

pro
duces
diering
rein-
forcemen
ts
and
diering
states
on
t
w
o
o
ccasions.
Ho
w
ev
er,
w
e
assume
the
en
vironmen
t
is
stationary;
that
is,
that
the
pr
ob
abilities
of
making
state
transitions
or
receiving
sp
ecic
reinforcemen
t
signals
do
not
c
hange
o
v
er
time.

Reinforcemen
t
learning
diers
from
the
more
widely
studied
problem
of
sup
ervised
learn-
ing
in
sev
eral
w
a
ys.
The
most
imp
ortan
t
dierence
is
that
there
is
no
presen
tation
of
in-
put/output
pairs.
Instead,
after
c
ho
osing
an
action
the
agen
t
is
told
the
immediate
rew
ard
and
the
subsequen
t
state,
but
is
not
told
whic
h
action
w
ould
ha
v
e
b
een
in
its
b
est
long-term
in
terests.
It
is
necessary
for
the
agen
t
to
gather
useful
exp
erience
ab
out
the
p
ossible
system
states,
actions,
transitions
and
rew
ards
activ
ely
to
act
optimally
.
Another
dierence
from
sup
ervised
learning
is
that
on-line
p
erformance
is
imp
ortan
t:
the
ev
aluation
of
the
system
is
often
concurren
t
with
learning.
.
This
assumption
ma
y
b
e
disapp
oin
ti
ng;
after
all,
op
eration
in
non-stationary
en
vironmen
ts
is
one
of
the
motiv
ations
for
buildin
g
learning
systems.
In
fact,
man
y
of
the
algorithms
describ
ed
in
later
sections
are
eectiv
e
in
slo
wly-v
arying
non-stationary
en
vironmen
ts,
but
there
is
v
ery
little
theoretical
analysis
in
this
area.


----- Page 4 (native) -----
Kaelbling,
Littman,
&
Moore
Some
asp
ects
of
reinforcemen
t
learning
are
closely
related
to
searc
h
and
planning
issues
in
articial
in
telligence.
AI
searc
h
algorithms
generate
a
satisfactory
tra
jectory
through
a
graph
of
states.
Planning
op
erates
in
a
similar
manner,
but
t
ypically
within
a
construct
with
more
complexit
y
than
a
graph,
in
whic
h
states
are
represen
ted
b
y
comp
ositions
of
logical
expressions
instead
of
atomic
sym
b
ols.
These
AI
algorithms
are
less
general
than
the
reinforcemen
t-learning
metho
ds,
in
that
they
require
a
predened
mo
del
of
state
transitions,
and
with
a
few
exceptions
assume
determinism.
On
the
other
hand,
reinforcemen
t
learning,
at
least
in
the
kind
of
discrete
cases
for
whic
h
theory
has
b
een
dev
elop
ed,
assumes
that
the
en
tire
state
space
can
b
e
en
umerated
and
stored
in
memory|an
assumption
to
whic
h
con
v
en
tional
searc
h
algorithms
are
not
tied.
.
Mo
dels
of
Optimal
Beha
vior
Before
w
e
can
start
thinking
ab
out
algorithms
for
learning
to
b
eha
v
e
optimally
,
w
e
ha
v
e
to
decide
what
our
mo
del
of
optimalit
y
will
b
e.
In
particular,
w
e
ha
v
e
to
sp
ecify
ho
w
the
agen
t
should
tak
e
the
future
in
to
accoun
t
in
the
decisions
it
mak
es
ab
out
ho
w
to
b
eha
v
e
no
w.
There
are
three
mo
dels
that
ha
v
e
b
een
the
sub
ject
of
the
ma
jorit
y
of
w
ork
in
this
area.
The
nite-horizon
mo
del
is
the
easiest
to
think
ab
out;
at
a
giv
en
momen
t
in
time,
the
agen
t
should
optimize
its
exp
ected
rew
ard
for
the
next
h
steps:
E
(
h
X
t=0
r
t
)
;
it
need
not
w
orry
ab
out
what
will
happ
en
after
that.
In
this
and
subsequen
t
expressions,
r
t
represen
ts
the
scalar
rew
ard
receiv
ed
t
steps
in
to
the
future.
This
mo
del
can
b
e
used
in
t
w
o
w
a
ys.
In
the
rst,
the
agen
t
will
ha
v
e
a
non-stationary
p
olicy;
that
is,
one
that
c
hanges
o
v
er
time.
On
its
rst
step
it
will
tak
e
what
is
termed
a
h-step
optimal
action.
This
is
dened
to
b
e
the
b
est
action
a
v
ailable
giv
en
that
it
has
h
steps
remaining
in
whic
h
to
act
and
gain
reinforcemen
t.
On
the
next
step
it
will
tak
e
a
(h
 )-step
optimal
action,
and
so
on,
un
til
it
nally
tak
es
a
-step
optimal
action
and
terminates.
In
the
second,
the
agen
t
do
es
r
e
c
e
ding-horizon
c
ontr
ol,
in
whic
h
it
alw
a
ys
tak
es
the
h-step
optimal
action.
The
agen
t
alw
a
ys
acts
according
to
the
same
p
olicy
,
but
the
v
alue
of
h
limits
ho
w
far
ahead
it
lo
oks
in
c
ho
osing
its
actions.
The
nite-horizon
mo
del
is
not
alw
a
ys
appropriate.
In
man
y
cases
w
e
ma
y
not
kno
w
the
precise
length
of
the
agen
t's
life
in
adv
ance.
The
innite-horizon
discoun
ted
mo
del
tak
es
the
long-run
rew
ard
of
the
agen
t
in
to
ac-
coun
t,
but
rew
ards
that
are
receiv
ed
in
the
future
are
geometrically
discoun
ted
according
to
discoun
t
factor

,
(where
0


<
):
E
(

X
t=0

t
r
t
)
:
W
e
can
in
terpret

in
sev
eral
w
a
ys.
It
can
b
e
seen
as
an
in
terest
rate,
a
probabilit
y
of
living
another
step,
or
as
a
mathematical
tric
k
to
b
ound
the
innite
sum.
The
mo
del
is
conceptu-
ally
similar
to
receding-horizon
con
trol,
but
the
discoun
ted
mo
del
is
more
mathematically
tractable
than
the
nite-horizon
mo
del.
This
is
a
dominan
t
reason
for
the
wide
atten
tion
this
mo
del
has
receiv
ed.
0

----- Page 5 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
Another
optimalit
y
criterion
is
the
aver
age-r
ewar
d
mo
del,
in
whic
h
the
agen
t
is
supp
osed
to
tak
e
actions
that
optimize
its
long-run
a
v
erage
rew
ard:
lim
h!
E
(

h
h
X
t=0
r
t
)
:
Suc
h
a
p
olicy
is
referred
to
as
a
gain
optimal
p
olicy;
it
can
b
e
seen
as
the
limiting
case
of
the
innite-horizon
discoun
ted
mo
del
as
the
discoun
t
factor
approac
hes

(Bertsek
as,
		).
One
problem
with
this
criterion
is
that
there
is
no
w
a
y
to
distinguish
b
et
w
een
t
w
o
p
olicies,
one
of
whic
h
gains
a
large
amoun
t
of
rew
ard
in
the
initial
phases
and
the
other
of
whic
h
do
es
not.
Rew
ard
gained
on
an
y
initial
prex
of
the
agen
t's
life
is
o
v
ershado
w
ed
b
y
the
long-run
a
v
erage
p
erformance.
It
is
p
ossible
to
generalize
this
mo
del
so
that
it
tak
es
in
to
accoun
t
b
oth
the
long
run
a
v
erage
and
the
amoun
t
of
initial
rew
ard
than
can
b
e
gained.
In
the
generalized,
bias
optimal
mo
del,
a
p
olicy
is
preferred
if
it
maximizes
the
long-run
a
v
erage
and
ties
are
brok
en
b
y
the
initial
extra
rew
ard.
Figure

con
trasts
these
mo
dels
of
optimalit
y
b
y
pro
viding
an
en
vironmen
t
in
whic
h
c
hanging
the
mo
del
of
optimalit
y
c
hanges
the
optimal
p
olicy
.
In
this
example,
circles
represen
t
the
states
of
the
en
vironmen
t
and
arro
ws
are
state
transitions.
There
is
only
a
single
action
c
hoice
from
ev
ery
state
except
the
start
state,
whic
h
is
in
the
upp
er
left
and
mark
ed
with
an
incoming
arro
w.
All
rew
ards
are
zero
except
where
mark
ed.
Under
a
nite-horizon
mo
del
with
h
=
,
the
three
actions
yield
rew
ards
of
+:0,
+0:0,
and
+0:0,
so
the
rst
action
should
b
e
c
hosen;
under
an
innite-horizon
discoun
ted
mo
del
with

=
0:	,
the
three
c
hoices
yield
+:,
+	:0,
and
+:
so
the
second
action
should
b
e
c
hosen;
and
under
the
a
v
erage
rew
ard
mo
del,
the
third
action
should
b
e
c
hosen
since
it
leads
to
an
a
v
erage
rew
ard
of
+.
If
w
e
c
hange
h
to
000
and

to
0.,
then
the
second
action
is
optimal
for
the
nite-horizon
mo
del
and
the
rst
for
the
innite-horizon
discoun
ted
mo
del;
ho
w
ev
er,
the
a
v
erage
rew
ard
mo
del
will
alw
a
ys
prefer
the
b
est
long-term
a
v
erage.
Since
the
c
hoice
of
optimalit
y
mo
del
and
parameters
matters
so
m
uc
h,
it
is
imp
ortan
t
to
c
ho
ose
it
carefully
in
an
y
application.
The
nite-horizon
mo
del
is
appropriate
when
the
agen
t's
lifetime
is
kno
wn;
one
im-
p
ortan
t
asp
ect
of
this
mo
del
is
that
as
the
length
of
the
remaining
lifetime
decreases,
the
agen
t's
p
olicy
ma
y
c
hange.
A
system
with
a
hard
deadline
w
ould
b
e
appropriately
mo
deled
this
w
a
y
.
The
relativ
e
usefulness
of
innite-horizon
discoun
ted
and
bias-optimal
mo
dels
is
still
under
debate.
Bias-optimalit
y
has
the
adv
an
tage
of
not
requiring
a
discoun
t
parameter;
ho
w
ev
er,
algorithms
for
nding
bias-optimal
p
olicies
are
not
y
et
as
w
ell-understo
o
d
as
those
for
nding
optimal
innite-horizon
discoun
ted
p
olicies.
.
Measuring
Learning
P
erformance
The
criteria
giv
en
in
the
previous
section
can
b
e
used
to
assess
the
p
olicies
learned
b
y
a
giv
en
algorithm.
W
e
w
ould
also
lik
e
to
b
e
able
to
ev
aluate
the
qualit
y
of
learning
itself.
There
are
sev
eral
incompatible
measures
in
use.

Ev
en
tual
con
v
ergence
to
optimal.
Man
y
algorithms
come
with
a
pro
v
able
guar-
an
tee
of
asymptotic
con
v
ergence
to
optimal
b
eha
vior
(W
atkins
&
Da
y
an,
		).
This
is
reassuring,
but
useless
in
practical
terms.
An
agen
t
that
quic
kly
reac
hes
a
plateau


----- Page 6 (native) -----
Kaelbling,
Littman,
&
Moore
Finite horizon, h=4
Inï¬nite horizon, Î³=0.9
Average reward
+2
+10
+11
Figure
:
Comparing
mo
dels
of
optimalit
y
.
All
unlab
eled
arro
ws
pro
duce
a
rew
ard
of
zero.
at
		%
of
optimalit
y
ma
y
,
in
man
y
applications,
b
e
preferable
to
an
agen
t
that
has
a
guaran
tee
of
ev
en
tual
optimalit
y
but
a
sluggish
early
learning
rate.

Sp
eed
of
con
v
ergence
to
optimalit
y
.
Optimalit
y
is
usually
an
asymptotic
result,
and
so
con
v
ergence
sp
eed
is
an
ill-dened
measure.
More
practical
is
the
sp
e
e
d
of
c
onver
genc
e
to
ne
ar-optimality.
This
measure
b
egs
the
denition
of
ho
w
near
to
optimalit
y
is
sucien
t.
A
related
measure
is
level
of
p
erformanc
e
after
a
given
time,
whic
h
similarly
requires
that
someone
dene
the
giv
en
time.
It
should
b
e
noted
that
here
w
e
ha
v
e
another
dierence
b
et
w
een
reinforcemen
t
learning
and
con
v
en
tional
sup
ervised
learning.
In
the
latter,
exp
ected
future
predictiv
e
accu-
racy
or
statistical
eciency
are
the
prime
concerns.
F
or
example,
in
the
w
ell-kno
wn
P
A
C
framew
ork
(V
alian
t,
	),
there
is
a
learning
p
erio
d
during
whic
h
mistak
es
do
not
coun
t,
then
a
p
erformance
p
erio
d
during
whic
h
they
do.
The
framew
ork
pro
vides
b
ounds
on
the
necessary
length
of
the
learning
p
erio
d
in
order
to
ha
v
e
a
probabilistic
guaran
tee
on
the
subsequen
t
p
erformance.
That
is
usually
an
inappropriate
view
for
an
agen
t
with
a
long
existence
in
a
complex
en
vironmen
t.
In
spite
of
the
mismatc
h
b
et
w
een
em
b
edded
reinforcemen
t
learning
and
the
train/test
p
ersp
ectiv
e,
Fiec
h
ter
(		)
pro
vides
a
P
A
C
analysis
for
Q-learning
(describ
ed
in
Section
.)
that
sheds
some
ligh
t
on
the
connection
b
et
w
een
the
t
w
o
views.
Measures
related
to
sp
eed
of
learning
ha
v
e
an
additional
w
eakness.
An
algorithm
that
merely
tries
to
ac
hiev
e
optimalit
y
as
fast
as
p
ossible
ma
y
incur
unnecessarily
large
p
enalties
during
the
learning
p
erio
d.
A
less
aggressiv
e
strategy
taking
longer
to
ac
hiev
e
optimalit
y
,
but
gaining
greater
total
reinforcemen
t
during
its
learning
migh
t
b
e
preferable.

Regret.
A
more
appropriate
measure,
then,
is
the
exp
ected
decrease
in
rew
ard
gained
due
to
executing
the
learning
algorithm
instead
of
b
eha
ving
optimally
from
the
v
ery
b
eginning.
This
measure
is
kno
wn
as
r
e
gr
et
(Berry
&
F
ristedt,
	).
It
p
enalizes
mistak
es
wherev
er
they
o
ccur
during
the
run.
Unfortunately
,
results
concerning
the
regret
of
algorithms
are
quite
hard
to
obtain.


----- Page 7 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
.
Reinforcemen
t
Learning
and
Adaptiv
e
Con
trol
Adaptiv
e
con
trol
(Burghes
&
Graham,
	0;
Stengel,
	)
is
also
concerned
with
algo-
rithms
for
impro
ving
a
sequence
of
decisions
from
exp
erience.
Adaptiv
e
con
trol
is
a
m
uc
h
more
mature
discipline
that
concerns
itself
with
dynamic
systems
in
whic
h
states
and
ac-
tions
are
v
ectors
and
system
dynamics
are
smo
oth:
linear
or
lo
cally
linearizable
around
a
desired
tra
jectory
.
A
v
ery
common
form
ulation
of
cost
functions
in
adaptiv
e
con
trol
are
quadratic
p
enalties
on
deviation
from
desired
state
and
action
v
ectors.
Most
imp
ortan
tly
,
although
the
dynamic
mo
del
of
the
system
is
not
kno
wn
in
adv
ance,
and
m
ust
b
e
esti-
mated
from
data,
the
structur
e
of
the
dynamic
mo
del
is
xed,
lea
ving
mo
del
estimation
as
a
parameter
estimation
problem.
These
assumptions
p
ermit
deep,
elegan
t
and
p
o
w
erful
mathematical
analysis,
whic
h
in
turn
lead
to
robust,
practical,
and
widely
deplo
y
ed
adaptiv
e
con
trol
algorithms.
.
Exploitation
v
ersus
Exploration:
The
Single-State
Case
One
ma
jor
dierence
b
et
w
een
reinforcemen
t
learning
and
sup
ervised
learning
is
that
a
reinforcemen
t-learner
m
ust
explicitly
explore
its
en
vironmen
t.
In
order
to
highligh
t
the
problems
of
exploration,
w
e
treat
a
v
ery
simple
case
in
this
section.
The
fundamen
tal
issues
and
approac
hes
describ
ed
here
will,
in
man
y
cases,
transfer
to
the
more
complex
instances
of
reinforcemen
t
learning
discussed
later
in
the
pap
er.
The
simplest
p
ossible
reinforcemen
t-learning
problem
is
kno
wn
as
the
k
-armed
bandit
problem,
whic
h
has
b
een
the
sub
ject
of
a
great
deal
of
study
in
the
statistics
and
applied
mathematics
literature
(Berry
&
F
ristedt,
	).
The
agen
t
is
in
a
ro
om
with
a
collection
of
k
gam
bling
mac
hines
(eac
h
called
a
\one-armed
bandit"
in
collo
quial
English).
The
agen
t
is
p
ermitted
a
xed
n
um
b
er
of
pulls,
h.
An
y
arm
ma
y
b
e
pulled
on
eac
h
turn.
The
mac
hines
do
not
require
a
dep
osit
to
pla
y;
the
only
cost
is
in
w
asting
a
pull
pla
ying
a
sub
optimal
mac
hine.
When
arm
i
is
pulled,
mac
hine
i
pa
ys
o

or
0,
according
to
some
underlying
probabilit
y
parameter
p
i
,
where
pa
y
os
are
indep
enden
t
ev
en
ts
and
the
p
i
s
are
unkno
wn.
What
should
the
agen
t's
strategy
b
e?
This
problem
illustrates
the
fundamen
tal
tradeo
b
et
w
een
exploitation
and
exploration.
The
agen
t
migh
t
b
eliev
e
that
a
particular
arm
has
a
fairly
high
pa
y
o
probabilit
y;
should
it
c
ho
ose
that
arm
all
the
time,
or
should
it
c
ho
ose
another
one
that
it
has
less
information
ab
out,
but
seems
to
b
e
w
orse?
Answ
ers
to
these
questions
dep
end
on
ho
w
long
the
agen
t
is
exp
ected
to
pla
y
the
game;
the
longer
the
game
lasts,
the
w
orse
the
consequences
of
prematurely
con
v
erging
on
a
sub-optimal
arm,
and
the
more
the
agen
t
should
explore.
There
is
a
wide
v
ariet
y
of
solutions
to
this
problem.
W
e
will
consider
a
represen
tativ
e
selection
of
them,
but
for
a
deep
er
discussion
and
a
n
um
b
er
of
imp
ortan
t
theoretical
results,
see
the
b
o
ok
b
y
Berry
and
F
ristedt
(	).
W
e
use
the
term
\action"
to
indicate
the
agen
t's
c
hoice
of
arm
to
pull.
This
eases
the
transition
in
to
dela
y
ed
reinforcemen
t
mo
dels
in
Section
.
It
is
v
ery
imp
ortan
t
to
note
that
bandit
problems
t
our
denition
of
a
reinforcemen
t-learning
en
vironmen
t
with
a
single
state
with
only
self
transitions.
Section
.
discusses
three
solutions
to
the
basic
one-state
bandit
problem
that
ha
v
e
formal
correctness
results.
Although
they
can
b
e
extended
to
problems
with
real-v
alued
rew
ards,
they
do
not
apply
directly
to
the
general
m
ulti-state
dela
y
ed-reinforcemen
t
case.


----- Page 8 (native) -----
Kaelbling,
Littman,
&
Moore
Section
.
presen
ts
three
tec
hniques
that
are
not
formally
justied,
but
that
ha
v
e
had
wide
use
in
practice,
and
can
b
e
applied
(with
similar
lac
k
of
guaran
tee)
to
the
general
case.
.
F
ormally
Justied
T
ec
hniques
There
is
a
fairly
w
ell-dev
elop
ed
formal
theory
of
exploration
for
v
ery
simple
problems.
Although
it
is
instructiv
e,
the
metho
ds
it
pro
vides
do
not
scale
w
ell
to
more
complex
problems.
..
D
ynamic-Pr
ogramming
Appr
o
a
ch
If
the
agen
t
is
going
to
b
e
acting
for
a
total
of
h
steps,
it
can
use
basic
Ba
y
esian
reasoning
to
solv
e
for
an
optimal
strategy
(Berry
&
F
ristedt,
	).
This
requires
an
assumed
prior
join
t
distribution
for
the
parameters
fp
i
g,
the
most
natural
of
whic
h
is
that
eac
h
p
i
is
indep
enden
tly
uniformly
distributed
b
et
w
een
0
and
.
W
e
compute
a
mapping
from
b
elief
states
(summaries
of
the
agen
t's
exp
eriences
during
this
run)
to
actions.
Here,
a
b
elief
state
can
b
e
represen
ted
as
a
tabulation
of
action
c
hoices
and
pa
y
os:
fn

;
w

;
n

;
w

;
:
:
:
;
n
k
;
w
k
g
denotes
a
state
of
pla
y
in
whic
h
eac
h
arm
i
has
b
een
pulled
n
i
times
with
w
i
pa
y
os.
W
e
write
V

(n

;
w

;
:
:
:
;
n
k
;
w
k
)
as
the
exp
ected
pa
y
o
remaining,
giv
en
that
a
total
of
h
pulls
are
a
v
ailable,
and
w
e
use
the
remaining
pulls
optimally
.
If
P
i
n
i
=
h,
then
there
are
no
remaining
pulls,
and
V

(n

;
w

;
:
:
:
;
n
k
;
w
k
)
=
0.
This
is
the
basis
of
a
recursiv
e
denition.
If
w
e
kno
w
the
V

v
alue
for
all
b
elief
states
with
t
pulls
remaining,
w
e
can
compute
the
V

v
alue
of
an
y
b
elief
state
with
t
+

pulls
remaining:
V

(n

;
w

;
:
:
:
;
n
k
;
w
k
)
=
max
i
E
"
F
uture
pa
y
o
if
agen
t
tak
es
action
i,
then
acts
optimally
for
remaining
pulls
#
=
max
i
 

i
V

(n

;
w
i
;
:
:
:
;
n
i
+
;
w
i
+
;
:
:
:
;
n
k
;
w
k
)+
(
 
i
)V

(n

;
w
i
;
:
:
:
;
n
i
+
;
w
i
;
:
:
:
;
n
k
;
w
k
)
!
where

i
is
the
p
osterior
sub
jectiv
e
probabilit
y
of
action
i
pa
ying
o
giv
en
n
i
,
w
i
and
our
prior
probabilit
y
.
F
or
the
uniform
priors,
whic
h
result
in
a
b
eta
distribution,

i
=
(w
i
+
)=(n
i
+
).
The
exp
ense
of
lling
in
the
table
of
V

v
alues
in
this
w
a
y
for
all
attainable
b
elief
states
is
linear
in
the
n
um
b
er
of
b
elief
states
times
actions,
and
th
us
exp
onen
tial
in
the
horizon.
..
Gittins
Alloca
tion
Indices
Gittins
giv
es
an
\allo
cation
index"
metho
d
for
nding
the
optimal
c
hoice
of
action
at
eac
h
step
in
k
-armed
bandit
problems
(Gittins,
		).
The
tec
hnique
only
applies
under
the
discoun
ted
exp
ected
rew
ard
criterion.
F
or
eac
h
action,
consider
the
n
um
b
er
of
times
it
has
b
een
c
hosen,
n,
v
ersus
the
n
um
b
er
of
times
it
has
paid
o,
w
.
F
or
certain
discoun
t
factors,
there
are
published
tables
of
\index
v
alues,"
I
(n;
w
)
for
eac
h
pair
of
n
and
w
.
Lo
ok
up
the
index
v
alue
for
eac
h
action
i,
I
(n
i
;
w
i
).
It
represen
ts
a
comparativ
e
measure
of
the
com
bined
v
alue
of
the
exp
ected
pa
y
o
of
action
i
(giv
en
its
history
of
pa
y
os)
and
the
v
alue
of
the
information
that
w
e
w
ould
get
b
y
c
ho
osing
it.
Gittins
has
sho
wn
that
c
ho
osing
the
action
with
the
largest
index
v
alue
guaran
tees
the
optimal
balance
b
et
w
een
exploration
and
exploitation.


----- Page 9 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
1
2
3
N-1
N
2N
2N-1
N+3
N+2
N+1
a = 0
a = 1
r = 0
r = 1
1
2
3
N-1
N
2N
2N-1
N+3
N+2
N+1
a = 0
a = 1
Figure
:
A
Tsetlin
automaton
with
N
states.
The
top
ro
w
sho
ws
the
state
transitions
that
are
made
when
the
previous
action
resulted
in
a
rew
ard
of
;
the
b
ottom
ro
w
sho
ws
transitions
after
a
rew
ard
of
0.
In
states
in
the
left
half
of
the
gure,
action
0
is
tak
en;
in
those
on
the
righ
t,
action

is
tak
en.
Because
of
the
guaran
tee
of
optimal
exploration
and
the
simplicit
y
of
the
tec
hnique
(giv
en
the
table
of
index
v
alues),
this
approac
h
holds
a
great
deal
of
promise
for
use
in
more
complex
applications.
This
metho
d
pro
v
ed
useful
in
an
application
to
rob
otic
manipulation
with
immediate
rew
ard
(Salganico
&
Ungar,
		).
Unfortunately
,
no
one
has
y
et
b
een
able
to
nd
an
analog
of
index
v
alues
for
dela
y
ed
reinforcemen
t
problems.
..
Learning
A
utoma
t
a
A
branc
h
of
the
theory
of
adaptiv
e
con
trol
is
dev
oted
to
le
arning
automata,
surv
ey
ed
b
y
Narendra
and
Thathac
har
(		),
whic
h
w
ere
originally
describ
ed
explicitly
as
nite
state
automata.
The
Tsetlin
automaton
sho
wn
in
Figure

pro
vides
an
example
that
solv
es
a
-armed
bandit
arbitrarily
near
optimally
as
N
approac
hes
innit
y
.
It
is
incon
v
enien
t
to
describ
e
algorithms
as
nite-state
automata,
so
a
mo
v
e
w
as
made
to
describ
e
the
in
ternal
state
of
the
agen
t
as
a
probabilit
y
distribution
according
to
whic
h
actions
w
ould
b
e
c
hosen.
The
probabilities
of
taking
dieren
t
actions
w
ould
b
e
adjusted
according
to
their
previous
successes
and
failures.
An
example,
whic
h
stands
among
a
set
of
algorithms
indep
enden
tly
dev
elop
ed
in
the
mathematical
psyc
hology
literature
(Hilgard
&
Bo
w
er,
	),
is
the
line
ar
r
ewar
d-inaction
algorithm.
Let
p
i
b
e
the
agen
t's
probabilit
y
of
taking
action
i.

When
action
a
i
succeeds,
p
i
:=
p
i
+
(
 p
i
)
p
j
:=
p
j
 p
j
for
j
=
i

When
action
a
i
fails,
p
j
remains
unc
hanged
(for
all
j
).
This
algorithm
con
v
erges
with
probabilit
y

to
a
v
ector
con
taining
a
single

and
the
rest
0's
(c
ho
osing
a
particular
action
with
probabilit
y
).
Unfortunately
,
it
do
es
not
alw
a
ys
con
v
erge
to
the
correct
action;
but
the
probabilit
y
that
it
con
v
erges
to
the
wrong
one
can
b
e
made
arbitrarily
small
b
y
making

small
(Narendra
&
Thathac
har,
	).
There
is
no
literature
on
the
regret
of
this
algorithm.


----- Page 10 (native) -----
Kaelbling,
Littman,
&
Moore
.
Ad-Ho
c
T
ec
hniques
In
reinforcemen
t-learning
practice,
some
simple,
ad
ho
c
strategies
ha
v
e
b
een
p
opular.
They
are
rarely
,
if
ev
er,
the
b
est
c
hoice
for
the
mo
dels
of
optimalit
y
w
e
ha
v
e
used,
but
they
ma
y
b
e
view
ed
as
reasonable,
computationally
tractable,
heuristics.
Thrun
(		)
has
surv
ey
ed
a
v
ariet
y
of
these
tec
hniques.
..
Greed
y
Stra
tegies
The
rst
strategy
that
comes
to
mind
is
to
alw
a
ys
c
ho
ose
the
action
with
the
highest
esti-
mated
pa
y
o.
The
a
w
is
that
early
unluc
ky
sampling
migh
t
indicate
that
the
b
est
action's
rew
ard
is
less
than
the
rew
ard
obtained
from
a
sub
optimal
action.
The
sub
optimal
action
will
alw
a
ys
b
e
pic
k
ed,
lea
ving
the
true
optimal
action
starv
ed
of
data
and
its
sup
eriorit
y
nev
er
disco
v
ered.
An
agen
t
m
ust
explore
to
ameliorate
this
outcome.
A
useful
heuristic
is
optimism
in
the
fac
e
of
unc
ertainty
in
whic
h
actions
are
selected
greedily
,
but
strongly
optimistic
prior
b
eliefs
are
put
on
their
pa
y
os
so
that
strong
negativ
e
evidence
is
needed
to
eliminate
an
action
from
consideration.
This
still
has
a
measurable
danger
of
starving
an
optimal
but
unluc
ky
action,
but
the
risk
of
this
can
b
e
made
arbitrar-
ily
small.
T
ec
hniques
lik
e
this
ha
v
e
b
een
used
in
sev
eral
reinforcemen
t
learning
algorithms
including
the
in
terv
al
exploration
metho
d
(Kaelbling,
		b)
(describ
ed
shortly),
the
ex-
plor
ation
b
onus
in
Dyna
(Sutton,
		0),
curiosity-driven
explor
ation
(Sc
hmidh
ub
er,
		a),
and
the
exploration
mec
hanism
in
prioritized
sw
eeping
(Mo
ore
&
A
tk
eson,
		).
..
Randomized
Stra
tegies
Another
simple
exploration
strategy
is
to
tak
e
the
action
with
the
b
est
estimated
exp
ected
rew
ard
b
y
default,
but
with
probabilit
y
p,
c
ho
ose
an
action
at
random.
Some
v
ersions
of
this
strategy
start
with
a
large
v
alue
of
p
to
encourage
initial
exploration,
whic
h
is
slo
wly
decreased.
An
ob
jection
to
the
simple
strategy
is
that
when
it
exp
erimen
ts
with
a
non-greedy
action
it
is
no
more
lik
ely
to
try
a
promising
alternativ
e
than
a
clearly
hop
eless
alternativ
e.
A
sligh
tly
more
sophisticated
strategy
is
Boltzmann
explor
ation.
In
this
case,
the
exp
ected
rew
ard
for
taking
action
a,
E
R(a)
is
used
to
c
ho
ose
an
action
probabilisticall
y
according
to
the
distribution
P
(a)
=
e
E
R(a)=T
P
a
0
A
e
E
R(a
0
)=T
:
The
temp
er
atur
e
parameter
T
can
b
e
decreased
o
v
er
time
to
decrease
exploration.
This
metho
d
w
orks
w
ell
if
the
b
est
action
is
w
ell
separated
from
the
others,
but
suers
somewhat
when
the
v
alues
of
the
actions
are
close.
It
ma
y
also
con
v
erge
unnecessarily
slo
wly
unless
the
temp
erature
sc
hedule
is
man
ually
tuned
with
great
care.
..
Inter
v
al-based
Techniques
Exploration
is
often
more
ecien
t
when
it
is
based
on
second-order
information
ab
out
the
certain
t
y
or
v
ariance
of
the
estimated
v
alues
of
actions.
Kaelbling's
interval
estimation
algorithm
(		b)
stores
statistics
for
eac
h
action
a
i
:
w
i
is
the
n
um
b
er
of
successes
and
n
i
the
n
um
b
er
of
trials.
An
action
is
c
hosen
b
y
computing
the
upp
er
b
ound
of
a
00

(
 )%


----- Page 11 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
condence
in
terv
al
on
the
success
probabilit
y
of
eac
h
action
and
c
ho
osing
the
action
with
the
highest
upp
er
b
ound.
Smaller
v
alues
of
the

parameter
encourage
greater
exploration.
When
pa
y
os
are
b
o
olean,
the
normal
appro
ximation
to
the
binomial
distribution
can
b
e
used
to
construct
the
condence
in
terv
al
(though
the
binomial
should
b
e
used
for
small
n).
Other
pa
y
o
distributions
can
b
e
handled
using
their
asso
ciated
statistics
or
with
nonparametric
metho
ds.
The
metho
d
w
orks
v
ery
w
ell
in
empirical
trials.
It
is
also
related
to
a
certain
class
of
statistical
tec
hniques
kno
wn
as
exp
eriment
design
metho
ds
(Bo
x
&
Drap
er,
	),
whic
h
are
used
for
comparing
m
ultiple
treatmen
ts
(for
example,
fertilizers
or
drugs)
to
determine
whic
h
treatmen
t
(if
an
y)
is
b
est
in
as
small
a
set
of
exp
erimen
ts
as
p
ossible.
.
More
General
Problems
When
there
are
m
ultiple
states,
but
reinforcemen
t
is
still
immediate,
then
an
y
of
the
ab
o
v
e
solutions
can
b
e
replicated,
once
for
eac
h
state.
Ho
w
ev
er,
when
generalization
is
required,
these
solutions
m
ust
b
e
in
tegrated
with
generalization
metho
ds
(see
section
);
this
is
straigh
tforw
ard
for
the
simple
ad-ho
c
metho
ds,
but
it
is
not
understo
o
d
ho
w
to
main
tain
theoretical
guaran
tees.
Man
y
of
these
tec
hniques
fo
cus
on
con
v
erging
to
some
regime
in
whic
h
exploratory
actions
are
tak
en
rarely
or
nev
er;
this
is
appropriate
when
the
en
vironmen
t
is
stationary
.
Ho
w
ev
er,
when
the
en
vironmen
t
is
non-stationary
,
exploration
m
ust
con
tin
ue
to
tak
e
place,
in
order
to
notice
c
hanges
in
the
w
orld.
Again,
the
more
ad-ho
c
tec
hniques
can
b
e
mo
died
to
deal
with
this
in
a
plausible
manner
(k
eep
temp
erature
parameters
from
going
to
0;
deca
y
the
statistics
in
in
terv
al
estimation),
but
none
of
the
theoretically
guaran
teed
metho
ds
can
b
e
applied.
.
Dela
y
ed
Rew
ard
In
the
general
case
of
the
reinforcemen
t
learning
problem,
the
agen
t's
actions
determine
not
only
its
immediate
rew
ard,
but
also
(at
least
probabilistically)
the
next
state
of
the
en
vironmen
t.
Suc
h
en
vironmen
ts
can
b
e
though
t
of
as
net
w
orks
of
bandit
problems,
but
the
agen
t
m
ust
tak
e
in
to
accoun
t
the
next
state
as
w
ell
as
the
immediate
rew
ard
when
it
decides
whic
h
action
to
tak
e.
The
mo
del
of
long-run
optimalit
y
the
agen
t
is
using
determines
exactly
ho
w
it
should
tak
e
the
v
alue
of
the
future
in
to
accoun
t.
The
agen
t
will
ha
v
e
to
b
e
able
to
learn
from
dela
y
ed
reinforcemen
t:
it
ma
y
tak
e
a
long
sequence
of
actions,
receiving
insignican
t
reinforcemen
t,
then
nally
arriv
e
at
a
state
with
high
reinforcemen
t.
The
agen
t
m
ust
b
e
able
to
learn
whic
h
of
its
actions
are
desirable
based
on
rew
ard
that
can
tak
e
place
arbitrarily
far
in
the
future.
.
Mark
o
v
Decision
Pro
cesses
Problems
with
dela
y
ed
reinforcemen
t
are
w
ell
mo
deled
as
Markov
de
cision
pr
o
c
esses
(MDPs).
An
MDP
consists
of

a
set
of
states
S
,

a
set
of
actions
A,


----- Page 12 (native) -----
Kaelbling,
Littman,
&
Moore

a
rew
ard
function
R
:
S

A
!
<,
and

a
state
transition
function
T
:
S

A
!
(S
),
where
a
mem
b
er
of
(S
)
is
a
probabilit
y
distribution
o
v
er
the
set
S
(i.e.
it
maps
states
to
probabilities).
W
e
write
T
(s;
a;
s
0
)
for
the
probabilit
y
of
making
a
transition
from
state
s
to
state
s
0
using
action
a.
The
state
transition
function
probabilistically
sp
ecies
the
next
state
of
the
en
vironmen
t
as
a
function
of
its
curren
t
state
and
the
agen
t's
action.
The
rew
ard
function
sp
ecies
exp
ected
instan
taneous
rew
ard
as
a
function
of
the
curren
t
state
and
action.
The
mo
del
is
Markov
if
the
state
transitions
are
indep
enden
t
of
an
y
previous
en
vironmen
t
states
or
agen
t
actions.
There
are
man
y
go
o
d
references
to
MDP
mo
dels
(Bellman,
	;
Bertsek
as,
	;
Ho
w
ard,
	0;
Puterman,
		).
Although
general
MDPs
ma
y
ha
v
e
innite
(ev
en
uncoun
table)
state
and
action
spaces,
w
e
will
only
discuss
metho
ds
for
solving
nite-state
and
nite-action
problems.
In
section
,
w
e
discuss
metho
ds
for
solving
problems
with
con
tin
uous
input
and
output
spaces.
.
Finding
a
P
olicy
Giv
en
a
Mo
del
Before
w
e
consider
algorithms
for
learning
to
b
eha
v
e
in
MDP
en
vironmen
ts,
w
e
will
ex-
plore
tec
hniques
for
determining
the
optimal
p
olicy
giv
en
a
correct
mo
del.
These
dynamic
programming
tec
hniques
will
serv
e
as
the
foundation
and
inspiration
for
the
learning
al-
gorithms
to
follo
w.
W
e
restrict
our
atten
tion
mainly
to
nding
optimal
p
olicies
for
the
innite-horizon
discoun
ted
mo
del,
but
most
of
these
algorithms
ha
v
e
analogs
for
the
nite-
horizon
and
a
v
erage-case
mo
dels
as
w
ell.
W
e
rely
on
the
result
that,
for
the
innite-horizon
discoun
ted
mo
del,
there
exists
an
optimal
deterministic
stationary
p
olicy
(Bellman,
	).
W
e
will
sp
eak
of
the
optimal
value
of
a
state|it
is
the
exp
ected
innite
discoun
ted
sum
of
rew
ard
that
the
agen
t
will
gain
if
it
starts
in
that
state
and
executes
the
optimal
p
olicy
.
Using

as
a
complete
decision
p
olicy
,
it
is
written
V

(s)
=
max

E
 

X
t=0

t
r
t
!
:
This
optimal
v
alue
function
is
unique
and
can
b
e
dened
as
the
solution
to
the
sim
ultaneous
equations
V

(s)
=
max
a
0
@
R(s;
a)
+

X
s
0
S
T
(s;
a;
s
0
)V

(s
0
)

A
;
s

S
;
()
whic
h
assert
that
the
v
alue
of
a
state
s
is
the
exp
ected
instan
taneous
rew
ard
plus
the
exp
ected
discoun
ted
v
alue
of
the
next
state,
using
the
b
est
a
v
ailable
action.
Giv
en
the
optimal
v
alue
function,
w
e
can
sp
ecify
the
optimal
p
olicy
as


(s)
=
arg
max
a
0
@
R(s;
a)
+

X
s
0
S
T
(s;
a;
s
0
)V

(s
0
)

A
:
..
V
alue
Itera
tion
One
w
a
y
,
then,
to
nd
an
optimal
p
olicy
is
to
nd
the
optimal
v
alue
function.
It
can
b
e
determined
b
y
a
simple
iterativ
e
algorithm
called
value
iter
ation
that
can
b
e
sho
wn
to
con
v
erge
to
the
correct
V

v
alues
(Bellman,
	;
Bertsek
as,
	).


----- Page 13 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
initialize
V
(s)
arbitrarily
loop
until
policy
good
enough
loop
for
s

S
loop
for
a

A
Q(s;
a)
:=
R(s;
a)
+

P
s
0
S
T
(s;
a;
s
0
)V
(s
0
)
V
(s)
:=
max
a
Q(s;
a)
end
loop
end
loop
It
is
not
ob
vious
when
to
stop
the
v
alue
iteration
algorithm.
One
imp
ortan
t
result
b
ounds
the
p
erformance
of
the
curren
t
greedy
p
olicy
as
a
function
of
the
Bel
lman
r
esidual
of
the
curren
t
v
alue
function
(Williams
&
Baird,
		b).
It
sa
ys
that
if
the
maxim
um
dierence
b
et
w
een
t
w
o
successiv
e
v
alue
functions
is
less
than
,
then
the
v
alue
of
the
greedy
p
olicy
,
(the
p
olicy
obtained
b
y
c
ho
osing,
in
ev
ery
state,
the
action
that
maximizes
the
estimated
discoun
ted
rew
ard,
using
the
curren
t
estimate
of
the
v
alue
function)
diers
from
the
v
alue
function
of
the
optimal
p
olicy
b
y
no
more
than

=(
 
)
at
an
y
state.
This
pro
vides
an
eectiv
e
stopping
criterion
for
the
algorithm.
Puterman
(		)
discusses
another
stopping
criterion,
based
on
the
sp
an
semi-norm,
whic
h
ma
y
result
in
earlier
termination.
Another
imp
ortan
t
result
is
that
the
greedy
p
olicy
is
guaran
teed
to
b
e
optimal
in
some
nite
n
um
b
er
of
steps
ev
en
though
the
v
alue
function
ma
y
not
ha
v
e
con
v
erged
(Bertsek
as,
	).
And
in
practice,
the
greedy
p
olicy
is
often
optimal
long
b
efore
the
v
alue
function
has
con
v
erged.
V
alue
iteration
is
v
ery
exible.
The
assignmen
ts
to
V
need
not
b
e
done
in
strict
order
as
sho
wn
ab
o
v
e,
but
instead
can
o
ccur
async
hronously
in
parallel
pro
vided
that
the
v
alue
of
ev
ery
state
gets
up
dated
innitely
often
on
an
innite
run.
These
issues
are
treated
extensiv
ely
b
y
Bertsek
as
(		),
who
also
pro
v
es
con
v
ergence
results.
Up
dates
based
on
Equation

are
kno
wn
as
ful
l
b
ackups
since
they
mak
e
use
of
infor-
mation
from
all
p
ossible
successor
states.
It
can
b
e
sho
wn
that
up
dates
of
the
form
Q(s;
a)
:=
Q(s;
a)
+
(r
+

max
a
0
Q(s
0
;
a
0
)
 Q(s;
a))
can
also
b
e
used
as
long
as
eac
h
pairing
of
a
and
s
is
up
dated
innitely
often,
s
0
is
sampled
from
the
distribution
T
(s;
a;
s
0
),
r
is
sampled
with
mean
R(s;
a)
and
b
ounded
v
ariance,
and
the
learning
rate

is
decreased
slo
wly
.
This
t
yp
e
of
sample
b
ackup
(Singh,
		)
is
critical
to
the
op
eration
of
the
mo
del-free
metho
ds
discussed
in
the
next
section.
The
computational
complexit
y
of
the
v
alue-iteration
algorithm
with
full
bac
kups,
p
er
iteration,
is
quadratic
in
the
n
um
b
er
of
states
and
linear
in
the
n
um
b
er
of
actions.
Com-
monly
,
the
transition
probabilities
T
(s;
a;
s
0
)
are
sparse.
If
there
are
on
a
v
erage
a
constan
t
n
um
b
er
of
next
states
with
non-zero
probabilit
y
then
the
cost
p
er
iteration
is
linear
in
the
n
um
b
er
of
states
and
linear
in
the
n
um
b
er
of
actions.
The
n
um
b
er
of
iterations
required
to
reac
h
the
optimal
v
alue
function
is
p
olynomial
in
the
n
um
b
er
of
states
and
the
magnitude
of
the
largest
rew
ard
if
the
discoun
t
factor
is
held
constan
t.
Ho
w
ev
er,
in
the
w
orst
case
the
n
um
b
er
of
iterations
gro
ws
p
olynomially
in
=(
 
),
so
the
con
v
ergence
rate
slo
ws
considerably
as
the
discoun
t
factor
approac
hes

(Littman,
Dean,
&
Kaelbling,
		b).


----- Page 14 (native) -----
Kaelbling,
Littman,
&
Moore
..
Policy
Itera
tion
The
p
olicy
iter
ation
algorithm
manipulates
the
p
olicy
directly
,
rather
than
nding
it
indi-
rectly
via
the
optimal
v
alue
function.
It
op
erates
as
follo
ws:
choose
an
arbitrary
policy

0
loop

:=

0
compute
the
value
function
of
policy

:
solve
the
linear
equations
V

(s)
=
R(s;

(s))
+

P
s
0
S
T
(s;

(s);
s
0
)V

(s
0
)
improve
the
policy
at
each
state:

0
(s)
:=
arg
max
a
(R(s;
a)
+

P
s
0
S
T
(s;
a;
s
0
)V

(s
0
))
until

=

0
The
v
alue
function
of
a
p
olicy
is
just
the
exp
ected
innite
discoun
ted
rew
ard
that
will
b
e
gained,
at
eac
h
state,
b
y
executing
that
p
olicy
.
It
can
b
e
determined
b
y
solving
a
set
of
linear
equations.
Once
w
e
kno
w
the
v
alue
of
eac
h
state
under
the
curren
t
p
olicy
,
w
e
consider
whether
the
v
alue
could
b
e
impro
v
ed
b
y
c
hanging
the
rst
action
tak
en.
If
it
can,
w
e
c
hange
the
p
olicy
to
tak
e
the
new
action
whenev
er
it
is
in
that
situation.
This
step
is
guaran
teed
to
strictly
impro
v
e
the
p
erformance
of
the
p
olicy
.
When
no
impro
v
emen
ts
are
p
ossible,
then
the
p
olicy
is
guaran
teed
to
b
e
optimal.
Since
there
are
at
most
jAj
jS
j
distinct
p
olicies,
and
the
sequence
of
p
olicies
impro
v
es
at
eac
h
step,
this
algorithm
terminates
in
at
most
an
exp
onen
tial
n
um
b
er
of
iterations
(Puter-
man,
		).
Ho
w
ev
er,
it
is
an
imp
ortan
t
op
en
question
ho
w
man
y
iterations
p
olicy
iteration
tak
es
in
the
w
orst
case.
It
is
kno
wn
that
the
running
time
is
pseudop
olynomial
and
that
for
an
y
xed
discoun
t
factor,
there
is
a
p
olynomial
b
ound
in
the
total
size
of
the
MDP
(Littman
et
al.,
		b).
..
Enhancement
to
V
alue
Itera
tion
and
Policy
Itera
tion
In
practice,
v
alue
iteration
is
m
uc
h
faster
p
er
iteration,
but
p
olicy
iteration
tak
es
few
er
iterations.
Argumen
ts
ha
v
e
b
een
put
forth
to
the
eect
that
eac
h
approac
h
is
b
etter
for
large
problems.
Puterman's
mo
die
d
p
olicy
iter
ation
algorithm
(Puterman
&
Shin,
	)
pro
vides
a
metho
d
for
trading
iteration
time
for
iteration
impro
v
emen
t
in
a
smo
other
w
a
y
.
The
basic
idea
is
that
the
exp
ensiv
e
part
of
p
olicy
iteration
is
solving
for
the
exact
v
alue
of
V

.
Instead
of
nding
an
exact
v
alue
for
V

,
w
e
can
p
erform
a
few
steps
of
a
mo
died
v
alue-iteration
step
where
the
p
olicy
is
held
xed
o
v
er
successiv
e
iterations.
This
can
b
e
sho
wn
to
pro
duce
an
appro
ximation
to
V

that
con
v
erges
linearly
in

.
In
practice,
this
can
result
in
substan
tial
sp
eedups.
Sev
eral
standard
n
umerical-analysis
tec
hniques
that
sp
eed
the
con
v
ergence
of
dynamic
programming
can
b
e
used
to
accelerate
v
alue
and
p
olicy
iteration.
Multigrid
metho
ds
can
b
e
used
to
quic
kly
seed
a
go
o
d
initial
appro
ximation
to
a
high
resolution
v
alue
function
b
y
initially
p
erforming
v
alue
iteration
at
a
coarser
resolution
(R

ude,
		).
State
aggr
e-
gation
w
orks
b
y
collapsing
groups
of
states
to
a
single
meta-state
solving
the
abstracted
problem
(Bertsek
as
&
Casta
~
non,
		).
0

----- Page 15 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
..
Comput
a
tional
Complexity
V
alue
iteration
w
orks
b
y
pro
ducing
successiv
e
appro
ximations
of
the
optimal
v
alue
function.
Eac
h
iteration
can
b
e
p
erformed
in
O
(jAjjS
j

)
steps,
or
faster
if
there
is
sparsit
y
in
the
transition
function.
Ho
w
ev
er,
the
n
um
b
er
of
iterations
required
can
gro
w
exp
onen
tially
in
the
discoun
t
factor
(Condon,
		);
as
the
discoun
t
factor
approac
hes
,
the
decisions
m
ust
b
e
based
on
results
that
happ
en
farther
and
farther
in
to
the
future.
In
practice,
p
olicy
iteration
con
v
erges
in
few
er
iterations
than
v
alue
iteration,
although
the
p
er-iteration
costs
of
O
(jAjjS
j

+
jS
j

)
can
b
e
prohibitiv
e.
There
is
no
kno
wn
tigh
t
w
orst-case
b
ound
a
v
ailable
for
p
olicy
iteration
(Littman
et
al.,
		b).
Mo
died
p
olicy
iteration
(Puterman
&
Shin,
	)
seeks
a
trade-o
b
et
w
een
c
heap
and
eectiv
e
iterations
and
is
preferred
b
y
some
practictioners
(Rust,
		).
Linear
programming
(Sc
hrijv
er,
	)
is
an
extremely
general
problem,
and
MDPs
can
b
e
solv
ed
b
y
general-purp
ose
linear-programming
pac
k
ages
(Derman,
	0;
D'Ep
enoux,
	;
Homan
&
Karp,
	).
An
adv
an
tage
of
this
approac
h
is
that
commercial-qualit
y
linear-programming
pac
k
ages
are
a
v
ailable,
although
the
time
and
space
requiremen
ts
can
still
b
e
quite
high.
F
rom
a
theoretic
p
ersp
ectiv
e,
linear
programming
is
the
only
kno
wn
algorithm
that
can
solv
e
MDPs
in
p
olynomial
time,
although
the
theoretically
ecien
t
algorithms
ha
v
e
not
b
een
sho
wn
to
b
e
ecien
t
in
practice.
.
Learning
an
Optimal
P
olicy:
Mo
del-free
Metho
ds
In
the
previous
section
w
e
review
ed
metho
ds
for
obtaining
an
optimal
p
olicy
for
an
MDP
assuming
that
w
e
already
had
a
mo
del.
The
mo
del
consists
of
kno
wledge
of
the
state
tran-
sition
probabilit
y
function
T
(s;
a;
s
0
)
and
the
reinforcemen
t
function
R(s;
a).
Reinforcemen
t
learning
is
primarily
concerned
with
ho
w
to
obtain
the
optimal
p
olicy
when
suc
h
a
mo
del
is
not
kno
wn
in
adv
ance.
The
agen
t
m
ust
in
teract
with
its
en
vironmen
t
directly
to
obtain
information
whic
h,
b
y
means
of
an
appropriate
algorithm,
can
b
e
pro
cessed
to
pro
duce
an
optimal
p
olicy
.
A
t
this
p
oin
t,
there
are
t
w
o
w
a
ys
to
pro
ceed.

Mo
del-free:
Learn
a
con
troller
without
learning
a
mo
del.

Mo
del-based:
Learn
a
mo
del,
and
use
it
to
deriv
e
a
con
troller.
Whic
h
approac
h
is
b
etter?
This
is
a
matter
of
some
debate
in
the
reinforcemen
t-learning
comm
unit
y
.
A
n
um
b
er
of
algorithms
ha
v
e
b
een
prop
osed
on
b
oth
sides.
This
question
also
app
ears
in
other
elds,
suc
h
as
adaptiv
e
con
trol,
where
the
dic
hotom
y
is
b
et
w
een
dir
e
ct
and
indir
e
ct
adaptiv
e
con
trol.
This
section
examines
mo
del-free
learning,
and
Section

examines
mo
del-based
meth-
o
ds.
The
biggest
problem
facing
a
reinforcemen
t-learning
agen
t
is
temp
or
al
cr
e
dit
assignment.
Ho
w
do
w
e
kno
w
whether
the
action
just
tak
en
is
a
go
o
d
one,
when
it
migh
t
ha
v
e
far-
reac
hing
eects?
One
strategy
is
to
w
ait
un
til
the
\end"
and
rew
ard
the
actions
tak
en
if
the
result
w
as
go
o
d
and
punish
them
if
the
result
w
as
bad.
In
ongoing
tasks,
it
is
dicult
to
kno
w
what
the
\end"
is,
and
this
migh
t
require
a
great
deal
of
memory
.
Instead,
w
e
will
use
insigh
ts
from
v
alue
iteration
to
adjust
the
estimated
v
alue
of
a
state
based
on


----- Page 16 (native) -----
Kaelbling,
Littman,
&
Moore
AHC
RL
v
s
r
a
Figure
:
Arc
hitecture
for
the
adaptiv
e
heuristic
critic.
the
immediate
rew
ard
and
the
estimated
v
alue
of
the
next
state.
This
class
of
algorithms
is
kno
wn
as
temp
or
al
dier
enc
e
metho
ds
(Sutton,
	).
W
e
will
consider
t
w
o
dieren
t
temp
oral-dierence
learning
strategies
for
the
discoun
ted
innite-horizon
mo
del.
.
Adaptiv
e
Heuristic
Critic
and
TD
()
The
adaptive
heuristic
critic
algorithm
is
an
adaptiv
e
v
ersion
of
p
olicy
iteration
(Barto,
Sutton,
&
Anderson,
	)
in
whic
h
the
v
alue-function
computation
is
no
longer
imple-
men
ted
b
y
solving
a
set
of
linear
equations,
but
is
instead
computed
b
y
an
algorithm
called
T
D
(0).
A
blo
c
k
diagram
for
this
approac
h
is
giv
en
in
Figure
.
It
consists
of
t
w
o
comp
o-
nen
ts:
a
critic
(lab
eled
AHC),
and
a
reinforcemen
t-learning
comp
onen
t
(lab
eled
RL).
The
reinforcemen
t-learning
comp
onen
t
can
b
e
an
instance
of
an
y
of
the
k
-armed
bandit
algo-
rithms,
mo
died
to
deal
with
m
ultiple
states
and
non-stationary
rew
ards.
But
instead
of
acting
to
maximize
instan
taneous
rew
ard,
it
will
b
e
acting
to
maximize
the
heuristic
v
alue,
v
,
that
is
computed
b
y
the
critic.
The
critic
uses
the
real
external
reinforcemen
t
signal
to
learn
to
map
states
to
their
exp
ected
discoun
ted
v
alues
giv
en
that
the
p
olicy
b
eing
executed
is
the
one
curren
tly
instan
tiated
in
the
RL
comp
onen
t.
W
e
can
see
the
analogy
with
mo
died
p
olicy
iteration
if
w
e
imagine
these
comp
onen
ts
w
orking
in
alternation.
The
p
olicy

implemen
ted
b
y
RL
is
xed
and
the
critic
learns
the
v
alue
function
V

for
that
p
olicy
.
No
w
w
e
x
the
critic
and
let
the
RL
comp
onen
t
learn
a
new
p
olicy

0
that
maximizes
the
new
v
alue
function,
and
so
on.
In
most
implemen
tations,
ho
w
ev
er,
b
oth
comp
onen
ts
op
erate
sim
ultaneously
.
Only
the
alternating
implemen
tation
can
b
e
guaran
teed
to
con
v
erge
to
the
optimal
p
olicy
,
under
appropriate
conditions.
Williams
and
Baird
explored
the
con
v
ergence
prop
erties
of
a
class
of
AHC-related
algorithms
they
call
\incremen
tal
v
arian
ts
of
p
olicy
iteration"
(Williams
&
Baird,
		a).
It
remains
to
explain
ho
w
the
critic
can
learn
the
v
alue
of
a
p
olicy
.
W
e
dene
hs;
a;
r
;
s
0
i
to
b
e
an
exp
erienc
e
tuple
summarizing
a
single
transition
in
the
en
vironmen
t.
Here
s
is
the
agen
t's
state
b
efore
the
transition,
a
is
its
c
hoice
of
action,
r
the
instan
taneous
rew
ard
it
receiv
es,
and
s
0
its
resulting
state.
The
v
alue
of
a
p
olicy
is
learned
using
Sutton's
T
D
(0)
algorithm
(Sutton,
	)
whic
h
uses
the
up
date
rule
V
(s)
:=
V
(s)
+
(r
+

V
(s
0
)
 V
(s))
:
Whenev
er
a
state
s
is
visited,
its
estimated
v
alue
is
up
dated
to
b
e
closer
to
r
+

V
(s
0
),
since
r
is
the
instan
taneous
rew
ard
receiv
ed
and
V
(s
0
)
is
the
estimated
v
alue
of
the
actually
o
ccurring
next
state.
This
is
analogous
to
the
sample-bac
kup
rule
from
v
alue
iteration|the
only
dierence
is
that
the
sample
is
dra
wn
from
the
real
w
orld
rather
than
b
y
sim
ulating
a
kno
wn
mo
del.
The
k
ey
idea
is
that
r
+

V
(s
0
)
is
a
sample
of
the
v
alue
of
V
(s),
and
it
is


----- Page 17 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
more
lik
ely
to
b
e
correct
b
ecause
it
incorp
orates
the
real
r
.
If
the
learning
rate

is
adjusted
prop
erly
(it
m
ust
b
e
slo
wly
decreased)
and
the
p
olicy
is
held
xed,
T
D
(0)
is
guaran
teed
to
con
v
erge
to
the
optimal
v
alue
function.
The
T
D
(0)
rule
as
presen
ted
ab
o
v
e
is
really
an
instance
of
a
more
general
class
of
algorithms
called
T
D
(),
with

=
0.
T
D
(0)
lo
oks
only
one
step
ahead
when
adjusting
v
alue
estimates;
although
it
will
ev
en
tually
arriv
e
at
the
correct
answ
er,
it
can
tak
e
quite
a
while
to
do
so.
The
general
T
D
()
rule
is
similar
to
the
T
D
(0)
rule
giv
en
ab
o
v
e,
V
(u)
:=
V
(u)
+
(r
+

V
(s
0
)
 V
(s))e(u)
;
but
it
is
applied
to
every
state
according
to
its
eligibili
t
y
e(u),
rather
than
just
to
the
immediately
previous
state,
s.
One
v
ersion
of
the
eligibil
it
y
trace
is
dened
to
b
e
e(s)
=
t
X
k
=
(
)
t k

s;s
k
,
where

s;s
k
=
(

if
s
=
s
k
0
otherwise
.
The
eligibili
t
y
of
a
state
s
is
the
degree
to
whic
h
it
has
b
een
visited
in
the
recen
t
past;
when
a
reinforcemen
t
is
receiv
ed,
it
is
used
to
up
date
all
the
states
that
ha
v
e
b
een
recen
tly
visited,
according
to
their
eligibili
t
y
.
When

=
0
this
is
equiv
alen
t
to
T
D
(0).
When

=
,
it
is
roughly
equiv
alen
t
to
up
dating
all
the
states
according
to
the
n
um
b
er
of
times
they
w
ere
visited
b
y
the
end
of
a
run.
Note
that
w
e
can
up
date
the
eligibili
t
y
online
as
follo
ws:
e(s)
:=
(

e(s)
+

if
s
=
curren
t
state

e(s)
otherwise
.
It
is
computationally
more
exp
ensiv
e
to
execute
the
general
T
D
(),
though
it
often
con
v
erges
considerably
faster
for
large

(Da
y
an,
		;
Da
y
an
&
Sejno
wski,
		).
There
has
b
een
some
recen
t
w
ork
on
making
the
up
dates
more
ecien
t
(Cic
hosz
&
Mula
wk
a,
		)
and
on
c
hanging
the
denition
to
mak
e
T
D
()
more
consisten
t
with
the
certain
t
y-equiv
alen
t
metho
d
(Singh
&
Sutton,
		),
whic
h
is
discussed
in
Section
..
.
Q-learning
The
w
ork
of
the
t
w
o
comp
onen
ts
of
AHC
can
b
e
accomplished
in
a
unied
manner
b
y
W
atkins'
Q-learning
algorithm
(W
atkins,
		;
W
atkins
&
Da
y
an,
		).
Q-learning
is
t
ypically
easier
to
implemen
t.
In
order
to
understand
Q-learning,
w
e
ha
v
e
to
dev
elop
some
additional
notation.
Let
Q

(s;
a)
b
e
the
exp
ected
discoun
ted
reinforcemen
t
of
taking
action
a
in
state
s,
then
con
tin
uing
b
y
c
ho
osing
actions
optimally
.
Note
that
V

(s)
is
the
v
alue
of
s
assuming
the
b
est
action
is
tak
en
initially
,
and
so
V

(s)
=
max
a
Q

(s;
a).
Q

(s;
a)
can
hence
b
e
written
recursiv
ely
as
Q

(s;
a)
=
R(s;
a)
+

X
s
0
S
T
(s;
a;
s
0
)
max
a
0
Q

(s
0
;
a
0
)
:
Note
also
that,
since
V

(s)
=
max
a
Q

(s;
a),
w
e
ha
v
e


(s)
=
arg
max
a
Q

(s;
a)
as
an
optimal
p
olicy
.
Because
the
Q
function
mak
es
the
action
explicit,
w
e
can
estimate
the
Q
v
alues
on-
line
using
a
metho
d
essen
tially
the
same
as
T
D
(0),
but
also
use
them
to
dene
the
p
olicy
,


----- Page 18 (native) -----
Kaelbling,
Littman,
&
Moore
b
ecause
an
action
can
b
e
c
hosen
just
b
y
taking
the
one
with
the
maxim
um
Q
v
alue
for
the
curren
t
state.
The
Q-learning
rule
is
Q(s;
a)
:=
Q(s;
a)
+
(r
+

max
a
0
Q(s
0
;
a
0
)
 Q(s;
a))
;
where
hs;
a;
r
;
s
0
i
is
an
exp
erience
tuple
as
describ
ed
earlier.
If
eac
h
action
is
executed
in
eac
h
state
an
innite
n
um
b
er
of
times
on
an
innite
run
and

is
deca
y
ed
appropriately
,
the
Q
v
alues
will
con
v
erge
with
probabilit
y

to
Q

(W
atkins,
		;
Tsitsiklis,
		;
Jaakk
ola,
Jordan,
&
Singh,
		).
Q-learning
can
also
b
e
extended
to
up
date
states
that
o
ccurred
more
than
one
step
previously
,
as
in
T
D
()
(P
eng
&
Williams,
		).
When
the
Q
v
alues
are
nearly
con
v
erged
to
their
optimal
v
alues,
it
is
appropriate
for
the
agen
t
to
act
greedily
,
taking,
in
eac
h
situation,
the
action
with
the
highest
Q
v
alue.
During
learning,
ho
w
ev
er,
there
is
a
dicult
exploitation
v
ersus
exploration
trade-o
to
b
e
made.
There
are
no
go
o
d,
formally
justied
approac
hes
to
this
problem
in
the
general
case;
standard
practice
is
to
adopt
one
of
the
ad
ho
c
metho
ds
discussed
in
section
..
AHC
arc
hitectures
seem
to
b
e
more
dicult
to
w
ork
with
than
Q-learning
on
a
practical
lev
el.
It
can
b
e
hard
to
get
the
relativ
e
learning
rates
righ
t
in
AHC
so
that
the
t
w
o
comp
onen
ts
con
v
erge
together.
In
addition,
Q-learning
is
explor
ation
insensitive:
that
is,
that
the
Q
v
alues
will
con
v
erge
to
the
optimal
v
alues,
indep
enden
t
of
ho
w
the
agen
t
b
eha
v
es
while
the
data
is
b
eing
collected
(as
long
as
all
state-action
pairs
are
tried
often
enough).
This
means
that,
although
the
exploration-exploitation
issue
m
ust
b
e
addressed
in
Q-learning,
the
details
of
the
exploration
strategy
will
not
aect
the
con
v
ergence
of
the
learning
algorithm.
F
or
these
reasons,
Q-learning
is
the
most
p
opular
and
seems
to
b
e
the
most
eectiv
e
mo
del-free
algorithm
for
learning
from
dela
y
ed
reinforcemen
t.
It
do
es
not,
ho
w
ev
er,
address
an
y
of
the
issues
in
v
olv
ed
in
generalizing
o
v
er
large
state
and/or
action
spaces.
In
addition,
it
ma
y
con
v
erge
quite
slo
wly
to
a
go
o
d
p
olicy
.
.
Mo
del-free
Learning
With
Av
erage
Rew
ard
As
describ
ed,
Q-learning
can
b
e
applied
to
discoun
ted
innite-horizon
MDPs.
It
can
also
b
e
applied
to
undiscoun
ted
problems
as
long
as
the
optimal
p
olicy
is
guaran
teed
to
reac
h
a
rew
ard-free
absorbing
state
and
the
state
is
p
erio
dicall
y
reset.
Sc
h
w
artz
(		)
examined
the
problem
of
adapting
Q-learning
to
an
a
v
erage-rew
ard
framew
ork.
Although
his
R-learning
algorithm
seems
to
exhibit
con
v
ergence
problems
for
some
MDPs,
sev
eral
researc
hers
ha
v
e
found
the
a
v
erage-rew
ard
criterion
closer
to
the
true
problem
they
wish
to
solv
e
than
a
discoun
ted
criterion
and
therefore
prefer
R-learning
to
Q-learning
(Mahadev
an,
		).
With
that
in
mind,
researc
hers
ha
v
e
studied
the
problem
of
learning
optimal
a
v
erage-
rew
ard
p
olicies.
Mahadev
an
(		)
surv
ey
ed
mo
del-based
a
v
erage-rew
ard
algorithms
from
a
reinforcemen
t-learning
p
ersp
ectiv
e
and
found
sev
eral
diculties
with
existing
algorithms.
In
particular,
he
sho
w
ed
that
existing
reinforcemen
t-learning
algorithms
for
a
v
erage
rew
ard
(and
some
dynamic
programming
algorithms)
do
not
alw
a
ys
pro
duce
bias-optimal
p
oli-
cies.
Jaakk
ola,
Jordan
and
Singh
(		)
describ
ed
an
a
v
erage-rew
ard
learning
algorithm
with
guaran
teed
con
v
ergence
prop
erties.
It
uses
a
Mon
te-Carlo
comp
onen
t
to
estimate
the
exp
ected
future
rew
ard
for
eac
h
state
as
the
agen
t
mo
v
es
through
the
en
vironmen
t.
In


----- Page 19 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
addition,
Bertsek
as
presen
ts
a
Q-learning-lik
e
algorithm
for
a
v
erage-case
rew
ard
in
his
new
textb
o
ok
(		).
Although
this
recen
t
w
ork
pro
vides
a
m
uc
h
needed
theoretical
foundation
to
this
area
of
reinforcemen
t
learning,
man
y
imp
ortan
t
problems
remain
unsolv
ed.
.
Computing
Optimal
P
olicies
b
y
Learning
Mo
dels
The
previous
section
sho
w
ed
ho
w
it
is
p
ossible
to
learn
an
optimal
p
olicy
without
kno
wing
the
mo
dels
T
(s;
a;
s
0
)
or
R(s;
a)
and
without
ev
en
learning
those
mo
dels
en
route.
Although
man
y
of
these
metho
ds
are
guaran
teed
to
nd
optimal
p
olicies
ev
en
tually
and
use
v
ery
little
computation
time
p
er
exp
erience,
they
mak
e
extremely
inecien
t
use
of
the
data
they
gather
and
therefore
often
require
a
great
deal
of
exp
erience
to
ac
hiev
e
go
o
d
p
erformance.
In
this
section
w
e
still
b
egin
b
y
assuming
that
w
e
don't
kno
w
the
mo
dels
in
adv
ance,
but
w
e
examine
algorithms
that
do
op
erate
b
y
learning
these
mo
dels.
These
algorithms
are
esp
ecially
imp
ortan
t
in
applications
in
whic
h
computation
is
considered
to
b
e
c
heap
and
real-w
orld
exp
erience
costly
.
.
Certain
t
y
Equiv
alen
t
Metho
ds
W
e
b
egin
with
the
most
conceptually
straigh
tforw
ard
metho
d:
rst,
learn
the
T
and
R
functions
b
y
exploring
the
en
vironmen
t
and
k
eeping
statistics
ab
out
the
results
of
eac
h
action;
next,
compute
an
optimal
p
olicy
using
one
of
the
metho
ds
of
Section
.
This
metho
d
is
kno
wn
as
c
ertainty
e
quivlanc
e
(Kumar
&
V
araiy
a,
	).
There
are
some
serious
ob
jections
to
this
metho
d:

It
mak
es
an
arbitrary
division
b
et
w
een
the
learning
phase
and
the
acting
phase.

Ho
w
should
it
gather
data
ab
out
the
en
vironmen
t
initially?
Random
exploration
migh
t
b
e
dangerous,
and
in
some
en
vironmen
ts
is
an
immensely
inecien
t
metho
d
of
gathering
data,
requiring
exp
onen
tially
more
data
(Whitehead,
		)
than
a
system
that
in
terlea
v
es
exp
erience
gathering
with
p
olicy-buil
din
g
more
tigh
tly
(Ko
enig
&
Simmons,
		).
See
Figure

for
an
example.

The
p
ossibilit
y
of
c
hanges
in
the
en
vironmen
t
is
also
problematic.
Breaking
up
an
agen
t's
life
in
to
a
pure
learning
and
a
pure
acting
phase
has
a
considerable
risk
that
the
optimal
con
troller
based
on
early
life
b
ecomes,
without
detection,
a
sub
optimal
con
troller
if
the
en
vironmen
t
c
hanges.
A
v
ariation
on
this
idea
is
c
ertainty
e
quivalenc
e,
in
whic
h
the
mo
del
is
learned
con
tin
ually
through
the
agen
t's
lifetime
and,
at
eac
h
step,
the
curren
t
mo
del
is
used
to
compute
an
optimal
p
olicy
and
v
alue
function.
This
metho
d
mak
es
v
ery
eectiv
e
use
of
a
v
ailable
data,
but
still
ignores
the
question
of
exploration
and
is
extremely
computationally
demanding,
ev
en
for
fairly
small
state
spaces.
F
ortunately
,
there
are
a
n
um
b
er
of
other
mo
del-based
algorithms
that
are
more
practical.
.
Dyna
Sutton's
Dyna
arc
hitecture
(		0,
		)
exploits
a
middle
ground,
yielding
strategies
that
are
b
oth
more
eectiv
e
than
mo
del-free
learning
and
more
computationally
ecien
t
than


----- Page 20 (native) -----
Kaelbling,
Littman,
&
Moore
. . . . . . . 
Goal
1
2
3
n
Figure
:
In
this
en
vironmen
t,
due
to
Whitehead
(		),
random
exploration
w
ould
tak
e
tak
e
O
(
n
)
steps
to
reac
h
the
goal
ev
en
once,
whereas
a
more
in
telligen
t
explo-
ration
strategy
(e.g.
\assume
an
y
un
tried
action
leads
directly
to
goal")
w
ould
require
only
O
(n

)
steps.
the
certain
t
y-equiv
alence
approac
h.
It
sim
ultaneously
uses
exp
erience
to
build
a
mo
del
(
^
T
and
^
R),
uses
exp
erience
to
adjust
the
p
olicy
,
and
uses
the
mo
del
to
adjust
the
p
olicy
.
Dyna
op
erates
in
a
lo
op
of
in
teraction
with
the
en
vironmen
t.
Giv
en
an
exp
erience
tuple
hs;
a;
s
0
;
r
i,
it
b
eha
v
es
as
follo
ws:

Up
date
the
mo
del,
incremen
ting
statistics
for
the
transition
from
s
to
s
0
on
action
a
and
for
receiving
rew
ard
r
for
taking
action
a
in
state
s.
The
up
dated
mo
dels
are
^
T
and
^
R.

Up
date
the
p
olicy
at
state
s
based
on
the
newly
up
dated
mo
del
using
the
rule
Q(s;
a)
:=
^
R(s;
a)
+

X
s
0
^
T
(s;
a;
s
0
)
max
a
0
Q(s
0
;
a
0
)
;
whic
h
is
a
v
ersion
of
the
v
alue-iteration
up
date
for
Q
v
alues.

P
erform
k
additional
up
dates:
c
ho
ose
k
state-action
pairs
at
random
and
up
date
them
according
to
the
same
rule
as
b
efore:
Q(s
k
;
a
k
):=
^
R(s
k
;
a
k
)
+

X
s
0
^
T
(s
k
;
a
k
;
s
0
)
max
a
0
Q(s
0
;
a
0
)
:

Cho
ose
an
action
a
0
to
p
erform
in
state
s
0
,
based
on
the
Q
v
alues
but
p
erhaps
mo
died
b
y
an
exploration
strategy
.
The
Dyna
algorithm
requires
ab
out
k
times
the
computation
of
Q-learning
p
er
instance,
but
this
is
t
ypically
v
astly
less
than
for
the
naiv
e
mo
del-based
metho
d.
A
reasonable
v
alue
of
k
can
b
e
determined
based
on
the
relativ
e
sp
eeds
of
computation
and
of
taking
action.
Figure

sho
ws
a
grid
w
orld
in
whic
h
in
eac
h
cell
the
agen
t
has
four
actions
(N,
S,
E,
W)
and
transitions
are
made
deterministically
to
an
adjacen
t
cell,
unless
there
is
a
blo
c
k,
in
whic
h
case
no
mo
v
emen
t
o
ccurs.
As
w
e
will
see
in
T
able
,
Dyna
requires
an
order
of
magnitude
few
er
steps
of
exp
erience
than
do
es
Q-learning
to
arriv
e
at
an
optimal
p
olicy
.
Dyna
requires
ab
out
six
times
more
computational
eort,
ho
w
ev
er.


----- Page 21 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
Figure
:
A
-state
grid
w
orld.
This
w
as
form
ulated
as
a
shortest-path
reinforcemen
t-
learning
problem,
whic
h
yields
the
same
result
as
if
a
rew
ard
of

is
giv
en
at
the
goal,
a
rew
ard
of
zero
elsewhere
and
a
discoun
t
factor
is
used.
Steps
b
efore
Bac
kups
b
efore
con
v
ergence
con
v
ergence
Q-learning
,000
,000
Dyna
,000
,0,000
prioritized
sw
eeping
,000
,00,000
T
able
:
The
p
erformance
of
three
algorithms
describ
ed
in
the
text.
All
metho
ds
used
the
exploration
heuristic
of
\optimism
in
the
face
of
uncertain
t
y":
an
y
state
not
previously
visited
w
as
assumed
b
y
default
to
b
e
a
goal
state.
Q-learning
used
its
optimal
learning
rate
parameter
for
a
deterministic
maze:

=
.
Dyna
and
prioritized
sw
eeping
w
ere
p
ermitted
to
tak
e
k
=
00
bac
kups
p
er
transition.
F
or
prioritized
sw
eeping,
the
priorit
y
queue
often
emptied
b
efore
all
bac
kups
w
ere
used.


----- Page 22 (native) -----
Kaelbling,
Littman,
&
Moore
.
Prioritized
Sw
eeping
/
Queue-Dyna
Although
Dyna
is
a
great
impro
v
emen
t
on
previous
metho
ds,
it
suers
from
b
eing
relativ
ely
undirected.
It
is
particularly
unhelpful
when
the
goal
has
just
b
een
reac
hed
or
when
the
agen
t
is
stuc
k
in
a
dead
end;
it
con
tin
ues
to
up
date
random
state-action
pairs,
rather
than
concen
trating
on
the
\in
teresting"
parts
of
the
state
space.
These
problems
are
addressed
b
y
prioritized
sw
eeping
(Mo
ore
&
A
tk
eson,
		)
and
Queue-Dyna
(P
eng
&
Williams,
		),
whic
h
are
t
w
o
indep
enden
tly-dev
el
op
ed
but
v
ery
similar
tec
hniques.
W
e
will
describ
e
prioritized
sw
eeping
in
some
detail.
The
algorithm
is
similar
to
Dyna,
except
that
up
dates
are
no
longer
c
hosen
at
random
and
v
alues
are
no
w
asso
ciated
with
states
(as
in
v
alue
iteration)
instead
of
state-action
pairs
(as
in
Q-learning).
T
o
mak
e
appropriate
c
hoices,
w
e
m
ust
store
additional
information
in
the
mo
del.
Eac
h
state
remem
b
ers
its
pr
e
de
c
essors:
the
states
that
ha
v
e
a
non-zero
transition
probabilit
y
to
it
under
some
action.
In
addition,
eac
h
state
has
a
priority,
initially
set
to
zero.
Instead
of
up
dating
k
random
state-action
pairs,
prioritized
sw
eeping
up
dates
k
states
with
the
highest
priorit
y
.
F
or
eac
h
high-priorit
y
state
s,
it
w
orks
as
follo
ws:

Remem
b
er
the
curren
t
v
alue
of
the
state:
V
old
=
V
(s).

Up
date
the
state's
v
alue
V
(s)
:=
max
a
 
^
R(s;
a)
+

X
s
0
^
T
(s;
a;
s
0
)V
(s
0
)
!
:

Set
the
state's
priorit
y
bac
k
to
0.

Compute
the
v
alue
c
hange

=
jV
old
 V
(s)j.

Use

to
mo
dify
the
priorities
of
the
predecessors
of
s.
If
w
e
ha
v
e
up
dated
the
V
v
alue
for
state
s
0
and
it
has
c
hanged
b
y
amoun
t
,
then
the
immediate
predecessors
of
s
0
are
informed
of
this
ev
en
t.
An
y
state
s
for
whic
h
there
exists
an
action
a
suc
h
that
^
T
(s;
a;
s
0
)
=
0
has
its
priorit
y
promoted
to


^
T
(s;
a;
s
0
),
unless
its
priorit
y
already
exceeded
that
v
alue.
The
global
b
eha
vior
of
this
algorithm
is
that
when
a
real-w
orld
transition
is
\surprising"
(the
agen
t
happ
ens
up
on
a
goal
state,
for
instance),
then
lots
of
computation
is
directed
to
propagate
this
new
information
bac
k
to
relev
an
t
predecessor
states.
When
the
real-
w
orld
transition
is
\b
oring"
(the
actual
result
is
v
ery
similar
to
the
predicted
result),
then
computation
con
tin
ues
in
the
most
deserving
part
of
the
space.
Running
prioritized
sw
eeping
on
the
problem
in
Figure
,
w
e
see
a
large
impro
v
emen
t
o
v
er
Dyna.
The
optimal
p
olicy
is
reac
hed
in
ab
out
half
the
n
um
b
er
of
steps
of
exp
erience
and
one-third
the
computation
as
Dyna
required
(and
therefore
ab
out
0
times
few
er
steps
and
t
wice
the
computational
eort
of
Q-learning).


----- Page 23 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
.
Other
Mo
del-Based
Metho
ds
Metho
ds
prop
osed
for
solving
MDPs
giv
en
a
mo
del
can
b
e
used
in
the
con
text
of
mo
del-
based
metho
ds
as
w
ell.
R
TDP
(real-time
dynamic
programming)
(Barto,
Bradtk
e,
&
Singh,
		)
is
another
mo
del-based
metho
d
that
uses
Q-learning
to
concen
trate
computational
eort
on
the
areas
of
the
state-space
that
the
agen
t
is
most
lik
ely
to
o
ccup
y
.
It
is
sp
ecic
to
problems
in
whic
h
the
agen
t
is
trying
to
ac
hiev
e
a
particular
goal
state
and
the
rew
ard
ev
erywhere
else
is
0.
By
taking
in
to
accoun
t
the
start
state,
it
can
nd
a
short
path
from
the
start
to
the
goal,
without
necessarily
visiting
the
rest
of
the
state
space.
The
Plexus
planning
system
(Dean,
Kaelbling,
Kirman,
&
Nic
holson,
		;
Kirman,
		)
exploits
a
similar
in
tuition.
It
starts
b
y
making
an
appro
ximate
v
ersion
of
the
MDP
whic
h
is
m
uc
h
smaller
than
the
original
one.
The
appro
ximate
MDP
con
tains
a
set
of
states,
called
the
envelop
e,
that
includes
the
agen
t's
curren
t
state
and
the
goal
state,
if
there
is
one.
States
that
are
not
in
the
en
v
elop
e
are
summarized
b
y
a
single
\out"
state.
The
planning
pro
cess
is
an
alternation
b
et
w
een
nding
an
optimal
p
olicy
on
the
appro
ximate
MDP
and
adding
useful
states
to
the
en
v
elop
e.
Action
ma
y
tak
e
place
in
parallel
with
planning,
in
whic
h
case
irrelev
an
t
states
are
also
pruned
out
of
the
en
v
elop
e.
.
Generalization
All
of
the
previous
discussion
has
tacitly
assumed
that
it
is
p
ossible
to
en
umerate
the
state
and
action
spaces
and
store
tables
of
v
alues
o
v
er
them.
Except
in
v
ery
small
en
vironmen
ts,
this
means
impractical
memory
requiremen
ts.
It
also
mak
es
inecien
t
use
of
exp
erience.
In
a
large,
smo
oth
state
space
w
e
generally
exp
ect
similar
states
to
ha
v
e
similar
v
alues
and
sim-
ilar
optimal
actions.
Surely
,
therefore,
there
should
b
e
some
more
compact
represen
tation
than
a
table.
Most
problems
will
ha
v
e
con
tin
uous
or
large
discrete
state
spaces;
some
will
ha
v
e
large
or
con
tin
uous
action
spaces.
The
problem
of
learning
in
large
spaces
is
addressed
through
gener
alization
te
chniques,
whic
h
allo
w
compact
storage
of
learned
information
and
transfer
of
kno
wledge
b
et
w
een
\similar"
states
and
actions.
The
large
literature
of
generalization
tec
hniques
from
inductiv
e
concept
learning
can
b
e
applied
to
reinforcemen
t
learning.
Ho
w
ev
er,
tec
hniques
often
need
to
b
e
tailored
to
sp
ecic
details
of
the
problem.
In
the
follo
wing
sections,
w
e
explore
the
application
of
standard
function-appro
ximation
tec
hniques,
adaptiv
e
resolution
mo
dels,
and
hierarc
hical
metho
ds
to
the
problem
of
reinforcemen
t
learning.
The
reinforcemen
t-learning
arc
hitectures
and
algorithms
discussed
ab
o
v
e
ha
v
e
included
the
storage
of
a
v
ariet
y
of
mappings,
including
S
!
A
(p
olicies),
S
!
<
(v
alue
functions),
S

A
!
<
(Q
functions
and
rew
ards),
S

A
!
S
(deterministic
transitions),
and
S

A

S
!
[0;
]
(transition
probabilities).
Some
of
these
mappings,
suc
h
as
transitions
and
immediate
rew
ards,
can
b
e
learned
using
straigh
tforw
ard
sup
ervised
learning,
and
can
b
e
handled
using
an
y
of
the
wide
v
ariet
y
of
function-appro
ximation
tec
hniques
for
sup
ervised
learning
that
supp
ort
noisy
training
examples.
P
opular
tec
hniques
include
v
arious
neural-
net
w
ork
metho
ds
(Rumelhart
&
McClelland,
	),
fuzzy
logic
(Berenji,
		;
Lee,
		).
CMA
C
(Albus,
	),
and
lo
cal
memory-based
metho
ds
(Mo
ore,
A
tk
eson,
&
Sc
haal,
		),
suc
h
as
generalizations
of
nearest
neigh
b
or
metho
ds.
Other
mappings,
esp
ecially
the
p
olicy


----- Page 24 (native) -----
Kaelbling,
Littman,
&
Moore
mapping,
t
ypically
need
sp
ecialized
algorithms
b
ecause
training
sets
of
input-output
pairs
are
not
a
v
ailable.
.
Generalization
o
v
er
Input
A
reinforcemen
t-learning
agen
t's
curren
t
state
pla
ys
a
cen
tral
role
in
its
selection
of
rew
ard-
maximizing
actions.
Viewing
the
agen
t
as
a
state-free
blac
k
b
o
x,
a
description
of
the
curren
t
state
is
its
input.
Dep
ending
on
the
agen
t
arc
hitecture,
its
output
is
either
an
action
selection,
or
an
ev
aluation
of
the
curren
t
state
that
can
b
e
used
to
select
an
action.
The
problem
of
deciding
ho
w
the
dieren
t
asp
ects
of
an
input
aect
the
v
alue
of
the
output
is
sometimes
called
the
\structural
credit-assignmen
t"
problem.
This
section
examines
approac
hes
to
generating
actions
or
ev
aluations
as
a
function
of
a
description
of
the
agen
t's
curren
t
state.
The
rst
group
of
tec
hniques
co
v
ered
here
is
sp
ecialized
to
the
case
when
rew
ard
is
not
dela
y
ed;
the
second
group
is
more
generally
applicable.
..
Immedia
te
Rew
ard
When
the
agen
t's
actions
do
not
inuence
state
transitions,
the
resulting
problem
b
ecomes
one
of
c
ho
osing
actions
to
maximize
immediate
rew
ard
as
a
function
of
the
agen
t's
curren
t
state.
These
problems
b
ear
a
resem
blance
to
the
bandit
problems
discussed
in
Section

except
that
the
agen
t
should
condition
its
action
selection
on
the
curren
t
state.
F
or
this
reason,
this
class
of
problems
has
b
een
describ
ed
as
asso
ciative
reinforcemen
t
learning.
The
algorithms
in
this
section
address
the
problem
of
learning
from
immediate
b
o
olean
reinforcemen
t
where
the
state
is
v
ector
v
alued
and
the
action
is
a
b
o
olean
v
ector.
Suc
h
algorithms
can
and
ha
v
e
b
een
used
in
the
con
text
of
a
dela
y
ed
reinforcemen
t,
for
instance,
as
the
RL
comp
onen
t
in
the
AHC
arc
hitecture
describ
ed
in
Section
..
They
can
also
b
e
generalized
to
real-v
alued
rew
ard
through
r
ewar
d
c
omp
arison
metho
ds
(Sutton,
	).
CRBP
The
complemen
tary
reinforcemen
t
bac
kpropagation
algorithm
(Ac
kley
&
Littman,
		0)
(crbp)
consists
of
a
feed-forw
ard
net
w
ork
mapping
an
enco
ding
of
the
state
to
an
enco
ding
of
the
action.
The
action
is
determined
probabilistically
from
the
activ
ation
of
the
output
units:
if
output
unit
i
has
activ
ation
y
i
,
then
bit
i
of
the
action
v
ector
has
v
alue

with
probabilit
y
y
i
,
and
0
otherwise.
An
y
neural-net
w
ork
sup
ervised
training
pro
cedure
can
b
e
used
to
adapt
the
net
w
ork
as
follo
ws.
If
the
result
of
generating
action
a
is
r
=
,
then
the
net
w
ork
is
trained
with
input-output
pair
hs;
ai.
If
the
result
is
r
=
0,
then
the
net
w
ork
is
trained
with
input-output
pair
hs;

a
i,
where

a
=
(
 a

;
:
:
:
;

 a
n
).
The
idea
b
ehind
this
training
rule
is
that
whenev
er
an
action
fails
to
generate
rew
ard,
crbp
will
try
to
generate
an
action
that
is
dieren
t
from
the
curren
t
c
hoice.
Although
it
seems
lik
e
the
algorithm
migh
t
oscillate
b
et
w
een
an
action
and
its
complemen
t,
that
do
es
not
happ
en.
One
step
of
training
a
net
w
ork
will
only
c
hange
the
action
sligh
tly
and
since
the
output
probabilities
will
tend
to
mo
v
e
to
w
ard
0.,
this
mak
es
action
selection
more
random
and
increases
searc
h.
The
hop
e
is
that
the
random
distribution
will
generate
an
action
that
w
orks
b
etter,
and
then
that
action
will
b
e
reinforced.
AR
C
The
asso
ciativ
e
reinforcemen
t
comparison
(ar
c)
algorithm
(Sutton,
	)
is
an
instance
of
the
ahc
arc
hitecture
for
the
case
of
b
o
olean
actions,
consisting
of
t
w
o
feed-
0

----- Page 25 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
forw
ard
net
w
orks.
One
learns
the
v
alue
of
situations,
the
other
learns
a
p
olicy
.
These
can
b
e
simple
linear
net
w
orks
or
can
ha
v
e
hidden
units.
In
the
simplest
case,
the
en
tire
system
learns
only
to
optimize
immediate
rew
ard.
First,
let
us
consider
the
b
eha
vior
of
the
net
w
ork
that
learns
the
p
olicy
,
a
mapping
from
a
v
ector
describing
s
to
a
0
or
.
If
the
output
unit
has
activ
ation
y
i
,
then
a,
the
action
generated,
will
b
e

if
y
+

>
0,
where

is
normal
noise,
and
0
otherwise.
The
adjustmen
t
for
the
output
unit
is,
in
the
simplest
case,
e
=
r
(a
 =)
;
where
the
rst
factor
is
the
rew
ard
receiv
ed
for
taking
the
most
recen
t
action
and
the
second
enco
des
whic
h
action
w
as
tak
en.
The
actions
are
enco
ded
as
0
and
,
so
a
 =
alw
a
ys
has
the
same
magnitude;
if
the
rew
ard
and
the
action
ha
v
e
the
same
sign,
then
action

will
b
e
made
more
lik
ely
,
otherwise
action
0
will
b
e.
As
describ
ed,
the
net
w
ork
will
tend
to
seek
actions
that
giv
en
p
ositiv
e
rew
ard.
T
o
extend
this
approac
h
to
maximize
rew
ard,
w
e
can
compare
the
rew
ard
to
some
baseline,
b.
This
c
hanges
the
adjustmen
t
to
e
=
(r
 b)(a
 =)
;
where
b
is
the
output
of
the
second
net
w
ork.
The
second
net
w
ork
is
trained
in
a
standard
sup
ervised
mo
de
to
estimate
r
as
a
function
of
the
input
state
s.
V
ariations
of
this
approac
h
ha
v
e
b
een
used
in
a
v
ariet
y
of
applications
(Anderson,
	;
Barto
et
al.,
	;
Lin,
		b;
Sutton,
	).
REINF
OR
CE
Algorithms
Williams
(	,
		)
studied
the
problem
of
c
ho
osing
ac-
tions
to
maximize
immedate
rew
ard.
He
iden
tied
a
broad
class
of
up
date
rules
that
p
er-
form
gradien
t
descen
t
on
the
exp
ected
rew
ard
and
sho
w
ed
ho
w
to
in
tegrate
these
rules
with
bac
kpropagation.
This
class,
called
reinf
or
ce
algorithms,
includes
linear
rew
ard-inaction
(Section
..)
as
a
sp
ecial
case.
The
generic
reinf
or
ce
up
date
for
a
parameter
w
ij
can
b
e
written
w
ij
=

ij
(r
 b
ij
)
@
@
w
ij
ln
(g
j
)
where

ij
is
a
non-negativ
e
factor,
r
the
curren
t
reinforcemen
t,
b
ij
a
reinforcemen
t
baseline,
and
g
i
is
the
probabilit
y
densit
y
function
used
to
randomly
generate
actions
based
on
unit
activ
ations.
Both

ij
and
b
ij
can
tak
e
on
dieren
t
v
alues
for
eac
h
w
ij
,
ho
w
ev
er,
when

ij
is
constan
t
throughout
the
system,
the
exp
ected
up
date
is
exactly
in
the
direction
of
the
exp
ected
rew
ard
gradien
t.
Otherwise,
the
up
date
is
in
the
same
half
space
as
the
gradien
t
but
not
necessarily
in
the
direction
of
steep
est
increase.
Williams
p
oin
ts
out
that
the
c
hoice
of
baseline,
b
ij
,
can
ha
v
e
a
profound
eect
on
the
con
v
ergence
sp
eed
of
the
algorithm.
Logic-Based
Metho
ds
Another
strategy
for
generalization
in
reinforcemen
t
learning
is
to
reduce
the
learning
problem
to
an
asso
ciativ
e
problem
of
learning
b
o
olean
functions.
A
b
o
olean
function
has
a
v
ector
of
b
o
olean
inputs
and
a
single
b
o
olean
output.
T
aking
inspiration
from
mainstream
mac
hine
learning
w
ork,
Kaelbling
dev
elop
ed
t
w
o
algorithms
for
learning
b
o
olean
functions
from
reinforcemen
t:
one
uses
the
bias
of
k
-DNF
to
driv
e


----- Page 26 (native) -----
Kaelbling,
Littman,
&
Moore
the
generalization
pro
cess
(Kaelbling,
		b);
the
other
searc
hes
the
space
of
syn
tactic
descriptions
of
functions
using
a
simple
generate-and-test
metho
d
(Kaelbling,
		a).
The
restriction
to
a
single
b
o
olean
output
mak
es
these
tec
hniques
dicult
to
apply
.
In
v
ery
b
enign
learning
situations,
it
is
p
ossible
to
extend
this
approac
h
to
use
a
collection
of
learners
to
indep
enden
tly
learn
the
individual
bits
that
mak
e
up
a
complex
output.
In
general,
ho
w
ev
er,
that
approac
h
suers
from
the
problem
of
v
ery
unreliable
reinforcemen
t:
if
a
single
learner
generates
an
inappropriate
output
bit,
all
of
the
learners
receiv
e
a
lo
w
reinforcemen
t
v
alue.
The
cascade
metho
d
(Kaelbling,
		b)
allo
ws
a
collection
of
learners
to
b
e
trained
collectiv
ely
to
generate
appropriate
join
t
outputs;
it
is
considerably
more
reliable,
but
can
require
additional
computational
eort.
..
Dela
yed
Rew
ard
Another
metho
d
to
allo
w
reinforcemen
t-learning
tec
hniques
to
b
e
applied
in
large
state
spaces
is
mo
deled
on
v
alue
iteration
and
Q-learning.
Here,
a
function
appro
ximator
is
used
to
represen
t
the
v
alue
function
b
y
mapping
a
state
description
to
a
v
alue.
Man
y
reseac
hers
ha
v
e
exp
erimen
ted
with
this
approac
h:
Bo
y
an
and
Mo
ore
(		)
used
lo
cal
memory-based
metho
ds
in
conjunction
with
v
alue
iteration;
Lin
(		)
used
bac
kprop-
agation
net
w
orks
for
Q-learning;
W
atkins
(		)
used
CMA
C
for
Q-learning;
T
esauro
(		,
		)
used
bac
kpropagation
for
learning
the
v
alue
function
in
bac
kgammon
(describ
ed
in
Section
.);
Zhang
and
Dietteric
h
(		)
used
bac
kpropagation
and
T
D
()
to
learn
go
o
d
strategies
for
job-shop
sc
heduling.
Although
there
ha
v
e
b
een
some
p
ositiv
e
examples,
in
general
there
are
unfortunate
in-
teractions
b
et
w
een
function
appro
ximation
and
the
learning
rules.
In
discrete
en
vironmen
ts
there
is
a
guaran
tee
that
an
y
op
eration
that
up
dates
the
v
alue
function
(according
to
the
Bellman
equations)
can
only
reduce
the
error
b
et
w
een
the
curren
t
v
alue
function
and
the
optimal
v
alue
function.
This
guaran
tee
no
longer
holds
when
generalization
is
used.
These
issues
are
discussed
b
y
Bo
y
an
and
Mo
ore
(		),
who
giv
e
some
simple
examples
of
v
alue
function
errors
gro
wing
arbitrarily
large
when
generalization
is
used
with
v
alue
iteration.
Their
solution
to
this,
applicable
only
to
certain
classes
of
problems,
discourages
suc
h
div
er-
gence
b
y
only
p
ermitting
up
dates
whose
estimated
v
alues
can
b
e
sho
wn
to
b
e
near-optimal
via
a
battery
of
Mon
te-Carlo
exp
erimen
ts.
Thrun
and
Sc
h
w
artz
(		)
theorize
that
function
appro
ximation
of
v
alue
functions
is
also
dangerous
b
ecause
the
errors
in
v
alue
functions
due
to
generalization
can
b
ecome
comp
ounded
b
y
the
\max"
op
erator
in
the
denition
of
the
v
alue
function.
Sev
eral
recen
t
results
(Gordon,
		;
Tsitsiklis
&
V
an
Ro
y
,
		)
sho
w
ho
w
the
appro-
priate
c
hoice
of
function
appro
ximator
can
guaran
tee
con
v
ergence,
though
not
necessarily
to
the
optimal
v
alues.
Baird's
r
esidual
gr
adient
tec
hnique
(Baird,
		)
pro
vides
guaran
teed
con
v
ergence
to
lo
cally
optimal
solutions.
P
erhaps
the
glo
ominess
of
these
coun
ter-examples
is
misplaced.
Bo
y
an
and
Mo
ore
(		)
rep
ort
that
their
coun
ter-examples
c
an
b
e
made
to
w
ork
with
problem-sp
ecic
hand-tuning
despite
the
unreliabilit
y
of
un
tuned
algorithms
that
pro
v
ably
con
v
erge
in
discrete
domains.
Sutton
(		)
sho
ws
ho
w
mo
died
v
ersions
of
Bo
y
an
and
Mo
ore's
examples
can
con
v
erge
successfully
.
An
op
en
question
is
whether
general
principles,
ideally
supp
orted
b
y
theory
,
can
help
us
understand
when
v
alue
function
appro
ximation
will
succeed.
In
Sutton's
com-


----- Page 27 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
parativ
e
exp
erimen
ts
with
Bo
y
an
and
Mo
ore's
coun
ter-examples,
he
c
hanges
four
asp
ects
of
the
exp
erimen
ts:
.
Small
c
hanges
to
the
task
sp
ecications.
.
A
v
ery
dieren
t
kind
of
function
appro
ximator
(CMA
C
(Albus,
	))
that
has
w
eak
generalization.
.
A
dieren
t
learning
algorithm:
SARSA
(Rummery
&
Niranjan,
		)
instead
of
v
alue
iteration.
.
A
dieren
t
training
regime.
Bo
y
an
and
Mo
ore
sampled
states
uniformly
in
state
space,
whereas
Sutton's
metho
d
sampled
along
empirical
tra
jectories.
There
are
in
tuitiv
e
reasons
to
b
eliev
e
that
the
fourth
factor
is
particularly
imp
ortan
t,
but
more
careful
researc
h
is
needed.
Adaptiv
e
Resolution
Mo
dels
In
man
y
cases,
what
w
e
w
ould
lik
e
to
do
is
partition
the
en
vironmen
t
in
to
regions
of
states
that
can
b
e
considered
the
same
for
the
purp
oses
of
learning
and
generating
actions.
Without
detailed
prior
kno
wledge
of
the
en
vironmen
t,
it
is
v
ery
dicult
to
kno
w
what
gran
ularit
y
or
placemen
t
of
partitions
is
appropriate.
This
problem
is
o
v
ercome
in
metho
ds
that
use
adaptiv
e
resolution;
during
the
course
of
learning,
a
partition
is
constructed
that
is
appropriate
to
the
en
vironmen
t.
Decision
T
rees
In
en
vironmen
ts
that
are
c
haracterized
b
y
a
set
of
b
o
olean
or
discrete-
v
alued
v
ariables,
it
is
p
ossible
to
learn
compact
decision
trees
for
represen
ting
Q
v
alues.
The
G-le
arning
algorithm
(Chapman
&
Kaelbling,
		),
w
orks
as
follo
ws.
It
starts
b
y
assuming
that
no
partitioning
is
necessary
and
tries
to
learn
Q
v
alues
for
the
en
tire
en
vironmen
t
as
if
it
w
ere
one
state.
In
parallel
with
this
pro
cess,
it
gathers
statistics
based
on
individual
input
bits;
it
asks
the
question
whether
there
is
some
bit
b
in
the
state
description
suc
h
that
the
Q
v
alues
for
states
in
whic
h
b
=

are
signican
tly
dieren
t
from
Q
v
alues
for
states
in
whic
h
b
=
0.
If
suc
h
a
bit
is
found,
it
is
used
to
split
the
decision
tree.
Then,
the
pro
cess
is
rep
eated
in
eac
h
of
the
lea
v
es.
This
metho
d
w
as
able
to
learn
v
ery
small
represen
tations
of
the
Q
function
in
the
presence
of
an
o
v
erwhelming
n
um
b
er
of
irrelev
an
t,
noisy
state
attributes.
It
outp
erformed
Q-learning
with
bac
kpropagation
in
a
simple
video-
game
en
vironmen
t
and
w
as
used
b
y
McCallum
(		)
(in
conjunction
with
other
tec
hniques
for
dealing
with
partial
observ
abilit
y)
to
learn
b
eha
viors
in
a
complex
driving-sim
ulator.
It
cannot,
ho
w
ev
er,
acquire
partitions
in
whic
h
attributes
are
only
signican
t
in
com
bination
(suc
h
as
those
needed
to
solv
e
parit
y
problems).
V
ariable
Resolution
Dynamic
Programming
The
VRDP
algorithm
(Mo
ore,
		)
enables
con
v
en
tional
dynamic
programming
to
b
e
p
erformed
in
real-v
alued
m
ultiv
ariate
state-spaces
where
straigh
tforw
ard
discretization
w
ould
fall
prey
to
the
curse
of
dimension-
alit
y
.
A
k
d-tree
(similar
to
a
decision
tree)
is
used
to
partition
state
space
in
to
coarse
regions.
The
coarse
regions
are
rened
in
to
detailed
regions,
but
only
in
parts
of
the
state
space
whic
h
are
predicted
to
b
e
imp
ortan
t.
This
notion
of
imp
ortance
is
obtained
b
y
run-
ning
\men
tal
tra
jectories"
through
state
space.
This
algorithm
pro
v
ed
eectiv
e
on
a
n
um
b
er
of
problems
for
whic
h
full
high-resolution
arra
ys
w
ould
ha
v
e
b
een
impractical.
It
has
the
disadv
an
tage
of
requiring
a
guess
at
an
initially
v
alid
tra
jectory
through
state-space.


----- Page 28 (native) -----
Kaelbling,
Littman,
&
Moore
G
Start
Goal
(a)
G
(b)
G
(c)
Figure
:
(a)
A
t
w
o-dimensional
maze
problem.
The
p
oin
t
rob
ot
m
ust
nd
a
path
from
start
to
goal
without
crossing
an
y
of
the
barrier
lines.
(b)
The
path
tak
en
b
y
P
artiGame
during
the
en
tire
rst
trial.
It
b
egins
with
in
tense
exploration
to
nd
a
route
out
of
the
almost
en
tirely
enclosed
start
region.
Ha
ving
ev
en
tually
reac
hed
a
sucien
tly
high
resolution,
it
disco
v
ers
the
gap
and
pro
ceeds
greedily
to
w
ards
the
goal,
only
to
b
e
temp
orarily
blo
c
k
ed
b
y
the
goal's
barrier
region.
(c)
The
second
trial.
P
artiGame
Algorithm
Mo
ore's
P
artiGame
algorithm
(Mo
ore,
		)
is
another
solution
to
the
problem
of
learning
to
ac
hiev
e
goal
congurations
in
deterministic
high-dimensional
con
tin
uous
spaces
b
y
learning
an
adaptiv
e-resolution
mo
del.
It
also
divides
the
en
vironmen
t
in
to
cells;
but
in
eac
h
cell,
the
actions
a
v
ailable
consist
of
aiming
at
the
neigh
b
oring
cells
(this
aiming
is
accomplished
b
y
a
lo
cal
con
troller,
whic
h
m
ust
b
e
pro
vided
as
part
of
the
problem
statemen
t).
The
graph
of
cell
transitions
is
solv
ed
for
shortest
paths
in
an
online
incremen
tal
manner,
but
a
minimax
criterion
is
used
to
detect
when
a
group
of
cells
is
to
o
coarse
to
prev
en
t
mo
v
emen
t
b
et
w
een
obstacles
or
to
a
v
oid
limit
cycles.
The
oending
cells
are
split
to
higher
resolution.
Ev
en
tually
,
the
en
vironmen
t
is
divided
up
just
enough
to
c
ho
ose
appropriate
actions
for
ac
hieving
the
goal,
but
no
unnecessary
distinctions
are
made.
An
imp
ortan
t
feature
is
that,
as
w
ell
as
reducing
memory
and
computational
requiremen
ts,
it
also
structures
exploration
of
state
space
in
a
m
ulti-resolution
manner.
Giv
en
a
failure,
the
agen
t
will
initially
try
something
v
ery
dieren
t
to
rectify
the
failure,
and
only
resort
to
small
lo
cal
c
hanges
when
all
the
qualitativ
ely
dieren
t
strategies
ha
v
e
b
een
exhausted.
Figure
a
sho
ws
a
t
w
o-dimensional
con
tin
uous
maze.
Figure
b
sho
ws
the
p
erformance
of
a
rob
ot
using
the
P
artiGame
algorithm
during
the
v
ery
rst
trial.
Figure
c
sho
ws
the
second
trial,
started
from
a
sligh
tly
dieren
t
p
osition.
This
is
a
v
ery
fast
algorithm,
learning
p
olicies
in
spaces
of
up
to
nine
dimensions
in
less
than
a
min
ute.
The
restriction
of
the
curren
t
implemen
tation
to
deterministic
en
vironmen
ts
limits
its
applicabili
t
y
,
ho
w
ev
er.
McCallum
(		)
suggests
some
related
tree-structured
metho
ds.


----- Page 29 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
.
Generalization
o
v
er
Actions
The
net
w
orks
describ
ed
in
Section
..
generalize
o
v
er
state
descriptions
presen
ted
as
inputs.
They
also
pro
duce
outputs
in
a
discrete,
factored
represen
tation
and
th
us
could
b
e
seen
as
generalizing
o
v
er
actions
as
w
ell.
In
cases
suc
h
as
this
when
actions
are
describ
ed
com
binatorially
,
it
is
imp
ortan
t
to
generalize
o
v
er
actions
to
a
v
oid
k
eeping
separate
statistics
for
the
h
uge
n
um
b
er
of
actions
that
can
b
e
c
hosen.
In
con
tin
uous
action
spaces,
the
need
for
generalization
is
ev
en
more
pronounced.
When
estimating
Q
v
alues
using
a
neural
net
w
ork,
it
is
p
ossible
to
use
either
a
distinct
net
w
ork
for
eac
h
action,
or
a
net
w
ork
with
a
distinct
output
for
eac
h
action.
When
the
action
space
is
con
tin
uous,
neither
approac
h
is
p
ossible.
An
alternativ
e
strategy
is
to
use
a
single
net
w
ork
with
b
oth
the
state
and
action
as
input
and
Q
v
alue
as
the
output.
T
raining
suc
h
a
net
w
ork
is
not
conceptually
dicult,
but
using
the
net
w
ork
to
nd
the
optimal
action
can
b
e
a
c
hallenge.
One
metho
d
is
to
do
a
lo
cal
gradien
t-ascen
t
searc
h
on
the
action
in
order
to
nd
one
with
high
v
alue
(Baird
&
Klopf,
		).
Gullapalli
(		0,
		)
has
dev
elop
ed
a
\neural"
reinforcemen
t-learning
unit
for
use
in
con
tin
uous
action
spaces.
The
unit
generates
actions
with
a
normal
distribution;
it
adjusts
the
mean
and
v
ariance
based
on
previous
exp
erience.
When
the
c
hosen
actions
are
not
p
erforming
w
ell,
the
v
ariance
is
high,
resulting
in
exploration
of
the
range
of
c
hoices.
When
an
action
p
erforms
w
ell,
the
mean
is
mo
v
ed
in
that
direction
and
the
v
ariance
decreased,
resulting
in
a
tendency
to
generate
more
action
v
alues
near
the
successful
one.
This
metho
d
w
as
successfully
emplo
y
ed
to
learn
to
con
trol
a
rob
ot
arm
with
man
y
con
tin
uous
degrees
of
freedom.
.
Hierarc
hical
Metho
ds
Another
strategy
for
dealing
with
large
state
spaces
is
to
treat
them
as
a
hierarc
h
y
of
learning
problems.
In
man
y
cases,
hierarc
hical
solutions
in
tro
duce
sligh
t
sub-optimalit
y
in
p
erformance,
but
p
oten
tially
gain
a
go
o
d
deal
of
eciency
in
execution
time,
learning
time,
and
space.
Hierarc
hical
learners
are
commonly
structured
as
gate
d
b
ehaviors,
as
sho
wn
in
Figure
.
There
is
a
collection
of
b
ehaviors
that
map
en
vironmen
t
states
in
to
lo
w-lev
el
actions
and
a
gating
function
that
decides,
based
on
the
state
of
the
en
vironmen
t,
whic
h
b
eha
vior's
actions
should
b
e
switc
hed
through
and
actually
executed.
Maes
and
Bro
oks
(		0)
used
a
v
ersion
of
this
arc
hitecture
in
whic
h
the
individual
b
eha
viors
w
ere
xed
a
priori
and
the
gating
function
w
as
learned
from
reinforcemen
t.
Mahadev
an
and
Connell
(		b)
used
the
dual
approac
h:
they
xed
the
gating
function,
and
supplied
reinforcemen
t
functions
for
the
individual
b
eha
viors,
whic
h
w
ere
learned.
Lin
(		a)
and
Dorigo
and
Colom
b
etti
(		,
		)
b
oth
used
this
approac
h,
rst
training
the
b
eha
viors
and
then
training
the
gating
function.
Man
y
of
the
other
hierarc
hical
learning
metho
ds
can
b
e
cast
in
this
framew
ork.
..
Feud
al
Q-learning
F
eudal
Q-learning
(Da
y
an
&
Hin
ton,
		;
W
atkins,
		)
in
v
olv
es
a
hierarc
h
y
of
learning
mo
dules.
In
the
simplest
case,
there
is
a
high-lev
el
master
and
a
lo
w-lev
el
sla
v
e.
The
master
receiv
es
reinforcemen
t
from
the
external
en
vironmen
t.
Its
actions
consist
of
commands
that


----- Page 30 (native) -----
Kaelbling,
Littman,
&
Moore
s
b1
b2
b3
g
a
Figure
:
A
structure
of
gated
b
eha
viors.
it
can
giv
e
to
the
lo
w-lev
el
learner.
When
the
master
generates
a
particular
command
to
the
sla
v
e,
it
m
ust
rew
ard
the
sla
v
e
for
taking
actions
that
satisfy
the
command,
ev
en
if
they
do
not
result
in
external
reinforcemen
t.
The
master,
then,
learns
a
mapping
from
states
to
commands.
The
sla
v
e
learns
a
mapping
from
commands
and
states
to
external
actions.
The
set
of
\commands"
and
their
asso
ciated
reinforcemen
t
functions
are
established
in
adv
ance
of
the
learning.
This
is
really
an
instance
of
the
general
\gated
b
eha
viors"
approac
h,
in
whic
h
the
sla
v
e
can
execute
an
y
of
the
b
eha
viors
dep
ending
on
its
command.
The
reinforcemen
t
functions
for
the
individual
b
eha
viors
(commands)
are
giv
en,
but
learning
tak
es
place
sim
ultaneously
at
b
oth
the
high
and
lo
w
lev
els.
..
Compositional
Q-learning
Singh's
comp
ositional
Q-learning
(		b,
		a)
(C-QL)
consists
of
a
hierarc
h
y
based
on
the
temp
oral
sequencing
of
subgoals.
The
elemental
tasks
are
b
eha
viors
that
ac
hiev
e
some
recognizable
condition.
The
high-lev
el
goal
of
the
system
is
to
ac
hiev
e
some
set
of
condi-
tions
in
sequen
tial
order.
The
ac
hiev
emen
t
of
the
conditions
pro
vides
reinforcemen
t
for
the
elemen
tal
tasks,
whic
h
are
trained
rst
to
ac
hiev
e
individual
subgoals.
Then,
the
gating
function
learns
to
switc
h
the
elemen
tal
tasks
in
order
to
ac
hiev
e
the
appropriate
high-lev
el
sequen
tial
goal.
This
metho
d
w
as
used
b
y
Tham
and
Prager
(		)
to
learn
to
con
trol
a
sim
ulated
m
ulti-link
rob
ot
arm.
..
Hierar
chical
Dist
ance
to
Go
al
Esp
ecially
if
w
e
consider
reinforcemen
t
learning
mo
dules
to
b
e
part
of
larger
agen
t
arc
hi-
tectures,
it
is
imp
ortan
t
to
consider
problems
in
whic
h
goals
are
dynamically
input
to
the
learner.
Kaelbling's
HDG
algorithm
(		a)
uses
a
hierarc
hical
approac
h
to
solving
prob-
lems
when
goals
of
ac
hiev
emen
t
(the
agen
t
should
get
to
a
particular
state
as
quic
kly
as
p
ossible)
are
giv
en
to
an
agen
t
dynamically
.
The
HDG
algorithm
w
orks
b
y
analogy
with
na
vigation
in
a
harb
or.
The
en
vironmen
t
is
partitioned
(a
priori,
but
more
recen
t
w
ork
(Ashar,
		)
addresses
the
case
of
learning
the
partition)
in
to
a
set
of
regions
whose
cen
ters
are
kno
wn
as
\landmarks."
If
the
agen
t
is


----- Page 31 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
2/5
1/5
2/5
printer
ofï¬ce
+100
hall
hall
Figure
	:
An
example
of
a
partially
observ
able
en
vironmen
t.
curren
tly
in
the
same
region
as
the
goal,
then
it
uses
lo
w-lev
el
actions
to
mo
v
e
to
the
goal.
If
not,
then
high-lev
el
information
is
used
to
determine
the
next
landmark
on
the
shortest
path
from
the
agen
t's
closest
landmark
to
the
goal's
closest
landmark.
Then,
the
agen
t
uses
lo
w-lev
el
information
to
aim
to
w
ard
that
next
landmark.
If
errors
in
action
cause
deviations
in
the
path,
there
is
no
problem;
the
b
est
aiming
p
oin
t
is
recomputed
on
ev
ery
step.
.
P
artially
Observ
able
En
vironmen
ts
In
man
y
real-w
orld
en
vironmen
ts,
it
will
not
b
e
p
ossible
for
the
agen
t
to
ha
v
e
p
erfect
and
complete
p
erception
of
the
state
of
the
en
vironmen
t.
Unfortunately
,
complete
observ
abilit
y
is
necessary
for
learning
metho
ds
based
on
MDPs.
In
this
section,
w
e
consider
the
case
in
whic
h
the
agen
t
mak
es
observations
of
the
state
of
the
en
vironmen
t,
but
these
observ
ations
ma
y
b
e
noisy
and
pro
vide
incomplete
information.
In
the
case
of
a
rob
ot,
for
instance,
it
migh
t
observ
e
whether
it
is
in
a
corridor,
an
op
en
ro
om,
a
T-junction,
etc.,
and
those
observ
ations
migh
t
b
e
error-prone.
This
problem
is
also
referred
to
as
the
problem
of
\incomplete
p
erception,"
\p
erceptual
aliasing,"
or
\hidden
state."
In
this
section,
w
e
will
consider
extensions
to
the
basic
MDP
framew
ork
for
solving
partially
observ
able
problems.
The
resulting
formal
mo
del
is
called
a
p
artial
ly
observable
Markov
de
cision
pr
o
c
ess
or
POMDP
.
.
State-F
ree
Deterministic
P
olicies
The
most
naiv
e
strategy
for
dealing
with
partial
observ
abilit
y
is
to
ignore
it.
That
is,
to
treat
the
observ
ations
as
if
they
w
ere
the
states
of
the
en
vironmen
t
and
try
to
learn
to
b
eha
v
e.
Figure
	
sho
ws
a
simple
en
vironmen
t
in
whic
h
the
agen
t
is
attempting
to
get
to
the
prin
ter
from
an
oce.
If
it
mo
v
es
from
the
oce,
there
is
a
go
o
d
c
hance
that
the
agen
t
will
end
up
in
one
of
t
w
o
places
that
lo
ok
lik
e
\hall",
but
that
require
dieren
t
actions
for
getting
to
the
prin
ter.
If
w
e
consider
these
states
to
b
e
the
same,
then
the
agen
t
cannot
p
ossibly
b
eha
v
e
optimally
.
But
ho
w
w
ell
can
it
do?
The
resulting
problem
is
not
Mark
o
vian,
and
Q-learning
cannot
b
e
guaran
teed
to
con-
v
erge.
Small
breac
hes
of
the
Mark
o
v
requiremen
t
are
w
ell
handled
b
y
Q-learning,
but
it
is
p
ossible
to
construct
simple
en
vironmen
ts
that
cause
Q-learning
to
oscillate
(Chrisman
&


----- Page 32 (native) -----
Kaelbling,
Littman,
&
Moore
Littman,
		).
It
is
p
ossible
to
use
a
mo
del-based
approac
h,
ho
w
ev
er;
act
according
to
some
p
olicy
and
gather
statistics
ab
out
the
transitions
b
et
w
een
observ
ations,
then
solv
e
for
the
optimal
p
olicy
based
on
those
observ
ations.
Unfortunately
,
when
the
en
vironmen
t
is
not
Mark
o
vian,
the
transition
probabilities
dep
end
on
the
p
olicy
b
eing
executed,
so
this
new
p
olicy
will
induce
a
new
set
of
transition
probabilities.
This
approac
h
ma
y
yield
plausible
results
in
some
cases,
but
again,
there
are
no
guaran
tees.
It
is
reasonable,
though,
to
ask
what
the
optimal
p
olicy
(mapping
from
observ
ations
to
actions,
in
this
case)
is.
It
is
NP-hard
(Littman,
		b)
to
nd
this
mapping,
and
ev
en
the
b
est
mapping
can
ha
v
e
v
ery
p
o
or
p
erformance.
In
the
case
of
our
agen
t
trying
to
get
to
the
prin
ter,
for
instance,
an
y
deterministic
state-free
p
olicy
tak
es
an
innite
n
um
b
er
of
steps
to
reac
h
the
goal
on
a
v
erage.
.
State-F
ree
Sto
c
hastic
P
olicies
Some
impro
v
emen
t
can
b
e
gained
b
y
considering
sto
c
hastic
p
olicies;
these
are
mappings
from
observ
ations
to
probabilit
y
distributions
o
v
er
actions.
If
there
is
randomness
in
the
agen
t's
actions,
it
will
not
get
stuc
k
in
the
hall
forev
er.
Jaakk
ola,
Singh,
and
Jordan
(		)
ha
v
e
dev
elop
ed
an
algorithm
for
nding
lo
cally-optimal
sto
c
hastic
p
olicies,
but
nding
a
globally
optimal
p
olicy
is
still
NP
hard.
In
our
example,
it
turns
out
that
the
optimal
sto
c
hastic
p
olicy
is
for
the
agen
t,
when
in
a
state
that
lo
oks
lik
e
a
hall,
to
go
east
with
probabilit
y

 p


0:
and
w
est
with
probabilit
y
p

 

0:.
This
p
olicy
can
b
e
found
b
y
solving
a
simple
(in
this
case)
quadratic
program.
The
fact
that
suc
h
a
simple
example
can
pro
duce
irrational
n
um
b
ers
giv
es
some
indication
that
it
is
a
dicult
problem
to
solv
e
exactly
.
.
P
olicies
with
In
ternal
State
The
only
w
a
y
to
b
eha
v
e
truly
eectiv
ely
in
a
wide-range
of
en
vironmen
ts
is
to
use
memory
of
previous
actions
and
observ
ations
to
disam
biguate
the
curren
t
state.
There
are
a
v
ariet
y
of
approac
hes
to
learning
p
olicies
with
in
ternal
state.
Recurren
t
Q-learning
One
in
tuitiv
ely
simple
approac
h
is
to
use
a
recurren
t
neural
net-
w
ork
to
learn
Q
v
alues.
The
net
w
ork
can
b
e
trained
using
bac
kpropagation
through
time
(or
some
other
suitable
tec
hnique)
and
learns
to
retain
\history
features"
to
predict
v
alue.
This
approac
h
has
b
een
used
b
y
a
n
um
b
er
of
researc
hers
(Meeden,
McGra
w,
&
Blank,
		;
Lin
&
Mitc
hell,
		;
Sc
hmidh
ub
er,
		b).
It
seems
to
w
ork
eectiv
ely
on
simple
problems,
but
can
suer
from
con
v
ergence
to
lo
cal
optima
on
more
complex
problems.
Classier
Systems
Classier
systems
(Holland,
	;
Goldb
erg,
		)
w
ere
explicitly
dev
elop
ed
to
solv
e
problems
with
dela
y
ed
rew
ard,
including
those
requiring
short-term
memory
.
The
in
ternal
mec
hanism
t
ypically
used
to
pass
rew
ard
bac
k
through
c
hains
of
decisions,
called
the
bucket
brigade
algorithm,
b
ears
a
close
resem
blance
to
Q-learning.
In
spite
of
some
early
successes,
the
original
design
do
es
not
app
ear
to
handle
partially
ob-
serv
ed
en
vironmen
ts
robustly
.
Recen
tly
,
this
approac
h
has
b
een
reexamined
using
insigh
ts
from
the
reinforcemen
t-
learning
literature,
with
some
success.
Dorigo
did
a
comparativ
e
study
of
Q-learning
and
classier
systems
(Dorigo
&
Bersini,
		).
Cli
and
Ross
(		)
start
with
Wilson's
zeroth-


----- Page 33 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
i
b
a
SE
Ï€
Figure
0:
Structure
of
a
POMDP
agen
t.
lev
el
classier
system
(Wilson,
		)
and
add
one
and
t
w
o-bit
memory
registers.
They
nd
that,
although
their
system
can
learn
to
use
short-term
memory
registers
eectiv
ely
,
the
approac
h
is
unlik
ely
to
scale
to
more
complex
en
vironmen
ts.
Dorigo
and
Colom
b
etti
applied
classier
systems
to
a
mo
derately
complex
problem
of
learning
rob
ot
b
eha
vior
from
immediate
reinforcemen
t
(Dorigo,
		;
Dorigo
&
Colom
b
etti,
		).
Finite-history-windo
w
Approac
h
One
w
a
y
to
restore
the
Mark
o
v
prop
ert
y
is
to
allo
w
decisions
to
b
e
based
on
the
history
of
recen
t
observ
ations
and
p
erhaps
actions.
Lin
and
Mitc
hell
(		)
used
a
xed-width
nite
history
windo
w
to
learn
a
p
ole
balancing
task.
McCallum
(		)
describ
es
the
\utile
sux
memory"
whic
h
learns
a
v
ariable-width
windo
w
that
serv
es
sim
ultaneously
as
a
mo
del
of
the
en
vironmen
t
and
a
nite-memory
p
olicy
.
This
system
has
had
excellen
t
results
in
a
v
ery
complex
driving-sim
ulation
domain
(McCallum,
		).
Ring
(		)
has
a
neural-net
w
ork
approac
h
that
uses
a
v
ariable
history
windo
w,
adding
history
when
necessary
to
disam
biguate
situations.
POMDP
Approac
h
Another
strategy
consists
of
using
hidden
Mark
o
v
mo
del
(HMM)
tec
hniques
to
learn
a
mo
del
of
the
en
vironmen
t,
including
the
hidden
state,
then
to
use
that
mo
del
to
construct
a
p
erfe
ct
memory
con
troller
(Cassandra,
Kaelbling,
&
Littman,
		;
Lo
v
ejo
y
,
		;
Monahan,
	).
Chrisman
(		)
sho
w
ed
ho
w
the
forw
ard-bac
kw
ard
algorithm
for
learning
HMMs
could
b
e
adapted
to
learning
POMDPs.
He,
and
later
McCallum
(		),
also
ga
v
e
heuristic
state-
splitting
rules
to
attempt
to
learn
the
smallest
p
ossible
mo
del
for
a
giv
en
en
vironmen
t.
The
resulting
mo
del
can
then
b
e
used
to
in
tegrate
information
from
the
agen
t's
observ
ations
in
order
to
mak
e
decisions.
Figure
0
illustrates
the
basic
structure
for
a
p
erfect-memory
con
troller.
The
comp
onen
t
on
the
left
is
the
state
estimator,
whic
h
computes
the
agen
t's
b
elief
state,
b
as
a
function
of
the
old
b
elief
state,
the
last
action
a,
and
the
curren
t
observ
ation
i.
In
this
con
text,
a
b
elief
state
is
a
probabilit
y
distribution
o
v
er
states
of
the
en
vironmen
t,
indicating
the
lik
eliho
o
d,
giv
en
the
agen
t's
past
exp
erience,
that
the
en
vironmen
t
is
actually
in
eac
h
of
those
states.
The
state
estimator
can
b
e
constructed
straigh
tforw
ardly
using
the
estimated
w
orld
mo
del
and
Ba
y
es'
rule.
No
w
w
e
are
left
with
the
problem
of
nding
a
p
olicy
mapping
b
elief
states
in
to
action.
This
problem
can
b
e
form
ulated
as
an
MDP,
but
it
is
dicult
to
solv
e
using
the
tec
hniques
describ
ed
earlier,
b
ecause
the
input
space
is
con
tin
uous.
Chrisman's
approac
h
(		)
do
es
not
tak
e
in
to
accoun
t
future
uncertain
t
y
,
but
yields
a
p
olicy
after
a
small
amoun
t
of
com-
putation.
A
standard
approac
h
from
the
op
erations-researc
h
literature
is
to
solv
e
for
the


----- Page 34 (native) -----
Kaelbling,
Littman,
&
Moore
optimal
p
olicy
(or
a
close
appro
ximation
thereof
)
based
on
its
represen
tation
as
a
piecewise-
linear
and
con
v
ex
function
o
v
er
the
b
elief
space.
This
metho
d
is
computationally
in
tractable,
but
ma
y
serv
e
as
inspiration
for
metho
ds
that
mak
e
further
appro
ximations
(Cassandra
et
al.,
		;
Littman,
Cassandra,
&
Kaelbling,
		a).
.
Reinforcemen
t
Learning
Applications
One
reason
that
reinforcemen
t
learning
is
p
opular
is
that
is
serv
es
as
a
theoretical
to
ol
for
studying
the
principles
of
agen
ts
learning
to
act.
But
it
is
unsurprising
that
it
has
also
b
een
used
b
y
a
n
um
b
er
of
researc
hers
as
a
practical
computational
to
ol
for
constructing
autonomous
systems
that
impro
v
e
themselv
es
with
exp
erience.
These
applications
ha
v
e
ranged
from
rob
otics,
to
industrial
man
ufacturing,
to
com
binatorial
searc
h
problems
suc
h
as
computer
game
pla
ying.
Practical
applications
pro
vide
a
test
of
the
ecacy
and
usefulness
of
learning
algorithms.
They
are
also
an
inspiration
for
deciding
whic
h
comp
onen
ts
of
the
reinforcemen
t
learning
framew
ork
are
of
practical
imp
ortance.
F
or
example,
a
researc
her
with
a
real
rob
otic
task
can
pro
vide
a
data
p
oin
t
to
questions
suc
h
as:

Ho
w
imp
ortan
t
is
optimal
exploration?
Can
w
e
break
the
learning
p
erio
d
in
to
explo-
ration
phases
and
exploitation
phases?

What
is
the
most
useful
mo
del
of
long-term
rew
ard:
Finite
horizon?
Discoun
ted?
Innite
horizon?

Ho
w
m
uc
h
computation
is
a
v
ailable
b
et
w
een
agen
t
decisions
and
ho
w
should
it
b
e
used?

What
prior
kno
wledge
can
w
e
build
in
to
the
system,
and
whic
h
algorithms
are
capable
of
using
that
kno
wledge?
Let
us
examine
a
set
of
practical
applications
of
reinforcemen
t
learning,
while
b
earing
these
questions
in
mind.
.
Game
Pla
ying
Game
pla
ying
has
dominated
the
Articial
In
telligence
w
orld
as
a
problem
domain
ev
er
since
the
eld
w
as
b
orn.
Tw
o-pla
y
er
games
do
not
t
in
to
the
established
reinforcemen
t-learning
framew
ork
since
the
optimalit
y
criterion
for
games
is
not
one
of
maximizing
rew
ard
in
the
face
of
a
xed
en
vironmen
t,
but
one
of
maximizing
rew
ard
against
an
optimal
adv
ersary
(minimax).
Nonetheless,
reinforcemen
t-learning
algorithms
can
b
e
adapted
to
w
ork
for
a
v
ery
general
class
of
games
(Littman,
		a)
and
man
y
researc
hers
ha
v
e
used
reinforcemen
t
learning
in
these
en
vironmen
ts.
One
application,
sp
ectacularly
far
ahead
of
its
time,
w
as
Sam
uel's
c
hec
k
ers
pla
ying
system
(Sam
uel,
		).
This
learned
a
v
alue
function
represen
ted
b
y
a
linear
function
appro
ximator,
and
emplo
y
ed
a
training
sc
heme
similar
to
the
up
dates
used
in
v
alue
iteration,
temp
oral
dierences
and
Q-learning.
More
recen
tly
,
T
esauro
(		,
		,
		)
applied
the
temp
oral
dierence
algorithm
to
bac
kgammon.
Bac
kgammon
has
appro
ximately
0
0
states,
making
table-based
rein-
forcemen
t
learning
imp
ossible.
Instead,
T
esauro
used
a
bac
kpropagation-based
three-la
y
er
0

----- Page 35 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
T
raining
Games
Hidden
Units
Results
Basic
P
o
or
TD
.0
00,000
0
Lost
b
y

p
oin
ts
in

games
TD
.0
00,000
0
Lost
b
y

p
oin
ts
in

games
TD
.
,00,000
0
Lost
b
y

p
oin
t
in
0
games
T
able
:
TD-Gammon's
p
erformance
in
games
against
the
top
h
uman
professional
pla
y
ers.
A
bac
kgammon
tournamen
t
in
v
olv
es
pla
ying
a
series
of
games
for
p
oin
ts
un
til
one
pla
y
er
reac
hes
a
set
target.
TD-Gammon
w
on
none
of
these
tournamen
ts
but
came
sucien
tly
close
that
it
is
no
w
considered
one
of
the
b
est
few
pla
y
ers
in
the
w
orld.
neural
net
w
ork
as
a
function
appro
ximator
for
the
v
alue
function
Bo
ar
d
Position
!
Pr
ob
ability
of
victory
for
curr
ent
player:
Tw
o
v
ersions
of
the
learning
algorithm
w
ere
used.
The
rst,
whic
h
w
e
will
call
Basic
TD-
Gammon,
used
v
ery
little
predened
kno
wledge
of
the
game,
and
the
represen
tation
of
a
b
oard
p
osition
w
as
virtually
a
ra
w
enco
ding,
sucien
tly
p
o
w
erful
only
to
p
ermit
the
neural
net
w
ork
to
distinguish
b
et
w
een
conceptually
dieren
t
p
ositions.
The
second,
TD-Gammon,
w
as
pro
vided
with
the
same
ra
w
state
information
supplemen
ted
b
y
a
n
um
b
er
of
hand-
crafted
features
of
bac
kgammon
b
oard
p
ositions.
Pro
viding
hand-crafted
features
in
this
manner
is
a
go
o
d
example
of
ho
w
inductiv
e
biases
from
h
uman
kno
wledge
of
the
task
can
b
e
supplied
to
a
learning
algorithm.
The
training
of
b
oth
learning
algorithms
required
sev
eral
mon
ths
of
computer
time,
and
w
as
ac
hiev
ed
b
y
constan
t
self-pla
y
.
No
exploration
strategy
w
as
used|the
system
alw
a
ys
greedily
c
hose
the
mo
v
e
with
the
largest
exp
ected
probabilit
y
of
victory
.
This
naiv
e
explo-
ration
strategy
pro
v
ed
en
tirely
adequate
for
this
en
vironmen
t,
whic
h
is
p
erhaps
surprising
giv
en
the
considerable
w
ork
in
the
reinforcemen
t-learning
literature
whic
h
has
pro
duced
n
umerous
coun
ter-examples
to
sho
w
that
greedy
exploration
can
lead
to
p
o
or
learning
p
er-
formance.
Bac
kgammon,
ho
w
ev
er,
has
t
w
o
imp
ortan
t
prop
erties.
Firstly
,
whatev
er
p
olicy
is
follo
w
ed,
ev
ery
game
is
guaran
teed
to
end
in
nite
time,
meaning
that
useful
rew
ard
information
is
obtained
fairly
frequen
tly
.
Secondly
,
the
state
transitions
are
sucien
tly
sto
c
hastic
that
indep
enden
t
of
the
p
olicy
,
all
states
will
o
ccasionally
b
e
visited|a
wrong
initial
v
alue
function
has
little
danger
of
starving
us
from
visiting
a
critical
part
of
state
space
from
whic
h
imp
ortan
t
information
could
b
e
obtained.
The
results
(T
able
)
of
TD-Gammon
are
impressiv
e.
It
has
comp
eted
at
the
v
ery
top
lev
el
of
in
ternational
h
uman
pla
y
.
Basic
TD-Gammon
pla
y
ed
resp
ectably
,
but
not
at
a
professional
standard.


----- Page 36 (native) -----
Figure
:
Sc
haal
and
A
tk
eson's
devil-stic
king
rob
ot.
The
tap
ered
stic
k
is
hit
alternately
b
y
eac
h
of
the
t
w
o
hand
stic
ks.
The
task
is
to
k
eep
the
devil
stic
k
from
falling
for
as
man
y
hits
as
p
ossible.
The
rob
ot
has
three
motors
indicated
b
y
torque
v
ectors


;


;


.
Although
exp
erimen
ts
with
other
games
ha
v
e
in
some
cases
pro
duced
in
teresting
learning
b
eha
vior,
no
success
close
to
that
of
TD-Gammon
has
b
een
rep
eated.
Other
games
that
ha
v
e
b
een
studied
include
Go
(Sc
hraudolph,
Da
y
an,
&
Sejno
wski,
		)
and
Chess
(Thrun,
		).
It
is
still
an
op
en
question
as
to
if
and
ho
w
the
success
of
TD-Gammon
can
b
e
rep
eated
in
other
domains.
.
Rob
otics
and
Con
trol
In
recen
t
y
ears
there
ha
v
e
b
een
man
y
rob
otics
and
con
trol
applications
that
ha
v
e
used
reinforcemen
t
learning.
Here
w
e
will
concen
trate
on
the
follo
wing
four
examples,
although
man
y
other
in
teresting
ongoing
rob
otics
in
v
estigations
are
underw
a
y
.
.
Sc
haal
and
A
tk
eson
(		)
constructed
a
t
w
o-armed
rob
ot,
sho
wn
in
Figure
,
that
learns
to
juggle
a
device
kno
wn
as
a
devil-stic
k.
This
is
a
complex
non-linear
con
trol
task
in
v
olving
a
six-dimensional
state
space
and
less
than
00
msecs
p
er
con
trol
deci-
sion.
After
ab
out
0
initial
attempts
the
rob
ot
learns
to
k
eep
juggling
for
h
undreds
of
hits.
A
t
ypical
h
uman
learning
the
task
requires
an
order
of
magnitude
more
practice
to
ac
hiev
e
prociency
at
mere
tens
of
hits.
The
juggling
rob
ot
learned
a
w
orld
mo
del
from
exp
erience,
whic
h
w
as
generalized
to
un
visited
states
b
y
a
function
appro
ximation
sc
heme
kno
wn
as
lo
cally
w
eigh
ted
regression
(Clev
eland
&
Delvin,
	;
Mo
ore
&
A
tk
eson,
		).
Bet
w
een
eac
h
trial,
a
form
of
dynamic
programming
sp
ecic
to
linear
con
trol
p
olicies
and
lo
cally
linear
transitions
w
as
used
to
impro
v
e
the
p
olicy
.
The
form
of
dynamic
programming
is
kno
wn
as
linear-quadratic-regulator
design
(Sage
&
White,
	).


----- Page 37 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
.
Mahadev
an
and
Connell
(		a)
discuss
a
task
in
whic
h
a
mobile
rob
ot
pushes
large
b
o
xes
for
extended
p
erio
ds
of
time.
Bo
x-pushing
is
a
w
ell-kno
wn
dicult
rob
otics
problem,
c
haracterized
b
y
immense
uncertain
t
y
in
the
results
of
actions.
Q-learning
w
as
used
in
conjunction
with
some
no
v
el
clustering
tec
hniques
designed
to
enable
a
higher-dimensional
input
than
a
tabular
approac
h
w
ould
ha
v
e
p
ermitted.
The
rob
ot
learned
to
p
erform
comp
etitiv
ely
with
the
p
erformance
of
a
h
uman-programmed
so-
lution.
Another
asp
ect
of
this
w
ork,
men
tioned
in
Section
.,
w
as
a
pre-programmed
breakdo
wn
of
the
monolithic
task
description
in
to
a
set
of
lo
w
er
lev
el
tasks
to
b
e
learned.
.
Mataric
(		)
describ
es
a
rob
otics
exp
erimen
t
with,
from
the
viewp
oin
t
of
theoret-
ical
reinforcemen
t
learning,
an
un
think
ably
high
dimensional
state
space,
con
taining
man
y
dozens
of
degrees
of
freedom.
F
our
mobile
rob
ots
tra
v
eled
within
an
enclo-
sure
collecting
small
disks
and
transp
orting
them
to
a
destination
region.
There
w
ere
three
enhancemen
ts
to
the
basic
Q-learning
algorithm.
Firstly
,
pre-programmed
sig-
nals
called
pr
o
gr
ess
estimators
w
ere
used
to
break
the
monolithic
task
in
to
subtasks.
This
w
as
ac
hiev
ed
in
a
robust
manner
in
whic
h
the
rob
ots
w
ere
not
forced
to
use
the
estimators,
but
had
the
freedom
to
prot
from
the
inductiv
e
bias
they
pro
vided.
Secondly
,
con
trol
w
as
decen
tralized.
Eac
h
rob
ot
learned
its
o
wn
p
olicy
indep
enden
tly
without
explicit
comm
unication
with
the
others.
Thirdly
,
state
space
w
as
brutally
quan
tized
in
to
a
small
n
um
b
er
of
discrete
states
according
to
v
alues
of
a
small
n
um-
b
er
of
pre-programmed
b
o
olean
features
of
the
underlying
sensors.
The
p
erformance
of
the
Q-learned
p
olicies
w
ere
almost
as
go
o
d
as
a
simple
hand-crafted
con
troller
for
the
job.
.
Q-learning
has
b
een
used
in
an
elev
ator
dispatc
hing
task
(Crites
&
Barto,
		).
The
problem,
whic
h
has
b
een
implemen
ted
in
sim
ulation
only
at
this
stage,
in
v
olv
ed
four
elev
ators
servicing
ten
o
ors.
The
ob
jectiv
e
w
as
to
minimize
the
a
v
erage
squared
w
ait
time
for
passengers,
discoun
ted
in
to
future
time.
The
problem
can
b
e
p
osed
as
a
discrete
Mark
o
v
system,
but
there
are
0

states
ev
en
in
the
most
simplied
v
ersion
of
the
problem.
Crites
and
Barto
used
neural
net
w
orks
for
function
appro
ximation
and
pro
vided
an
excellen
t
comparison
study
of
their
Q-learning
approac
h
against
the
most
p
opular
and
the
most
sophisticated
elev
ator
dispatc
hing
algorithms.
The
squared
w
ait
time
of
their
con
troller
w
as
appro
ximately
%
less
than
the
b
est
alternativ
e
algorithm
(\Empt
y
the
System"
heuristic
with
a
receding
horizon
con
troller)
and
less
than
half
the
squared
w
ait
time
of
the
con
troller
most
frequen
tly
used
in
real
elev
ator
systems.
.
The
nal
example
concerns
an
application
of
reinforcemen
t
learning
b
y
one
of
the
authors
of
this
surv
ey
to
a
pac
k
aging
task
from
a
fo
o
d
pro
cessing
industry
.
The
problem
in
v
olv
es
lling
con
tainers
with
v
ariable
n
um
b
ers
of
non-iden
tical
pro
ducts.
The
pro
duct
c
haracteristics
also
v
ary
with
time,
but
can
b
e
sensed.
Dep
ending
on
the
task,
v
arious
constrain
ts
are
placed
on
the
con
tainer-lling
pro
cedure.
Here
are
three
examples:

The
mean
w
eigh
t
of
all
con
tainers
pro
duced
b
y
a
shift
m
ust
not
b
e
b
elo
w
the
man
ufacturer's
declared
w
eigh
t
W
.


----- Page 38 (native) -----
Kaelbling,
Littman,
&
Moore

The
n
um
b
er
of
con
tainers
b
elo
w
the
declared
w
eigh
t
m
ust
b
e
less
than
P
%.

No
con
tainers
ma
y
b
e
pro
duced
b
elo
w
w
eigh
t
W
0
.
Suc
h
tasks
are
con
trolled
b
y
mac
hinery
whic
h
op
erates
according
to
v
arious
setp
oints.
Con
v
en
tional
practice
is
that
setp
oin
ts
are
c
hosen
b
y
h
uman
op
erators,
but
this
c
hoice
is
not
easy
as
it
is
dep
enden
t
on
the
curren
t
pro
duct
c
haracteristics
and
the
curren
t
task
constrain
ts.
The
dep
endency
is
often
dicult
to
mo
del
and
highly
non-linear.
The
task
w
as
p
osed
as
a
nite-horizon
Mark
o
v
decision
task
in
whic
h
the
state
of
the
system
is
a
function
of
the
pro
duct
c
haracteristics,
the
amoun
t
of
time
remaining
in
the
pro
duction
shift
and
the
mean
w
astage
and
p
ercen
t
b
elo
w
declared
in
the
shift
so
far.
The
system
w
as
discretized
in
to
00,000
discrete
states
and
lo
cal
w
eigh
ted
regression
w
as
used
to
learn
and
generalize
a
transition
mo
del.
Prioritized
sw
eep-
ing
w
as
used
to
main
tain
an
optimal
v
alue
function
as
eac
h
new
piece
of
transition
information
w
as
obtained.
In
sim
ulated
exp
erimen
ts
the
sa
vings
w
ere
considerable,
t
ypically
with
w
astage
reduced
b
y
a
factor
of
ten.
Since
then
the
system
has
b
een
deplo
y
ed
successfully
in
sev
eral
factories
within
the
United
States.
Some
in
teresting
asp
ects
of
practical
reinforcemen
t
learning
come
to
ligh
t
from
these
examples.
The
most
striking
is
that
in
all
cases,
to
mak
e
a
real
system
w
ork
it
pro
v
ed
necessary
to
supplemen
t
the
fundamen
tal
algorithm
with
extra
pre-programmed
kno
wledge.
Supplying
extra
kno
wledge
comes
at
a
price:
more
h
uman
eort
and
insigh
t
is
required
and
the
system
is
subsequen
tly
less
autonomous.
But
it
is
also
clear
that
for
tasks
suc
h
as
these,
a
kno
wledge-free
approac
h
w
ould
not
ha
v
e
ac
hiev
ed
w
orth
while
p
erformance
within
the
nite
lifetime
of
the
rob
ots.
What
forms
did
this
pre-programmed
kno
wledge
tak
e?
It
included
an
assumption
of
linearit
y
for
the
juggling
rob
ot's
p
olicy
,
a
man
ual
breaking
up
of
the
task
in
to
subtasks
for
the
t
w
o
mobile-rob
ot
examples,
while
the
b
o
x-pusher
also
used
a
clustering
tec
hnique
for
the
Q
v
alues
whic
h
assumed
lo
cally
consisten
t
Q
v
alues.
The
four
disk-collecting
rob
ots
additionally
used
a
man
ually
discretized
state
space.
The
pac
k
aging
example
had
far
few
er
dimensions
and
so
required
corresp
ondingly
w
eak
er
assumptions,
but
there,
to
o,
the
as-
sumption
of
lo
cal
piecewise
con
tin
uit
y
in
the
transition
mo
del
enabled
massiv
e
reductions
in
the
amoun
t
of
learning
data
required.
The
exploration
strategies
are
in
teresting
to
o.
The
juggler
used
careful
statistical
anal-
ysis
to
judge
where
to
protably
exp
erimen
t.
Ho
w
ev
er,
b
oth
mobile
rob
ot
applications
w
ere
able
to
learn
w
ell
with
greedy
exploration|alw
a
ys
exploiting
without
delib
erate
ex-
ploration.
The
pac
k
aging
task
used
optimism
in
the
face
of
uncertain
t
y
.
None
of
these
strategies
mirrors
theoretically
optimal
(but
computationally
in
tractable)
exploration,
and
y
et
all
pro
v
ed
adequate.
Finally
,
it
is
also
w
orth
considering
the
computational
regimes
of
these
exp
erimen
ts.
They
w
ere
all
v
ery
dieren
t,
whic
h
indicates
that
the
diering
computational
demands
of
v
arious
reinforcemen
t
learning
algorithms
do
indeed
ha
v
e
an
arra
y
of
diering
applications.
The
juggler
needed
to
mak
e
v
ery
fast
decisions
with
lo
w
latency
b
et
w
een
eac
h
hit,
but
had
long
p
erio
ds
(0
seconds
and
more)
b
et
w
een
eac
h
trial
to
consolidate
the
exp
eriences
collected
on
the
previous
trial
and
to
p
erform
the
more
aggressiv
e
computation
necessary
to
pro
duce
a
new
reactiv
e
con
troller
on
the
next
trial.
The
b
o
x-pushing
rob
ot
w
as
mean
t
to


----- Page 39 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
op
erate
autonomously
for
hours
and
so
had
to
mak
e
decisions
with
a
uniform
length
con
trol
cycle.
The
cycle
w
as
sucien
tly
long
for
quite
substan
tial
computations
b
ey
ond
simple
Q-
learning
bac
kups.
The
four
disk-collecting
rob
ots
w
ere
particularly
in
teresting.
Eac
h
rob
ot
had
a
short
life
of
less
than
0
min
utes
(due
to
battery
constrain
ts)
meaning
that
substan
tial
n
um
b
er
crunc
hing
w
as
impractical,
and
an
y
signican
t
com
binatorial
searc
h
w
ould
ha
v
e
used
a
signican
t
fraction
of
the
rob
ot's
learning
lifetime.
The
pac
k
aging
task
had
easy
constrain
ts.
One
decision
w
as
needed
ev
ery
few
min
utes.
This
pro
vided
opp
ortunities
for
fully
computing
the
optimal
v
alue
function
for
the
00,000-state
system
b
et
w
een
ev
ery
con
trol
cycle,
in
addition
to
p
erforming
massiv
e
cross-v
alidation-based
optimization
of
the
transition
mo
del
b
eing
learned.
A
great
deal
of
further
w
ork
is
curren
tly
in
progress
on
practical
implemen
tations
of
reinforcemen
t
learning.
The
insigh
ts
and
task
constrain
ts
that
they
pro
duce
will
ha
v
e
an
imp
ortan
t
eect
on
shaping
the
kind
of
algorithms
that
are
dev
elop
ed
in
future.
	.
Conclusions
There
are
a
v
ariet
y
of
reinforcemen
t-learning
tec
hniques
that
w
ork
eectiv
ely
on
a
v
ariet
y
of
small
problems.
But
v
ery
few
of
these
tec
hniques
scale
w
ell
to
larger
problems.
This
is
not
b
ecause
researc
hers
ha
v
e
done
a
bad
job
of
in
v
en
ting
learning
tec
hniques,
but
b
ecause
it
is
v
ery
dicult
to
solv
e
arbitrary
problems
in
the
general
case.
In
order
to
solv
e
highly
complex
problems,
w
e
m
ust
giv
e
up
tabula
r
asa
learning
tec
hniques
and
b
egin
to
incorp
orate
bias
that
will
giv
e
lev
erage
to
the
learning
pro
cess.
The
necessary
bias
can
come
in
a
v
ariet
y
of
forms,
including
the
follo
wing:
shaping:
The
tec
hnique
of
shaping
is
used
in
training
animals
(Hilgard
&
Bo
w
er,
	);
a
teac
her
presen
ts
v
ery
simple
problems
to
solv
e
rst,
then
gradually
exp
oses
the
learner
to
more
complex
problems.
Shaping
has
b
een
used
in
sup
ervised-learning
systems,
and
can
b
e
used
to
train
hierarc
hical
reinforcemen
t-learning
systems
from
the
b
ottom
up
(Lin,
		),
and
to
alleviate
problems
of
dela
y
ed
reinforcemen
t
b
y
decreasing
the
dela
y
un
til
the
problem
is
w
ell
understo
o
d
(Dorigo
&
Colom
b
etti,
		;
Dorigo,
		).
lo
cal
reinforcemen
t
signals:
Whenev
er
p
ossible,
agen
ts
should
b
e
giv
en
reinforcemen
t
signals
that
are
lo
cal.
In
applications
in
whic
h
it
is
p
ossible
to
compute
a
gradien
t,
rew
arding
the
agen
t
for
taking
steps
up
the
gradien
t,
rather
than
just
for
ac
hieving
the
nal
goal,
can
sp
eed
learning
signican
tly
(Mataric,
		).
imitation:
An
agen
t
can
learn
b
y
\w
atc
hing"
another
agen
t
p
erform
the
task
(Lin,
		).
F
or
real
rob
ots,
this
requires
p
erceptual
abilities
that
are
not
y
et
a
v
ailable.
But
another
strategy
is
to
ha
v
e
a
h
uman
supply
appropriate
motor
commands
to
a
rob
ot
through
a
jo
ystic
k
or
steering
wheel
(P
omerleau,
		).
problem
decomp
osition:
Decomp
osing
a
h
uge
learning
problem
in
to
a
collection
of
smaller
ones,
and
pro
viding
useful
reinforcemen
t
signals
for
the
subproblems
is
a
v
ery
p
o
w
er-
ful
tec
hnique
for
biasing
learning.
Most
in
teresting
examples
of
rob
otic
reinforcemen
t
learning
emplo
y
this
tec
hnique
to
some
exten
t
(Connell
&
Mahadev
an,
		).
reexes:
One
thing
that
k
eeps
agen
ts
that
kno
w
nothing
from
learning
an
ything
is
that
they
ha
v
e
a
hard
time
ev
en
nding
the
in
teresting
parts
of
the
space;
they
w
ander


----- Page 40 (native) -----
Kaelbling,
Littman,
&
Moore
around
at
random
nev
er
getting
near
the
goal,
or
they
are
alw
a
ys
\killed"
immediately
.
These
problems
can
b
e
ameliorated
b
y
programming
a
set
of
\reexes"
that
cause
the
agen
t
to
act
initially
in
some
w
a
y
that
is
reasonable
(Mataric,
		;
Singh,
Barto,
Grup
en,
&
Connolly
,
		).
These
reexes
can
ev
en
tually
b
e
o
v
erridden
b
y
more
detailed
and
accurate
learned
kno
wledge,
but
they
at
least
k
eep
the
agen
t
aliv
e
and
p
oin
ted
in
the
righ
t
direction
while
it
is
trying
to
learn.
Recen
t
w
ork
b
y
Millan
(		)
explores
the
use
of
reexes
to
mak
e
rob
ot
learning
safer
and
more
ecien
t.
With
appropriate
biases,
supplied
b
y
h
uman
programmers
or
teac
hers,
complex
reinforcemen
t-
learning
problems
will
ev
en
tually
b
e
solv
able.
There
is
still
m
uc
h
w
ork
to
b
e
done
and
man
y
in
teresting
questions
remaining
for
learning
tec
hniques
and
esp
ecially
regarding
metho
ds
for
appro
ximating,
decomp
osing,
and
incorp
orating
bias
in
to
problems.
Ac
kno
wledgemen
ts
Thanks
to
Marco
Dorigo
and
three
anon
ymous
review
ers
for
commen
ts
that
ha
v
e
help
ed
to
impro
v
e
this
pap
er.
Also
thanks
to
our
man
y
colleagues
in
the
reinforcemen
t-learning
comm
unit
y
who
ha
v
e
done
this
w
ork
and
explained
it
to
us.
Leslie
P
ac
k
Kaelbling
w
as
supp
orted
in
part
b
y
NSF
gran
ts
IRI-	
and
IRI-
		.
Mic
hael
Littman
w
as
supp
orted
in
part
b
y
Bellcore.
Andrew
Mo
ore
w
as
supp
orted
in
part
b
y
an
NSF
Researc
h
Initiation
Aw
ard
and
b
y
M
Corp
oration.
References
Ac
kley
,
D.
H.,
&
Littman,
M.
L.
(		0).
Generalization
and
scaling
in
reinforcemen
t
learn-
ing.
In
T
ouretzky
,
D.
S.
(Ed.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems
,
pp.
0{
San
Mateo,
CA.
Morgan
Kaufmann.
Albus,
J.
S.
(	).
A
new
approac
h
to
manipulator
con
trol:
Cereb
ellar
mo
del
articulation
con
troller
(cmac).
Journal
of
Dynamic
Systems,
Me
asur
ement
and
Contr
ol,
	,
0{
.
Albus,
J.
S.
(	).
Br
ains,
Behavior,
and
R
ob
otics.
BYTE
Bo
oks,
Subsidiary
of
McGra
w-
Hill,
P
eterb
orough,
New
Hampshire.
Anderson,
C.
W.
(	).
L
e
arning
and
Pr
oblem
Solving
with
Multilayer
Conne
ctionist
Systems.
Ph.D.
thesis,
Univ
ersit
y
of
Massac
h
usetts,
Amherst,
MA.
Ashar,
R.
R.
(		).
Hierarc
hical
learning
in
sto
c
hastic
domains.
Master's
thesis,
Bro
wn
Univ
ersit
y
,
Pro
vidence,
Rho
de
Island.
Baird,
L.
(		).
Residual
algorithms:
Reinforcemen
t
learning
with
function
appro
xima-
tion.
In
Prieditis,
A.,
&
Russell,
S.
(Eds.),
Pr
o
c
e
e
dings
of
the
Twelfth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
0{
San
F
rancisco,
CA.
Morgan
Kaufmann.
Baird,
L.
C.,
&
Klopf,
A.
H.
(		).
Reinforcemen
t
learning
with
high-dimensional,
con-
tin
uous
actions.
T
ec
h.
rep.
WL-TR-	-,
W
righ
t-P
atterson
Air
F
orce
Base
Ohio:
Wrigh
t
Lab
oratory
.


----- Page 41 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
Barto,
A.
G.,
Bradtk
e,
S.
J.,
&
Singh,
S.
P
.
(		).
Learning
to
act
using
real-time
dynamic
programming.
A
rticial
Intel
ligenc
e,

(),
{.
Barto,
A.
G.,
Sutton,
R.
S.,
&
Anderson,
C.
W.
(	).
Neuronlik
e
adaptiv
e
elemen
ts
that
can
solv
e
dicult
learning
con
trol
problems.
IEEE
T
r
ansactions
on
Systems,
Man,
and
Cyb
ernetics,
SMC-
(),
{.
Bellman,
R.
(	).
Dynamic
Pr
o
gr
amming.
Princeton
Univ
ersit
y
Press,
Princeton,
NJ.
Berenji,
H.
R.
(		).
Articial
neural
net
w
orks
and
appro
ximate
reasoning
for
in
telligen
t
con
trol
in
space.
In
A
meric
an
Contr
ol
Confer
enc
e,
pp.
0{00.
Berry
,
D.
A.,
&
F
ristedt,
B.
(	).
Bandit
Pr
oblems:
Se
quential
A
l
lo
c
ation
of
Exp
eriments.
Chapman
and
Hall,
London,
UK.
Bertsek
as,
D.
P
.
(	).
Dynamic
Pr
o
gr
amming:
Deterministic
and
Sto
chastic
Mo
dels.
Pren
tice-Hall,
Englew
o
o
d
Clis,
NJ.
Bertsek
as,
D.
P
.
(		).
Dynamic
Pr
o
gr
amming
and
Optimal
Contr
ol.
A
thena
Scien
tic,
Belmon
t,
Massac
h
usetts.
V
olumes

and
.
Bertsek
as,
D.
P
.,
&
Casta
~
non,
D.
A.
(		).
Adaptiv
e
aggregation
for
innite
horizon
dynamic
programming.
IEEE
T
r
ansactions
on
A
utomatic
Contr
ol,

(),
	{	.
Bertsek
as,
D.
P
.,
&
Tsitsiklis,
J.
N.
(		).
Par
al
lel
and
Distribute
d
Computation:
Numer-
ic
al
Metho
ds.
Pren
tice-Hall,
Englew
o
o
d
Clis,
NJ.
Bo
x,
G.
E.
P
.,
&
Drap
er,
N.
R.
(	).
Empiric
al
Mo
del-Building
and
R
esp
onse
Surfac
es.
Wiley
.
Bo
y
an,
J.
A.,
&
Mo
ore,
A.
W.
(		).
Generalization
in
reinforcemen
t
learning:
Safely
appro
ximating
the
v
alue
function.
In
T
esauro,
G.,
T
ouretzky
,
D.
S.,
&
Leen,
T.
K.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems

Cam
bridge,
MA.
The
MIT
Press.
Burghes,
D.,
&
Graham,
A.
(	0).
Intr
o
duction
to
Contr
ol
The
ory
including
Optimal
Contr
ol.
Ellis
Horw
o
o
d.
Cassandra,
A.
R.,
Kaelbling,
L.
P
.,
&
Littman,
M.
L.
(		).
Acting
optimally
in
partially
observ
able
sto
c
hastic
domains.
In
Pr
o
c
e
e
dings
of
the
Twelfth
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e
Seattle,
W
A.
Chapman,
D.,
&
Kaelbling,
L.
P
.
(		).
Input
generalization
in
dela
y
ed
reinforcemen
t
learning:
An
algorithm
and
p
erformance
comparisons.
In
Pr
o
c
e
e
dings
of
the
Interna-
tional
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e
Sydney
,
Australia.
Chrisman,
L.
(		).
Reinforcemen
t
learning
with
p
erceptual
aliasing:
The
p
erceptual
distinctions
approac
h.
In
Pr
o
c
e
e
dings
of
the
T
enth
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
{
San
Jose,
CA.
AAAI
Press.


----- Page 42 (native) -----
Kaelbling,
Littman,
&
Moore
Chrisman,
L.,
&
Littman,
M.
(		).
Hidden
state
and
short-term
memory
..
Presen
tation
at
Reinforcemen
t
Learning
W
orkshop,
Mac
hine
Learning
Conference.
Cic
hosz,
P
.,
&
Mula
wk
a,
J.
J.
(		).
F
ast
and
ecien
t
reinforcemen
t
learning
with
trun-
cated
temp
oral
dierences.
In
Prieditis,
A.,
&
Russell,
S.
(Eds.),
Pr
o
c
e
e
dings
of
the
Twelfth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
		{0
San
F
rancisco,
CA.
Morgan
Kaufmann.
Clev
eland,
W.
S.,
&
Delvin,
S.
J.
(	).
Lo
cally
w
eigh
ted
regression:
An
approac
h
to
regression
analysis
b
y
lo
cal
tting.
Journal
of
the
A
meric
an
Statistic
al
Asso
ciation,

(0),
	{0.
Cli,
D.,
&
Ross,
S.
(		).
Adding
temp
orary
memory
to
ZCS.
A
daptive
Behavior,

(),
0{0.
Condon,
A.
(		).
The
complexit
y
of
sto
c
hastic
games.
Information
and
Computation,
	
(),
0{.
Connell,
J.,
&
Mahadev
an,
S.
(		).
Rapid
task
learning
for
real
rob
ots.
In
R
ob
ot
L
e
arning.
Klu
w
er
Academic
Publishers.
Crites,
R.
H.,
&
Barto,
A.
G.
(		).
Impro
ving
elev
ator
p
erformance
using
reinforcemen
t
learning.
In
T
ouretzky
,
D.,
Mozer,
M.,
&
Hasselmo,
M.
(Eds.),
Neur
al
Information
Pr
o
c
essing
Systems
.
Da
y
an,
P
.
(		).
The
con
v
ergence
of
TD()
for
general
.
Machine
L
e
arning,

(),
{
.
Da
y
an,
P
.,
&
Hin
ton,
G.
E.
(		).
F
eudal
reinforcemen
t
learning.
In
Hanson,
S.
J.,
Co
w
an,
J.
D.,
&
Giles,
C.
L.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems

San
Mateo,
CA.
Morgan
Kaufmann.
Da
y
an,
P
.,
&
Sejno
wski,
T.
J.
(		).
TD()
con
v
erges
with
probabilit
y
.
Machine
L
e
arn-
ing,

().
Dean,
T.,
Kaelbling,
L.
P
.,
Kirman,
J.,
&
Nic
holson,
A.
(		).
Planning
with
deadlines
in
sto
c
hastic
domains.
In
Pr
o
c
e
e
dings
of
the
Eleventh
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e
W
ashington,
DC.
D'Ep
enoux,
F.
(	).
A
probabilistic
pro
duction
and
in
v
en
tory
problem.
Management
Scienc
e,
0,
	{0.
Derman,
C.
(	0).
Finite
State
Markovian
De
cision
Pr
o
c
esses.
Academic
Press,
New
Y
ork.
Dorigo,
M.,
&
Bersini,
H.
(		).
A
comparison
of
q-learning
and
classier
systems.
In
F
r
om
A
nimals
to
A
nimats:
Pr
o
c
e
e
dings
of
the
Thir
d
International
Confer
enc
e
on
the
Simulation
of
A
daptive
Behavior
Brigh
ton,
UK.
Dorigo,
M.,
&
Colom
b
etti,
M.
(		).
Rob
ot
shaping:
Dev
eloping
autonomous
agen
ts
through
learning.
A
rticial
Intel
ligenc
e,

(),
{0.


----- Page 43 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
Dorigo,
M.
(		).
Alecsys
and
the
AutonoMouse:
Learning
to
con
trol
a
real
rob
ot
b
y
distributed
classier
systems.
Machine
L
e
arning,
	.
Fiec
h
ter,
C.-N.
(		).
Ecien
t
reinforcemen
t
learning.
In
Pr
o
c
e
e
dings
of
the
Seventh
A
nnual
A
CM
Confer
enc
e
on
Computational
L
e
arning
The
ory,
pp.
{	.
Asso
ciation
of
Computing
Mac
hinery
.
Gittins,
J.
C.
(		).
Multi-arme
d
Bandit
A
l
lo
c
ation
Indic
es.
Wiley-In
terscience
series
in
systems
and
optimization.
Wiley
,
Chic
hester,
NY.
Goldb
erg,
D.
(		).
Genetic
algorithms
in
se
ar
ch,
optimization,
and
machine
le
arning.
Addison-W
esley
,
MA.
Gordon,
G.
J.
(		).
Stable
function
appro
ximation
in
dynamic
programming.
In
Priedi-
tis,
A.,
&
Russell,
S.
(Eds.),
Pr
o
c
e
e
dings
of
the
Twelfth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
{
San
F
rancisco,
CA.
Morgan
Kaufmann.
Gullapalli,
V.
(		0).
A
sto
c
hastic
reinforcemen
t
learning
algorithm
for
learning
real-v
alued
functions.
Neur
al
Networks,
,
{	.
Gullapalli,
V.
(		).
R
einfor
c
ement
le
arning
and
its
applic
ation
to
c
ontr
ol.
Ph.D.
thesis,
Univ
ersit
y
of
Massac
h
usetts,
Amherst,
MA.
Hilgard,
E.
R.,
&
Bo
w
er,
G.
H.
(	).
The
ories
of
L
e
arning
(fourth
edition).
Pren
tice-Hall,
Englew
o
o
d
Clis,
NJ.
Homan,
A.
J.,
&
Karp,
R.
M.
(	).
On
non
terminating
sto
c
hastic
games.
Management
Scienc
e,
,
	{0.
Holland,
J.
H.
(	).
A
daptation
in
Natur
al
and
A
rticial
Systems.
Univ
ersit
y
of
Mic
higan
Press,
Ann
Arb
or,
MI.
Ho
w
ard,
R.
A.
(	0).
Dynamic
Pr
o
gr
amming
and
Markov
Pr
o
c
esses.
The
MIT
Press,
Cam
bridge,
MA.
Jaakk
ola,
T.,
Jordan,
M.
I.,
&
Singh,
S.
P
.
(		).
On
the
con
v
ergence
of
sto
c
hastic
iterativ
e
dynamic
programming
algorithms.
Neur
al
Computation,

().
Jaakk
ola,
T.,
Singh,
S.
P
.,
&
Jordan,
M.
I.
(		).
Mon
te-carlo
reinforcemen
t
learning
in
non-Mark
o
vian
decision
problems.
In
T
esauro,
G.,
T
ouretzky
,
D.
S.,
&
Leen,
T.
K.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems

Cam
bridge,
MA.
The
MIT
Press.
Kaelbling,
L.
P
.
(		a).
Hierarc
hical
learning
in
sto
c
hastic
domains:
Preliminary
results.
In
Pr
o
c
e
e
dings
of
the
T
enth
International
Confer
enc
e
on
Machine
L
e
arning
Amherst,
MA.
Morgan
Kaufmann.
Kaelbling,
L.
P
.
(		b).
L
e
arning
in
Emb
e
dde
d
Systems.
The
MIT
Press,
Cam
bridge,
MA.
Kaelbling,
L.
P
.
(		a).
Asso
ciativ
e
reinforcemen
t
learning:
A
generate
and
test
algorithm.
Machine
L
e
arning,

().


----- Page 44 (native) -----
Kaelbling,
Littman,
&
Moore
Kaelbling,
L.
P
.
(		b).
Asso
ciativ
e
reinforcemen
t
learning:
Functions
in
k
-DNF.
Machine
L
e
arning,

().
Kirman,
J.
(		).
Pr
e
dicting
R
e
al-Time
Planner
Performanc
e
by
Domain
Char
acterization.
Ph.D.
thesis,
Departmen
t
of
Computer
Science,
Bro
wn
Univ
ersit
y
.
Ko
enig,
S.,
&
Simmons,
R.
G.
(		).
Complexit
y
analysis
of
real-time
reinforcemen
t
learning.
In
Pr
o
c
e
e
dings
of
the
Eleventh
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
		{0
Menlo
P
ark,
California.
AAAI
Press/MIT
Press.
Kumar,
P
.
R.,
&
V
araiy
a,
P
.
P
.
(	).
Sto
chastic
Systems:
Estimation,
Identic
ation,
and
A
daptive
Contr
ol.
Pren
tice
Hall,
Englew
o
o
d
Clis,
New
Jersey
.
Lee,
C.
C.
(		).
A
self
learning
rule-based
con
troller
emplo
ying
appro
ximate
reasoning
and
neural
net
concepts.
International
Journal
of
Intel
ligent
Systems,

(),
{	.
Lin,
L.-J.
(		).
Programming
rob
ots
using
reinforcemen
t
learning
and
teac
hing.
In
Pr
o
c
e
e
dings
of
the
Ninth
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e.
Lin,
L.-J.
(		a).
Hierac
hical
learning
of
rob
ot
skills
b
y
reinforcemen
t.
In
Pr
o
c
e
e
dings
of
the
International
Confer
enc
e
on
Neur
al
Networks.
Lin,
L.-J.
(		b).
R
einfor
c
ement
L
e
arning
for
R
ob
ots
Using
Neur
al
Networks.
Ph.D.
thesis,
Carnegie
Mellon
Univ
ersit
y
,
Pittsburgh,
P
A.
Lin,
L.-J.,
&
Mitc
hell,
T.
M.
(		).
Memory
approac
hes
to
reinforcemen
t
learning
in
non-
Mark
o
vian
domains.
T
ec
h.
rep.
CMU-CS-	-,
Carnegie
Mellon
Univ
ersit
y
,
Sc
ho
ol
of
Computer
Science.
Littman,
M.
L.
(		a).
Mark
o
v
games
as
a
framew
ork
for
m
ulti-agen
t
reinforcemen
t
learn-
ing.
In
Pr
o
c
e
e
dings
of
the
Eleventh
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
{
San
F
rancisco,
CA.
Morgan
Kaufmann.
Littman,
M.
L.
(		b).
Memoryless
p
olicies:
Theoretical
limitations
and
practical
results.
In
Cli,
D.,
Husbands,
P
.,
Mey
er,
J.-A.,
&
Wilson,
S.
W.
(Eds.),
F
r
om
A
nimals
to
A
nimats
:
Pr
o
c
e
e
dings
of
the
Thir
d
International
Confer
enc
e
on
Simulation
of
A
daptive
Behavior
Cam
bridge,
MA.
The
MIT
Press.
Littman,
M.
L.,
Cassandra,
A.,
&
Kaelbling,
L.
P
.
(		a).
Learning
p
olicies
for
partially
observ
able
en
vironmen
ts:
Scaling
up.
In
Prieditis,
A.,
&
Russell,
S.
(Eds.),
Pr
o
c
e
e
d-
ings
of
the
Twelfth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
{0
San
F
rancisco,
CA.
Morgan
Kaufmann.
Littman,
M.
L.,
Dean,
T.
L.,
&
Kaelbling,
L.
P
.
(		b).
On
the
complexit
y
of
solving
Mark
o
v
decision
problems.
In
Pr
o
c
e
e
dings
of
the
Eleventh
A
nnual
Confer
enc
e
on
Unc
ertainty
in
A
rticial
Intel
ligenc
e
(UAI{	)
Mon
treal,
Qu

eb
ec,
Canada.
Lo
v
ejo
y
,
W.
S.
(		).
A
surv
ey
of
algorithmic
metho
ds
for
partially
observ
able
Mark
o
v
decision
pro
cesses.
A
nnals
of
Op
er
ations
R
ese
ar
ch,
,
{.
0

----- Page 45 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
Maes,
P
.,
&
Bro
oks,
R.
A.
(		0).
Learning
to
co
ordinate
b
eha
viors.
In
Pr
o
c
e
e
dings
Eighth
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
	{0.
Morgan
Kaufmann.
Mahadev
an,
S.
(		).
T
o
discoun
t
or
not
to
discoun
t
in
reinforcemen
t
learning:
A
case
study
comparing
R
learning
and
Q
learning.
In
Pr
o
c
e
e
dings
of
the
Eleventh
Inter-
national
Confer
enc
e
on
Machine
L
e
arning,
pp.
{
San
F
rancisco,
CA.
Morgan
Kaufmann.
Mahadev
an,
S.
(		).
Av
erage
rew
ard
reinforcemen
t
learning:
Foundations,
algorithms,
and
empirical
results.
Machine
L
e
arning,

().
Mahadev
an,
S.,
&
Connell,
J.
(		a).
Automatic
programming
of
b
eha
vior-based
rob
ots
using
reinforcemen
t
learning.
In
Pr
o
c
e
e
dings
of
the
Ninth
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e
Anaheim,
CA.
Mahadev
an,
S.,
&
Connell,
J.
(		b).
Scaling
reinforcemen
t
learning
to
rob
otics
b
y
ex-
ploiting
the
subsumption
arc
hitecture.
In
Pr
o
c
e
e
dings
of
the
Eighth
International
Workshop
on
Machine
L
e
arning,
pp.
{.
Mataric,
M.
J.
(		).
Rew
ard
functions
for
accelerated
learning.
In
Cohen,
W.
W.,
&
Hirsh,
H.
(Eds.),
Pr
o
c
e
e
dings
of
the
Eleventh
International
Confer
enc
e
on
Machine
L
e
arning.
Morgan
Kaufmann.
McCallum,
A.
K.
(		).
R
einfor
c
ement
L
e
arning
with
Sele
ctive
Per
c
eption
and
Hidden
State.
Ph.D.
thesis,
Departmen
t
of
Computer
Science,
Univ
ersit
y
of
Ro
c
hester.
McCallum,
R.
A.
(		).
Ov
ercoming
incomplete
p
erception
with
utile
distinction
memory
.
In
Pr
o
c
e
e
dings
of
the
T
enth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
	0{
	
Amherst,
Massac
h
usetts.
Morgan
Kaufmann.
McCallum,
R.
A.
(		).
Instance-based
utile
distinctions
for
reinforcemen
t
learning
with
hidden
state.
In
Pr
o
c
e
e
dings
of
the
Twelfth
International
Confer
enc
e
Machine
L
e
arn-
ing,
pp.
{	
San
F
rancisco,
CA.
Morgan
Kaufmann.
Meeden,
L.,
McGra
w,
G.,
&
Blank,
D.
(		).
Emergen
t
con
trol
and
planning
in
an
au-
tonomous
v
ehicle.
In
T
ouretsky
,
D.
(Ed.),
Pr
o
c
e
e
dings
of
the
Fifte
enth
A
nnual
Me
eting
of
the
Co
gnitive
Scienc
e
So
ciety,
pp.
{0.
La
w
erence
Erlbaum
Asso
ciates,
Hills-
dale,
NJ.
Millan,
J.
d.
R.
(		).
Rapid,
safe,
and
incremen
tal
learning
of
na
vigation
strategies.
IEEE
T
r
ansactions
on
Systems,
Man,
and
Cyb
ernetics,

().
Monahan,
G.
E.
(	).
A
surv
ey
of
partially
observ
able
Mark
o
v
decision
pro
cesses:
Theory
,
mo
dels,
and
algorithms.
Management
Scienc
e,
,
{.
Mo
ore,
A.
W.
(		).
V
ariable
resolution
dynamic
programming:
Ecien
tly
learning
ac-
tion
maps
in
m
ultiv
ariate
real-v
alued
spaces.
In
Pr
o
c.
Eighth
International
Machine
L
e
arning
Workshop.


----- Page 46 (native) -----
Kaelbling,
Littman,
&
Moore
Mo
ore,
A.
W.
(		).
The
parti-game
algorithm
for
v
ariable
resolution
reinforcemen
t
learn-
ing
in
m
ultidimensional
state-spaces.
In
Co
w
an,
J.
D.,
T
esauro,
G.,
&
Alsp
ector,
J.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems
,
pp.
{
San
Mateo,
CA.
Morgan
Kaufmann.
Mo
ore,
A.
W.,
&
A
tk
eson,
C.
G.
(		).
An
in
v
estigation
of
memory-based
function
ap-
pro
ximators
for
learning
con
trol.
T
ec
h.
rep.,
MIT
Artical
In
telligence
Lab
oratory
,
Cam
bridge,
MA.
Mo
ore,
A.
W.,
&
A
tk
eson,
C.
G.
(		).
Prioritized
sw
eeping:
Reinforcemen
t
learning
with
less
data
and
less
real
time.
Machine
L
e
arning,
.
Mo
ore,
A.
W.,
A
tk
eson,
C.
G.,
&
Sc
haal,
S.
(		).
Memory-based
learning
for
con
trol.
T
ec
h.
rep.
CMU-RI-TR-	-,
CMU
Rob
otics
Institute.
Narendra,
K.,
&
Thathac
har,
M.
A.
L.
(		).
L
e
arning
A
utomata:
An
Intr
o
duction.
Pren
tice-Hall,
Englew
o
o
d
Clis,
NJ.
Narendra,
K.
S.,
&
Thathac
har,
M.
A.
L.
(	).
Learning
automata|a
surv
ey
.
IEEE
T
r
ansactions
on
Systems,
Man,
and
Cyb
ernetics,

(),
{.
P
eng,
J.,
&
Williams,
R.
J.
(		).
Ecien
t
learning
and
planning
within
the
Dyna
frame-
w
ork.
A
daptive
Behavior,

(),
{.
P
eng,
J.,
&
Williams,
R.
J.
(		).
Incremen
tal
m
ulti-step
Q-learning.
In
Pr
o
c
e
e
dings
of
the
Eleventh
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
{
San
F
rancisco,
CA.
Morgan
Kaufmann.
P
omerleau,
D.
A.
(		).
Neur
al
network
p
er
c
eption
for
mobile
r
ob
ot
guidanc
e.
Klu
w
er
Academic
Publishing.
Puterman,
M.
L.
(		).
Markov
De
cision
Pr
o
c
esses|Discr
ete
Sto
chastic
Dynamic
Pr
o-
gr
amming.
John
Wiley
&
Sons,
Inc.,
New
Y
ork,
NY.
Puterman,
M.
L.,
&
Shin,
M.
C.
(	).
Mo
died
p
olicy
iteration
algorithms
for
discoun
ted
Mark
o
v
decision
pro
cesses.
Management
Scienc
e,
,
{.
Ring,
M.
B.
(		).
Continual
L
e
arning
in
R
einfor
c
ement
Envir
onments.
Ph.D.
thesis,
Univ
ersit
y
of
T
exas
at
Austin,
Austin,
T
exas.
R

ude,
U.
(		).
Mathematic
al
and
c
omputational
te
chniques
for
multilevel
adaptive
meth-
o
ds.
So
ciet
y
for
Industrial
and
Applied
Mathematics,
Philadelphi
a,
P
ennsylv
ania.
Rumelhart,
D.
E.,
&
McClelland,
J.
L.
(Eds.).
(	).
Par
al
lel
Distribute
d
Pr
o
c
essing:
Explor
ations
in
the
micr
ostructur
es
of
c
o
gnition.
V
olume
:
Foundations.
The
MIT
Press,
Cam
bridge,
MA.
Rummery
,
G.
A.,
&
Niranjan,
M.
(		).
On-line
Q-learning
using
connectionist
systems.
T
ec
h.
rep.
CUED/F-INFENG/TR,
Cam
bridge
Univ
ersit
y.


----- Page 47 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
Rust,
J.
(		).
Numerical
dynamic
programming
in
economics.
In
Handb
o
ok
of
Computa-
tional
Ec
onomics.
Elsevier,
North
Holland.
Sage,
A.
P
.,
&
White,
C.
C.
(	).
Optimum
Systems
Contr
ol.
Pren
tice
Hall.
Salganico,
M.,
&
Ungar,
L.
H.
(		).
Activ
e
exploration
and
learning
in
real-v
alued
spaces
using
m
ulti-armed
bandit
allo
cation
indices.
In
Prieditis,
A.,
&
Russell,
S.
(Eds.),
Pr
o
c
e
e
dings
of
the
Twelfth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
0{
San
F
rancisco,
CA.
Morgan
Kaufmann.
Sam
uel,
A.
L.
(		).
Some
studies
in
mac
hine
learning
using
the
game
of
c
hec
k
ers.
IBM
Journal
of
R
ese
ar
ch
and
Development,
,
{	.
Reprin
ted
in
E.
A.
F
eigen
baum
and
J.
F
eldman,
editors,
Computers
and
Thought,
McGra
w-Hill,
New
Y
ork
	.
Sc
haal,
S.,
&
A
tk
eson,
C.
(		).
Rob
ot
juggling:
An
implemen
tation
of
memory-based
learning.
Contr
ol
Systems
Magazine,
.
Sc
hmidh
ub
er,
J.
(		).
A
general
metho
d
for
m
ulti-agen
t
learning
and
incremen
tal
self-
impro
v
emen
t
in
unrestricted
en
vironmen
ts.
In
Y
ao,
X.
(Ed.),
Evolutionary
Computa-
tion:
The
ory
and
Applic
ations.
Scien
tic
Publ.
Co.,
Singap
ore.
Sc
hmidh
ub
er,
J.
H.
(		a).
Curious
mo
del-buildi
ng
con
trol
systems.
In
Pr
o
c.
International
Joint
Confer
enc
e
on
Neur
al
Networks,
Singap
or
e,
V
ol.
,
pp.
{.
IEEE.
Sc
hmidh
ub
er,
J.
H.
(		b).
Reinforcemen
t
learning
in
Mark
o
vian
and
non-Mark
o
vian
en
vironmen
ts.
In
Lippman,
D.
S.,
Mo
o
dy
,
J.
E.,
&
T
ouretzky
,
D.
S.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems
,
pp.
00{0
San
Mateo,
CA.
Morgan
Kaufmann.
Sc
hraudolph,
N.
N.,
Da
y
an,
P
.,
&
Sejno
wski,
T.
J.
(		).
T
emp
oral
dierence
learning
of
p
osition
ev
aluation
in
the
game
of
Go.
In
Co
w
an,
J.
D.,
T
esauro,
G.,
&
Alsp
ector,
J.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems
,
pp.
{
San
Mateo,
CA.
Morgan
Kaufmann.
Sc
hrijv
er,
A.
(	).
The
ory
of
Line
ar
and
Inte
ger
Pr
o
gr
amming.
Wiley-In
terscience,
New
Y
ork,
NY.
Sc
h
w
artz,
A.
(		).
A
reinforcemen
t
learning
metho
d
for
maximizing
undiscoun
ted
re-
w
ards.
In
Pr
o
c
e
e
dings
of
the
T
enth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
	{0
Amherst,
Massac
h
usetts.
Morgan
Kaufmann.
Singh,
S.
P
.,
Barto,
A.
G.,
Grup
en,
R.,
&
Connolly
,
C.
(		).
Robust
reinforcemen
t
learning
in
motion
planning.
In
Co
w
an,
J.
D.,
T
esauro,
G.,
&
Alsp
ector,
J.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems
,
pp.
{
San
Mateo,
CA.
Morgan
Kaufmann.
Singh,
S.
P
.,
&
Sutton,
R.
S.
(		).
Reinforcemen
t
learning
with
replacing
eligibili
t
y
traces.
Machine
L
e
arning,

().


----- Page 48 (native) -----
Kaelbling,
Littman,
&
Moore
Singh,
S.
P
.
(		a).
Reinforcemen
t
learning
with
a
hierarc
h
y
of
abstract
mo
dels.
In
Pr
o
c
e
e
dings
of
the
T
enth
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
0{0
San
Jose,
CA.
AAAI
Press.
Singh,
S.
P
.
(		b).
T
ransfer
of
learning
b
y
comp
osing
solutions
of
elemen
tal
sequen
tial
tasks.
Machine
L
e
arning,

(),
{0.
Singh,
S.
P
.
(		).
L
e
arning
to
Solve
Markovian
De
cision
Pr
o
c
esses.
Ph.D.
thesis,
Depart-
men
t
of
Computer
Science,
Univ
ersit
y
of
Massac
h
usetts.
Also,
CMPSCI
T
ec
hnical
Rep
ort
	-.
Stengel,
R.
F.
(	).
Sto
chastic
Optimal
Contr
ol.
John
Wiley
and
Sons.
Sutton,
R.
S.
(		).
Generalization
in
Reinforcemen
t
Learning:
Successful
Examples
Using
Sparse
Coarse
Co
ding.
In
T
ouretzky
,
D.,
Mozer,
M.,
&
Hasselmo,
M.
(Eds.),
Neur
al
Information
Pr
o
c
essing
Systems
.
Sutton,
R.
S.
(	).
T
emp
or
al
Cr
e
dit
Assignment
in
R
einfor
c
ement
L
e
arning.
Ph.D.
thesis,
Univ
ersit
y
of
Massac
h
usetts,
Amherst,
MA.
Sutton,
R.
S.
(	).
Learning
to
predict
b
y
the
metho
d
of
temp
oral
dierences.
Machine
L
e
arning,

(),
	{.
Sutton,
R.
S.
(		0).
In
tegrated
arc
hitectures
for
learning,
planning,
and
reacting
based
on
appro
ximating
dynamic
programming.
In
Pr
o
c
e
e
dings
of
the
Seventh
International
Confer
enc
e
on
Machine
L
e
arning
Austin,
TX.
Morgan
Kaufmann.
Sutton,
R.
S.
(		).
Planning
b
y
incremen
tal
dynamic
programming.
In
Pr
o
c
e
e
dings
of
the
Eighth
International
Workshop
on
Machine
L
e
arning,
pp.
{.
Morgan
Kaufmann.
T
esauro,
G.
(		).
Practical
issues
in
temp
oral
dierence
learning.
Machine
L
e
arning,
,
{.
T
esauro,
G.
(		).
TD-Gammon,
a
self-teac
hing
bac
kgammon
program,
ac
hiev
es
master-
lev
el
pla
y
.
Neur
al
Computation,

(),
{	.
T
esauro,
G.
(		).
T
emp
oral
dierence
learning
and
TD-Gammon.
Communic
ations
of
the
A
CM,

(),
{.
Tham,
C.-K.,
&
Prager,
R.
W.
(		).
A
mo
dular
q-learning
arc
hitecture
for
manipula-
tor
task
decomp
osition.
In
Pr
o
c
e
e
dings
of
the
Eleventh
International
Confer
enc
e
on
Machine
L
e
arning
San
F
rancisco,
CA.
Morgan
Kaufmann.
Thrun,
S.
(		).
Learning
to
pla
y
the
game
of
c
hess.
In
T
esauro,
G.,
T
ouretzky
,
D.
S.,
&
Leen,
T.
K.
(Eds.),
A
dvanc
es
in
Neur
al
Information
Pr
o
c
essing
Systems

Cam
bridge,
MA.
The
MIT
Press.


----- Page 49 (native) -----
Reinf
or
cement
Learning:
A
Sur
vey
Thrun,
S.,
&
Sc
h
w
artz,
A.
(		).
Issues
in
using
function
appro
ximation
for
reinforcemen
t
learning.
In
Mozer,
M.,
Smolensky
,
P
.,
T
ouretzky
,
D.,
Elman,
J.,
&
W
eigend,
A.
(Eds.),
Pr
o
c
e
e
dings
of
the
		
Conne
ctionist
Mo
dels
Summer
Scho
ol
Hillsdale,
NJ.
La
wrence
Erlbaum.
Thrun,
S.
B.
(		).
The
role
of
exploration
in
learning
con
trol.
In
White,
D.
A.,
&
Sofge,
D.
A.
(Eds.),
Handb
o
ok
of
Intel
ligent
Contr
ol:
Neur
al,
F
uzzy,
and
A
daptive
Appr
o
aches.
V
an
Nostrand
Reinhold,
New
Y
ork,
NY.
Tsitsiklis,
J.
N.
(		).
Async
hronous
sto
c
hastic
appro
ximation
and
Q-learning.
Machine
L
e
arning,

().
Tsitsiklis,
J.
N.,
&
V
an
Ro
y
,
B.
(		).
F
eature-based
metho
ds
for
large
scale
dynamic
programming.
Machine
L
e
arning,

().
V
alian
t,
L.
G.
(	).
A
theory
of
the
learnable.
Communic
ations
of
the
A
CM,

(),
{.
W
atkins,
C.
J.
C.
H.
(		).
L
e
arning
fr
om
Delaye
d
R
ewar
ds.
Ph.D.
thesis,
King's
College,
Cam
bridge,
UK.
W
atkins,
C.
J.
C.
H.,
&
Da
y
an,
P
.
(		).
Q-learning.
Machine
L
e
arning,

(),
	{	.
Whitehead,
S.
D.
(		).
Complexit
y
and
co
op
eration
in
Q-learning.
In
Pr
o
c
e
e
dings
of
the
Eighth
International
Workshop
on
Machine
L
e
arning
Ev
anston,
IL.
Morgan
Kauf-
mann.
Williams,
R.
J.
(	).
A
class
of
gradien
t-estimating
algorithms
for
reinforcemen
t
learning
in
neural
net
w
orks.
In
Pr
o
c
e
e
dings
of
the
IEEE
First
International
Confer
enc
e
on
Neur
al
Networks
San
Diego,
CA.
Williams,
R.
J.
(		).
Simple
statistical
gradien
t-follo
wing
algorithms
for
connectionist
reinforcemen
t
learning.
Machine
L
e
arning,

(),
	{.
Williams,
R.
J.,
&
Baird,
I
I
I,
L.
C.
(		a).
Analysis
of
some
incremen
tal
v
arian
ts
of
p
olicy
iteration:
First
steps
to
w
ard
understanding
actor-critic
learning
systems.
T
ec
h.
rep.
NU-CCS-	-,
Northeastern
Univ
ersit
y
,
College
of
Computer
Science,
Boston,
MA.
Williams,
R.
J.,
&
Baird,
I
I
I,
L.
C.
(		b).
Tigh
t
p
erformance
b
ounds
on
greedy
p
olicies
based
on
imp
erfect
v
alue
functions.
T
ec
h.
rep.
NU-CCS-	-,
Northeastern
Univ
er-
sit
y
,
College
of
Computer
Science,
Boston,
MA.
Wilson,
S.
(		).
Classier
tness
based
on
accuracy
.
Evolutionary
Computation,

(),
{.
Zhang,
W.,
&
Dietteric
h,
T.
G.
(		).
A
reinforcemen
t
learning
approac
h
to
job-shop
sc
heduling.
In
Pr
o
c
e
e
dings
of
the
International
Joint
Confer
enc
e
on
A
rticial
Intel-
lienc
e.
