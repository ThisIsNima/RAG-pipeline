

----- Page 1 (native) -----
Domain Adaptation via Transfer Component Analysis
Sinno Jialin Pan1, Ivor W. Tsang2, James T. Kwok1 and Qiang Yang1
1Department of Computer Science and Engineering
Hong Kong University of Science and Technology, Hong Kong
2School of Computer Engineering, Nanyang Technological University, Singapore 639798
1{sinnopan,jamesk,qyang}@cse.ust.hk, 2ivortsang@ntu.edu.sg
Abstract
Domain adaptation solves a learning problem in a
target domain by utilizing the training data in a dif-
ferent but related source domain. Intuitively, dis-
covering a good feature representation across do-
mains is crucial.
In this paper, we propose to
ﬁnd such a representation through a new learn-
ing method, transfer component analysis (TCA),
for domain adaptation. TCA tries to learn some
transfer components across domains in a Repro-
ducing Kernel Hilbert Space (RKHS) using Max-
imum Mean Discrepancy (MMD). In the subspace
spanned by these transfer components, data distri-
butions in different domains are close to each other.
As a result, with the new representations in this
subspace, we can apply standard machine learning
methods to train classiﬁers or regression models in
the source domain for use in the target domain. The
main contribution of our work is that we propose
a novel feature representation in which to perform
domain adaptation via a new parametric kernel us-
ing feature extraction methods, which can dramati-
cally minimize the distance between domain distri-
butions by projecting data onto the learned transfer
components. Furthermore, our approach can han-
dle large datsets and naturally lead to out-of-sample
generalization. The effectiveness and efﬁciency of
our approach in are veriﬁed by experiments on two
real-world applications: cross-domain indoor WiFi
localization and cross-domain text classiﬁcation.
1
Introduction
Domain adaptation aims at adapting a classiﬁer or regression
model trained in a source domain for use in a target domain,
where the source and target domains may be different but re-
lated. This is particularly crucial when labeled data are in
short supply in the target domain. For example, in indoor
WiFi localization, it is very expensive to calibrate a localiza-
tion model in a large-scale environment. However, the WiFi
signal strength may be a function of time, device or space,
depending on dynamic factors. To reduce the re-calibration
effort, we might want to adapt a localization model trained
in one time period (the source domain) for a new time pe-
riod (the target domain), or to adapt the localization model
trained on one mobile device (the source domain) for a new
mobile device (the target domain). However, the distributions
of WiFi data collected over time or across devices may be
very different, hence domain adaptation is needed [Yang et
al., 2008]. Another example is sentiment classiﬁcation. To
reduce the effort of annotating reviews for various products,
we might want to adapt a learning system trained on some
types of products (the source domain) for a new type of prod-
uct (the target domain). However, terms used in the reviews
of different types of products may be very different. As a re-
sult, distributions of the data over different types of products
may be different and thus domain adaptation is again needed
[Blitzer et al., 2007].
A major computational problem in domain adaptation is
how to reduce the difference between the distributions of
source and target domain data. Intuitively, discovering a good
feature representation across domains is crucial. A good fea-
ture representation should be able to reduce the difference in
distributions between domains as much as possible, while at
the same time preserving important (geometric or statistical)
properties of the original data.
Recently, several approaches have been proposed to learn
a common feature representation for domain adaptation
[Daum´e III, 2007; Blitzer et al., 2006]. Daum´e III [2007]
proposed a simple heuristic nonlinear mapping function to
map the data from both source and target domains to a high-
dimensional feature space, where standard machine learning
methods are used to train classiﬁers. Blitzer et al. [2006] pro-
posed the so-called structural correspondence learning (SCL)
algorithm to induce correspondencesamong features from the
different domains. This method depends on the heuristic se-
lections of pivot features that appear frequently in both do-
mains. Although it is experimentally shown that SCL can re-
duce the difference between domains based on the A-distance
measure [Ben-David et al., 2007], the heuristic criterion of
pivot feature selection may be sensitive to different applica-
tions. Pan et al. [2008] proposed a new dimensionality re-
duction method, Maximum Mean Discrepancy Embedding
(MMDE), for domain adaptation. The motivation of MMDE
is similar to our proposed work. It also aims at learning a
shared latent space underlying the domains where distance
between distributions can be reduced. However, MMDE suf-
1187

----- Page 2 (native) -----
fers from two major limitations: (1) MMDE is transductive,
and does not generalize to out-of-sample patterns; (2) MMDE
learns the latent space by solving a semi-deﬁnite program
(SDP), which is a very expensive optimization problem.
In this paper, we propose a new feature extraction ap-
proach, called transfer component analysis (TCA), for do-
main adaptation. It tries to learn a set of common transfer
components underlying both domains such that the differ-
ence in distributions of data in the different domains, when
projected onto this subspace, can be dramatically reduced.
Then, standard machine learning methods can be used in this
subspace to train classiﬁers or regression models across do-
mains. More speciﬁcally, if two domains are related to each
other, there may exist several common components (or latent
variables) underlying them. Some of these components may
cause the data distributions between domains to be different,
while others may not. Some of these components may cap-
ture the intrinsic structure underlying the original data, while
others may not. Our goal is to discover those components
that do not cause distribution change across the domains and
capture the structure of the original data well. We will show
in this paper that, compared to MMDE, TCA is much more
efﬁcient and can handle the out-of-sample extension problem.
The rest of the paper is organized as follows. Section 2 ﬁrst
describes the problem statement and preliminaries of domain
adaptation. Our proposed method is presented in Section 3.
We then review some related works in Section 4. In Section
5, we conduct a series of experiments on indoor WiFi local-
ization and text classiﬁcation. The last section gives some
conclusive discussions.
In the sequel, A ≻0 (resp. A ⪰0) means that the ma-
trix A is symmetric and positive deﬁnite (pd) (resp. positive
semideﬁnite (psd)). Moreover, the transpose of vector / ma-
trix (in both the input and feature spaces) is denoted by the
superscript ⊤, A† is the pseudo-inverse of the matrix A, and
tr(A) denotes the trace of A.
2
Preliminaries of Domain Adaptation
In this paper, we focus on the setting where the target domain
has no labeled training data, but has plenty of unlabeled data.
We also assume that some labeled data DS are available in
a source domain, while only unlabeled data DT are available
in the target domain. We denote the source domain data as
DS = {(xS1, yS1), . . . , (xSn1 , ySn1)}, where xSi ∈X is the
input and ySi ∈Y is the corresponding output. Similarly,
we denote the target domain data as DT = {xT1, . . . , xTn2 },
where the input xTi is also in X. Let P(XS) and Q(XT )
(or P and Q for short) be the marginal distributions of XS
and XT , respectively. In general, P and Q can be different.
Our task is then to predict the labels yTi’s corresponding to
the inputs xTi’s in the target domain. The key assumption
in a typical domain adaptation setting is that P ̸= Q, but
P(YS|XS) = P(YT |XT ) .
2.1
Maximum Mean Discrepancy
Many criteria, such as the Kullback-Leibler (KL) divergence,
can be used to estimate the distance between distributions.
However, many of these criteria are parametric, since an in-
termediate density estimate is usually required.
To avoid
such a non-trivial task, a non-parametric distance estimate
between distributions is more desirable.
Recently, Borg-
wardt et al. [2006] proposed the Maximum Mean Discrepancy
(MMD) as a relevant criterion for comparing distributions
based on the Reproducing Kernel Hilbert Space (RKHS). Let
X = {x1, . . . , xn1} and Y = {y1, . . . , yn2} be random vari-
able sets with distributions P and Q. The empirical estimate
of the distance between P and Q, as deﬁned by MMD, is
Dist(X,Y) =

1
n1
n1

i=1
φ(xi) −
1
n2
n2

i=1
φ(yi)

H
.
(1)
where H is a universal RKHS [Steinwart, 2001], and φ :
X →H.
Therefore, the distance between distributions of two sam-
ples can be well-estimated by the distance between the means
of the two samples mapped into a RKHS.
3
Transfer Component Analysis
Based on the inputs {xSi} and outputs {ySi} from the source
domain, and the inputs {xTi} from the target domain, our
task is to predict the unknown outputs {yTi} in the target do-
main. The general assumption in domain adaptation is that
the marginal densities, P(XS) and Q(XT ), are very differ-
ent.
In this section, we attempt to ﬁnd a common latent
representation for both XS and XT that preserves the data
conﬁguration of the two domains after transformation. Let
the desired nonlinear transformation be φ : X →H. Let
X′
S = {x′
Si} = {φ(xSi)}, X′
T = {x′
Ti} = {φ(xTi)} and
X′ = X′
S∪X′
T be the transformed input sets from the source,
target and combined domains, respectively. Then, we desire
that P′(X′
S) = Q′(X′
T ).
Assuming that φ is the feature map induced by a universal
kernel. As shown in Section 2.1, the distance between two
distributions P and Q can be empirically measured by the
(squared) distance between the empirical means of the two
domains:
Dist(X′
S, X′
T )=

1
n1
n1

i=1
φ(xSi) −1
n2
n2

i=1
φ(xTi)

2
H
. (2)
Therefore, a desired nonlinear mapping φ can be found by
minimizing this quantity. However, φ is usually highly non-
linear and a direct optimization of (2) can get stuck in poor
local minima. We thus need to ﬁnd a new approach, based on
the following assumption.
The key assumption in the proposed domain adaptation set-
ting is that P ̸= Q, but P(YS|φ(XS)) = P(YT |φ(XT )) un-
der a transformation mapping φ on the input.
In Section 3.1, we ﬁrst revisit Maximum Mean Discrep-
ancy Embedding (MMDE) which proposed to learn the kernel
matrix K corresponding to the nonlinear mapping φ by solv-
ing a SDP optimization problem. In Section 3.2, we then pro-
pose a factorization of the kernel matrix for MMDE. An ef-
ﬁcient eigendecomposition algorithm for kernel learning and
computational issues are discussed in Sections 3.3 and 3.4.
1188

----- Page 3 (native) -----
3.1
Kernel Learning for Domain Adaptation
Instead of ﬁnding the nonlinear transformation φ explicitly,
Pan et al. [2008] proposed to transform this problem as a
kernel learning problem. By virtue of the kernel trick, (i.e.,
k(xi, xj) = φ(xi)′φ(xj)), the distance between the empirical
means of the two domains in (2) can be written as:
Dist(X′
S, X′
T ) = tr(KL),
(3)
where
K =

KS,S
KS,T
KT,S
KT,T

(4)
is a (n1 + n2) × (n1 + n2) kernel matrix, KS,S, KT,T and
KS,T respectively are the kernel matrices deﬁned by k on the
data in the source domain, target domain, and cross domains;
and L = [Lij] ⪰0 with Lij =
1
n2
1 if xi, xj ∈XS; Lij =
1
n2
2
if xi, xj ∈XT ; otherwise, −
1
n1n2 .
In the transductive setting, learning the kernel k(·, ·) can
be solved by learning the kernel matrix K instead. In [Pan
et al., 2008], the resultant kernel matrix learning problem is
formulated as a semi-deﬁnite program (SDP). Principal Com-
ponent Analysis (PCA) is then applied on the learned kernel
matrix to ﬁnd a low-dimensional latent space across domains.
This is referred to as Maximum Mean Discrepancy Embed-
ding (MMDE).
3.2
Parametric Kernel Map for Unseen Patterns
There are several limitations of MMDE. First, it is trans-
ductive and cannot generalize on unseen patterns. Second,
the criterion (3) requires K to be positive semi-deﬁnite and
the resultant kernel learning problem has to be solved by
expensive SDP solvers. Finally, in order to construct low-
dimensional representations of X′
S and X′
T , the obtained K
has to be further post-processed by PCA. This may poten-
tially discard useful information in K.
In this paper, we propose an efﬁcient method to ﬁnd a non-
linear mapping φ based on kernel feature extraction. It avoids
the use of SDP and thus its high computational burden. More-
over, the learned kernel k can be generalized to out-of-sample
patterns directly. Besides, instead of using a two-step ap-
proach as in MMDE, we propose a uniﬁed kernel learning
method which utilizes an explicit low-rank representation.
First, recall that the kernel matrix K in (4) can be decom-
posed as K = (KK−1/2)(K−1/2K), which is often known
as the empirical kernel map [Sch¨olkopf et al., 1998]. Con-
sider the use of a (n1 + n2) × m matrix 
W to transform the
corresponding feature vectors to a m-dimensional space. In
general, m ≪n1 + n2. The resultant kernel matrix1 is then
K = (KK−1/2
W)(
W ⊤K−1/2K) = KWW ⊤K,
(5)
where W = K−1/2
W ∈R(n1+n2)×m. In particular, the cor-
responding kernel evaluation of k between any two patterns
xi and xj is given by
k(xi, xj) = k⊤
xiWW ⊤kxj,
(6)
1As is common practice, one can ensure that the kernel matrix
K is positive deﬁnite by adding a small ϵ > 0 to its diagonal [Pan et
al., 2008].
where kx
=
[k(x1, x), . . . , k(xn1+n2, x)]⊤
∈
Rn1+n2.
Hence, the kernel k in (6) facilitates a readily parametric form
for out-of-sample kernel evaluations.
Moreover, using the deﬁnition of K in (5), the distance be-
tween the empirical means of the two domains can be rewrit-
ten as:
Dist(X′
S, X′
T ) = tr((KWW ⊤K)L)
= tr(W ⊤KLKW).
(7)
3.3
Transfer Components Extraction
In minimizing criterion (7), a regularization term tr(W ⊤W)
is usually needed to control the complexity of W. As will
be shown later in this section, this regularization term can
avoid the rank deﬁciency of the denominator in the general-
ized eigendecomposition. The kernel learning problem for
domain adaptation then reduces to:
minW
tr(W ⊤W) + μ tr(W ⊤KLKW)
s.t.
W ⊤KHKW = I,
(8)
where μ is a trade-off parameter, I ∈Rm×m is the identity
matrix, H = In1+n2 −
1
n1+n2 11⊤is the centering matrix,
where 1 ∈Rn1+n2 is the column vector with all ones, and
In1+n2 ∈R(n1+n2)×(n1+n2) is the identity matrix. More-
over, note that the constraint W ⊤KHKW = I is added in
(8) to avoid the trivial solution (W = 0), such that the trans-
formed patterns do not collapse to one point, which can in-
ﬂate the learned kernel k such that the embedding of data x′
i
is preserved as in kernel PCA.
Though the optimization problem (8) involves a non-
convex norm constraint W ⊤KHKW = I, it can still be
solved efﬁciently by the following trace optimization prob-
lem:
Proposition 1 The optimization problem (8) can be re-
formulated as
min
W
tr((W ⊤KHKW)†W ⊤(I + μKLK)W),
(9)
or
max
W
tr((W ⊤(I + μKLK)W)−1W ⊤KHKW).
(10)
Proof. The Lagrangian of (8) is
tr(W ⊤(I + μKLK)W) −tr((W ⊤KHKW −I)Z), (11)
where Z is a symmetric matrix. Setting the derivative of (11)
w.r.t. W to zero, we have
(I + μKLK)W = KHKWZ.
(12)
Multiplying both sides on the left by W T , and then on
substituting it into (11), we obtain (9).
Since the matrix
I + μKLK is non-singular beneﬁted from the regularization
term tr(W ⊤W), we obtain an equivalent trace maximization
problem (10).
□
Similar to kernel Fisher discriminant (KFD), the solution
of W in (10) is the eigenvectors corresponding to the m lead-
ing eigenvalues of (I + μKLK)−1KHK, where at most
n1 + n2 −1 eigenvectors can be extracted. In the sequel,
the proposed method is referred to as Transfer Component
Analysis (TCA).
1189

----- Page 4 (native) -----
3.4
Computational Issues
The kernel learning algorithm in [Pan et al., 2008] relies on
SDPs. As there are O((n1 +n2)2) variables in K, the overall
training complexity is O((n1 + n2)6.5) [Nesterov and Ne-
mirovskii, 1994]. This becomes computationally prohibitive
even for small-sized problems. Note that criterion (3) in this
kernel learning problem is similar to the recently proposed
supervised dimensionality reduction method colored MVU
[Song et al., 2008], in which low-rank approximation is used
to reduce the number of constraints and variables in the SDP.
However, gradient descent is required to reﬁne the embedding
space and thus the solution can get stuck in a local minimum.
On the other hand, our proposed kernel learning method re-
quires only a simple and efﬁcient eigendecomposition. This
takes only O(m(n1 +n2)2) time when m non-zero eigenvec-
tors are to be extracted [Sorensen, 1996].
4
Related Works
Domain adaptation, which can be considered as a special
setting of transfer learning [Pan and Yang, 2008], has been
widely studied in natural language processing (NLP) [Ando
and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007].
Ando and Zhang [2005] and Blitzer [2006] proposed struc-
tural correspondence learning (SCL) algorithms to learn the
common feature representation across domains based on
some heuristic selection of pivot features. Daum´e III [2007]
designed a heuristic kernel to augment features for solving
some speciﬁc domain adaptation problems in NLP. Besides,
domain adaptation has also been investigated in other appli-
cation areas such as sentiment classiﬁcation [Blitzer et al.,
2007]. Theoretical analysis of domain adaptation has also
been studied in [Ben-David et al., 2007].
The problem of sample selection bias (also referred to as
co-variate shift) is also related to domain adaption. In sam-
ple selection bias, the basic assumption is that the sampling
processes between the training data Xtrn and test data Xtst
may be different.
As a result, P(Xtrn) ̸= P(Xtst), but
P(Ytrn|Xtrn) = P(Ytst|Xtst). Instance re-weighting is a
major technique for correcting sample selection bias [Huang
et al., 2007; Sugiyama et al., 2008]. Recently, a state-of-
art method, called kernel mean matching (KMM), is pro-
posed [Huang et al., 2007].
It re-weights instances in a
RKHS based on the MMD theory, which is different from our
proposed method. Sugiyama et al. [2008] proposed another
re-weighting algorithm, Kullback-Leibler Importance Esti-
mation Procedure (KLIEP), which is integrated with cross-
validation to perform model selection automatically. Xing et
al. [2007] proposed to correct the labels predicted by a shift-
unaware classiﬁer towards a target distribution based on the
mixture distribution of the training and test data. Matching
distributions by re-weighting instances is also used success-
fully in Multi-task Learning [Bickel et al., 2008]. However,
unlike instance re-weighting, the proposed TCA method can
cope with noisy features (as in image data and WiFi data) by
effectively denoising and ﬁnding a latent space for matching
distributions across different domains simultaneously. Thus,
TCA can be treated as an integration of unsupervised feature
extraction and distribution matching in a latent space.
5
Experiments
In this section, we apply the proposed domain adaptation al-
gorithm TCA on two real-world problems: indoor WiFi lo-
calization and text classiﬁcation.
5.1
Cross-domain WiFi Localization
For cross-domain WiFi localization, we use a dataset pub-
lished in the 2007 IEEE ICDM Contest [Yang et al., 2008].
This dataset contains some labeled WiFi data collected in
time period A (the source domain) and a large amount of un-
labeled WiFi data collected in time period B (the target do-
main). Here, a label means the corresponding location where
the WiFi data are received. WiFi data collected from different
time periods are considered as different domains. The task is
to predict the labels of the WiFi data collected in time pe-
riod B. More speciﬁcally, all the WiFi data are collected in
an indoor building around 145.5 × 37.5 m2, 621 labeled data
are collected in time period A and 3128 unlabeled data are
collected in time period B.
We conduct a series of experiments to compare TCA with
some baselines, including other feature extraction methods
such as KPCA, sample selection bias (or co-variate shift)
methods, KMM and KLIEP and a domain adaptation method,
SCL. For each experiment, all labeled data in the source
domain and some unlabeled data in the target domain are
used for training. Evaluation is then performed on the re-
maining unlabeled data (out-of-sample) in the target domain.
This is repeated 10 times and the average performance is
used to measure the generalization abilities of the methods.
In addition, to compare the performance between TCA and
MMDE, we conduct some experiments in the transductive
setting [Nigam et al., 2000]. The evaluation criterion is the
Average Error Distance (AED) on the test data, and the lower
the better. For determining parameters for each method, we
randomly select a very small subset of the target domain data
to tune parameters. The values of parameters are ﬁxed for all
the experiments.
Figure 1(a) compares the performance of Regularized
Least Square Regression (RLSR) model on different feature
representations learned by TCA, KPCA and SCL, and dif-
ferent re-weighted instances learned by KMM and KLIEP.
Here, we use μ = 0.1 for TCA and the Laplacian kernel.
As can be seen, the performance can be improved with the
new feature representations of TCA and KPCA. TCA can
achieve much higher performance because it aims at ﬁnding
the leading components that minimize the difference between
domains. Then, from the space spanned by these components,
the model trained in one domain can be used to perform ac-
curate prediction in the other domain.
Figure 1(b) shows the results under a varying number of
unlabeled data in the target main. As can be seen, with only
a few unlabeled data in the target domain, TCA can still ﬁnd
a good feature representation to bridge between domains.
Since MMDE cannot generalize to out-of-sample patterns,
in order to compare TCA with MMDE, we conduct another
series of experiments in a transductive setting, which means
that the trained models are only evaluated on the unlabeled
data that are used for learning the latent space. In Figure 1(c),
we apply MMDE and TCA on 621 labeled data from the
1190

----- Page 5 (native) -----
0
5
10
15
20
25
30
35
2
3
4
5
6
7
8
9
10
# Dimensions
Average Error Distance (unit: m)
TCA
Original
KPCA
KMM
KLIEP
SCL
(a) Varying # features of domain adaptation
methods. (# unlabeled data is 300).
0
100
200
300
400
500
600
700
800
900
2
3
4
5
6
7
8
9
10
11
12
# Unlabeled Data for Training
Average Error Distance (unit: m)
TCA
KPCA
KMM
KLIEP
SCL
(b) Varying # unlabeled data (# dimensions of
TCA and KPCA is 10).
0
5
10
15
20
25
30
35
1
2
3
4
5
6
7
8
# Dimensions
Average Error Distance (unit: m)
TCA
MMDE
(c) Comparison of MMDE and TCA in the
transductive setting (# unlabeled data is 300).
Figure 1: Comparison of Average Error Distance (in m).
source domain and 300 unlabeled data from the target do-
main to learn new representations, respectively, and then train
RLSR on them. More comparison results in terms of ACE
with varying number of training data are shown in Table 1.
The experimental results show that TCA is slightly higher
(worse) than MMDE in terms of AED. This is due to the non-
parametric kernel matrix learned by MMDE, which can ﬁt
the observed unlabeled data better. However, as mentioned in
Section 3.4, the cost of MMDE is expensive due to the com-
putationally intensive SDP. The comparison results between
TCA and MMDE in terms of computational time on the WiFi
dataset are shown in Table 2.
Table 1: ACE (in m) of MMDE and TCA with 10 dimensions
and varying # training data (# labeled data in the source do-
main is ﬁxed to 621, # unlabeled data in the target domain
varies from 100 to 800.)
# unlabeled and labeled data used for training
721
821
921
1,021
1,121
1,221
1,321
1,421
TCA
2.413 2.378 2.313 2.285
2.271
2.285
2.287
2.289
MMDE 2.315 2.247 2.208 2.212 2.207
2.182
2.257
2.279
Table 2: CPU training time (in sec) of MMDE and TCA with
varying # training data.
# unlabeled and labeled data used for training
721
821
921
1,021 1,121
1,221
1,321
1,421
TCA
25
30
46
59
72
94
115
145
MMDE 3,209 3,539 4,168 4,940 10,093 14,165 18,094 33,004
5.2
Cross-domain Text Classiﬁcation
In this section, we perform cross-domain binary classiﬁca-
tion experiments on a preprocessed dataset of Reuters-21578.
These data are categorized to a hierarchical structure. Data
from different sub-categories under the same parent category
are considered to be from different but related domains. The
task is to predict the labels of the parent category. By follow-
ing this strategy, three datasets orgs vs people, orgs vs places
and people vs places are constructed. We randomly select
50% labeled data from the source domain, and 35% unla-
beled data from the target domain. Evaluation is based on
the (out-of-sample) testing of the remaining 65% unlabeled
data in the target domain. This is repeated 10 times and the
average results reported.
Similar to the experimental setting on WiFi localization,
we conduct a series of experiments to compare TCA with
KPCA, KMM, KLIEP and SCL. Here, the support vector ma-
chine (SVM) is used as the classiﬁer. The evaluation crite-
rion is the classiﬁcation accuracy (the higher the better). We
experiment with both the RBF kernel and linear kernel for
feature extraction or re-weighting used by KPCA, TCA and
KMM. The kernel used in the SVM for ﬁnal prediction is a
linear kernel, and the parameter μ in TCA is set to 0.1.
As can be seen from Table 3, different from experiments
on the WiFi data, sample selection bias methods, such as
KMM and KLIEP perform better than KPCA and PCA on the
text data. However, with the feature presentations learned by
TCA, SVM performs the best for cross-domain classiﬁcation.
This is because TCA not only discovers latent topics behind
the text data, but also matches distributions across domains in
the latent space spanned by the latent topics. Moreover, the
performance of TCA using the RBF kernel is more stable.
6
Conclusion and Future Work
Learning feature representations is of primarily an important
task for domain adaptation. In this paper, we propose a new
feature extraction method, called Transfer Component Anal-
ysis (TCA), to learn a set of transfer components which re-
duce the distance across domains in a RKHS. Compared to
the previously proposed MMDE for the same task, TCA is
much more efﬁcient and can be generalized to out-of-sample
patterns. Experiments on two real-world datasets verify the
effectiveness of the proposed method. In the future, we are
planning to take side information into account when learning
the transfer components across domains, which may be better
for the ﬁnal classiﬁcation or regression tasks.
7
Acknowledgement
Sinno Jialin Pan and Qiang Yang thank the support from Mi-
crosoft Research MRA07/08.EG01 and Hong Kong CERG
Project 621307. Ivor W. Tsang thanks the support from Sin-
gapore MOE AcRF Tier-1 Research Grant (RG15/08). James
T. Kwok thanks the support from CERG project 614508.
1191

----- Page 6 (native) -----
Table 3: Comparison between Different Methods (number inside parentheses is the standard deviation over 10 repetitions).
features
#features
people vs places
orgs vs people
orgs vs places
Original
0.5198 (.0252)
0.6696 (.0287)
0.6683 (.0221)
PCA
5
0.5564 (.0788)
0.5574 (.0760)
0.5653 (.0984)
10
0.5453 (.0911)
0.6470 (.0598)
0.6140 (.0534)
20
0.5424 (.0590)
0.6703 (.0334)
0.6491 (.0391)
30
0.5631 (.0346)
0.6652 (.0549)
0.6114 (.0564)
KPCA (RBF)
5
0.5900 (.0185)
0.5863 (0.0405)
0.5883 (.0185)
10
0.5934 (.0169)
0.5955 (0.0676)
0.6267 (.0814)
20
0.6032 (.0323)
0.5968 (0.0705)
0.6098 (.0315)
30
0.6000 (.0267)
0.5964 (0.0742)
0.6247 (.0438)
TCA (linear)
5
0.5804 (.0528)
0.6397 (.0897)
0.6403 (.0722)
10
0.5495 (.0764)
0.7308 (.0495)
0.7006 (.0527)
20
0.5600 (.0969)
0.7425 (.0579)
0.6720 (.0374)
30
0.5468 (.0635)
0.7330 (.0432)
0.5989 (.0700)
TCA (RBF)
5
0.6129 (.0176)
0.6297 (.0302)
0.6899 (.0195)
10
0.5920 (.0148)
0.7088 (.0251)
0.7042 (.0218)
20
0.5954 (.0201)
0.7196 (.0235)
0.6942 (.0220)
30
0.5916 (.0166)
0.7217 (.0275)
0.6896 (.0203)
SCL
0.5267 (.0310)
0.6834 (.0327)
0.6733 (.0198)
KMM (linear)
0.5836 (.0159)
0.7006 (.0353)
0.6714 (.0263)
KMM (RBF)
0.5836 (.0159)
0.6968 (.0224)
0.6655 (.0245)
KLIEP
0.5758 (.0241)
0.6946 (.0192)
0.6638 (.0112)
References
[Ando and Zhang, 2005] Rie Kubota Ando and Tong Zhang.
A
high-performance semi-supervised learning method for text
chunking. In Proceedings of ACL, pages 1–9, Morristown, NJ,
USA, 2005.
[Ben-David et al., 2007] Shai Ben-David,
John Blitzer,
Koby
Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In NIPS 19, pages 137–144, 2007.
[Bickel et al., 2008] Steffen Bickel, Jasmina Bogojeska, Thomas
Lengauer, and Tobias Scheffer. Multi-task learning for hiv ther-
apy screening. In Proceedings of ICML, pages 56–63, Helsinki,
Finland, 2008.
[Blitzer et al., 2006] John Blitzer, Ryan McDonald, and Fernando
Pereira. Domain adaptation with structural correspondence learn-
ing. In Proceedings of EMNLP, pages 120–128, Sydney, Aus-
tralia, 2006.
[Blitzer et al., 2007] John Blitzer, Mark Dredze, and Fernando
Pereira. Biographies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classiﬁcation. In Proceedings of
ACL, pages 432–439, Prague, Czech Republic, 2007.
[Borgwardt et al., 2006] Karsten M. Borgwardt, Arthur Gretton,
Malte J. Rasch, Hans-Peter Kriegel, Bernhard Sch¨olkopf, and
Alexander J. Smola.
Integrating structured biological data by
kernel maximum mean discrepancy. In ISMB, pages 49–57, For-
taleza, Brazil, 2006.
[Daum´e III, 2007] Hal Daum´e III. Frustratingly easy domain adap-
tation. In Proceedings of ACL, pages 256–263, Prague, Czech
Republic, 2007.
[Huang et al., 2007] Jiayuan Huang, Alex Smola, Arthur Gretton,
Karsten M. Borgwardt, and Bernhard Sch¨olkopf. Correcting sam-
ple selection bias by unlabeled data. In NIPS 19, pages 601–608,
2007.
[Nesterov and Nemirovskii, 1994] Yurii Nesterov and Arkadii Ne-
mirovskii. Interior-point Polynomial Algorithms in Convex Pro-
gramming.
Society for Industrial and Applied Mathematics,
Philadelphia, PA, 1994.
[Nigam et al., 2000] Kamal Nigam, Andrew K. Mccallum, Sebas-
tian Thrun, and Tom Mitchell. Text classiﬁcation from labeled
and unlabeled documents using em. In Machine Learning, vol-
ume 39, pages 103–134, 2000.
[Pan and Yang, 2008] Sinno Jialin Pan and Qiang Yang. A survey
on transfer learning. Technical Report HKUST-CS08-08, Depart-
ment of Computer Science and Engineering, Hong Kong Univer-
sity of Science and Technology, Hong Kong, 2008.
[Pan et al., 2008] Sinno Jialin Pan, James T. Kwok, and Qiang
Yang. Transfer learning via dimensionality reduction. In Pro-
ceedings of AAAI, pages 677–682, Chicago, Illinois, USA, 2008.
[Sch¨olkopf et al., 1998] Bernhard Sch¨olkopf, Alexander Smola,
and Klaus-Robert M¨uller. Nonlinear component analysis as a ker-
nel eigenvalue problem. Neural Computation, 10(5):1299–1319,
1998.
[Song et al., 2008] Le Song, Alex Smola, Karsten Borgwardt, and
Arthur Gretton. Colored maximum variance unfolding. In NIPS
20, pages 1385–1392, 2008.
[Sorensen, 1996] Danny
C.
Sorensen.
Implicitly
restarted
Arnoldi/Lanczos methods for large scale eigenvalue calculations.
Technical Report TR-96-40, Department of Computational and
Applied Mathematics, Rice University, 1996.
[Steinwart, 2001] Ingo Steinwart. On the inﬂuence of the kernel on
the consistency of support vector machines. Journal of Machine
Learning Research, 2:67–93, 2001.
[Sugiyama et al., 2008] Masashi Sugiyama, Shinichi Nakajima,
Hisashi Kashima, Paul Von Buenau, and Motoaki Kawanabe. Di-
rect importance estimation with model selection and its applica-
tion to covariate shift adaptation. In NIPS 20, pages 1433–1440,
2008.
[Xing et al., 2007] Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and
Yong Yu. Bridged reﬁnement for transfer learning. In PKDD,
pages 324–335, Warsaw, Poland, 2007.
[Yang et al., 2008] Qiang Yang,
Sinno Jialin Pan,
and Vin-
cent Wenchen Zheng. Estimating location using Wi-Fi. IEEE
Intelligent Systems, 23(1):8–13, 2008.
1192