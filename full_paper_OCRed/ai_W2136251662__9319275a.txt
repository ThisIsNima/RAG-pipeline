

----- Page 1 (native) -----
1778
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
Classiﬁcation of Hyperspectral Remote Sensing
Images With Support Vector Machines
Farid Melgani, Member, IEEE, and Lorenzo Bruzzone, Senior Member, IEEE
Abstract—This paper addresses the problem of the classiﬁca-
tion of hyperspectral remote sensing images by support vector
machines (SVMs). First, we propose a theoretical discussion and
experimental analysis aimed at understanding and assessing the
potentialities of SVM classiﬁers in hyperdimensional feature
spaces. Then, we assess the effectiveness of SVMs with respect
to conventional feature-reduction-based approaches and their
performances in hypersubspaces of various dimensionalities. To
sustain such an analysis, the performances of SVMs are compared
with those of two other nonparametric classiﬁers (i.e., radial basis
function neural networks and the K-nearest neighbor classiﬁer).
Finally, we study the potentially critical issue of applying binary
SVMs to multiclass problems in hyperspectral data. In particular,
four different multiclass strategies are analyzed and compared:
the one-against-all, the one-against-one, and two hierarchical
tree-based strategies. Different performance indicators have
been used to support our experimental studies in a detailed and
accurate way, i.e., the classiﬁcation accuracy, the computational
time, the stability to parameter setting, and the complexity of
the multiclass architecture. The results obtained on a real Air-
borne Visible/Infrared Imaging Spectroradiometer hyperspectral
dataset allow to conclude that, whatever the multiclass strategy
adopted, SVMs are a valid and effective alternative to conventional
pattern recognition approaches (feature-reduction procedures
combined with a classiﬁcation method) for the classiﬁcation of
hyperspectral remote sensing data.
Index Terms—Classiﬁcation, feature reduction, Hughes phe-
nomenon, hyperspectral images, multiclass problems, remote
sensing, support vector machines (SVMs).
I. INTRODUCTION
R
EMOTE sensing images acquired by multispectral sen-
sors, such as the widely used Landsat Thematic Mapper
(TM) sensor, have shown their usefulness in numerous earth
observation (EO) applications. In general, the relatively small
number of acquisition channels that characterizes multispec-
tral sensors may be sufﬁcient to discriminate among different
land-cover classes (e.g., forestry, water, crops, urban areas, etc.).
However, their discrimination capability is very limited when
different types (or conditions) of the same species (e.g., different
types of forest) are to be recognized. Hyperspectral sensors can
be used to deal with this problem. These sensors are character-
ized by a very high spectral resolution that usually results in
hundreds of observation channels. Thanks to these channels, it
Manuscript received November 4, 2003; revised May 16, 2004. This work
was supported by the Italian Ministry of Education, Research and University
(MIUR).
The authors are with the Department of Information and Communication
Technologies, University of Trento, I-38050 Trento, Italy (e-mail: mel-
gani@dit.unitn.it; lorenzo.bruzzone@ing.unitn.it).
Digital Object Identiﬁer 10.1109/TGRS.2004.831865
is possible to address various additional applications requiring
very high discrimination capabilities in the spectral domain (in-
cluding material quantiﬁcation and target detection). From a
methodological viewpoint, the automatic analysis of hyperspec-
tral data is not a trivial task. In particular, it is made complex by
many factors, such as: 1) the large spatial variability of the hy-
perspectral signature of each land-cover class; 2) atmospheric
effects; and 3) the curse of dimensionality. In the context of su-
pervised classiﬁcation, one of the main difﬁculties is related to
the small ratio between the number of available training samples
and the number of features. This makes it impossible to obtain
reasonable estimates of the class-conditional hyperdimensional
probability density functions used in standard statistical classi-
ﬁers. As a consequence, on increasing the number of features
given as input to the classiﬁer over a given threshold (which
depends on the number of training samples and the kind of clas-
siﬁer adopted), the classiﬁcation accuracy decreases (this be-
havior is known as the Hughes phenomenon [1]).
Much work has been carried out in the literature to over-
come this methodological issue. Four main approaches can be
identiﬁed: 1) regularization of the sample covariance matrix; 2)
adaptive statistics estimation by the exploitation of the classi-
ﬁed (semilabeled) samples; 3) preprocessing techniques based
on feature selection/extraction, aimed at reducing/transforming
the original feature space into another space of a lower dimen-
sionality; and 4) analysis of the spectral signatures to model the
classes.
The ﬁrst approach uses the multivariate normal (Gaussian)
probability density model, which is a widely accepted statistical
model for optically remotely sensed data. For each information
class, such a model requires the correct estimation of ﬁrst- and
second-order statistics. In the presence of an unfavorable ratio
between the number of available training samples and features,
the common way of estimating the covariance matrix may lead
to inaccurate estimations (that may make it impossible to invert
the covariance matrix in maximum-likelihood (ML) classiﬁers).
Several alternatives and improved covariance matrix estimators
have been proposed to reduce the variance of the estimate for
limited training samples [2], [3]. The main problem involved by
improved covariance estimators is the risk that the estimated co-
variance matrices overﬁt the few available training samples and
lead to a poor approximation of statistics for the whole image
to be classiﬁed.
The second approach to overcome the Hughes phenomenon
proposes to use in an iterative way the semilabeled samples ob-
tained after classiﬁcation in order to enhance statistics estima-
tion and to improve classiﬁcation accuracy. Samples are initially
0196-2892/04$20.00 © 2004 IEEE

----- Page 2 (native) -----
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
1779
classiﬁed by using the available training samples. Then, the clas-
siﬁed samples, together with the training ones, are exploited it-
eratively to update the class statistics and, accordingly, the re-
sults of the classiﬁcation up to convergence [4], [5]. The process
of integration between these two typologies of samples (i.e.,
the training and the semilabeled samples) is carried out by the
expectation–maximization (EM) algorithm, which represents a
general and powerful solution to the problem of ML estimation
of statistics in the presence of incomplete data [6], [7]. The main
advantage of this approach is that it ﬁts the true class distribu-
tions better, since a larger portion of the image (available with no
extra cost) contributes to the estimation process. The main prob-
lems related to this second approach are two: 1) it is demanding
from the computational point of view and 2) it requires that the
initial class model estimated from the training samples should
match well enough the unlabeled samples in order to avoid di-
vergence of the estimation process and, accordingly, to improve
the accuracy of the model parameter estimation.
In order to overcome the problem of the curse of dimension-
ality, the third approach proposes to reduce the dimensionality
of the feature space by means of feature selection or extraction
techniques. Feature-selection techniques perform a reduction of
spectral channels by selecting a representative subset of original
features. This can be done following: 1) a selection criterion and
2) a search strategy. The former aims at assessing the discrim-
ination capabilities of a given subset of features according to
statistical distance measures among classes (e.g., Bhattacharyya
distance, Jeffries–Matusita distance, and the transformed diver-
gence measure [8], [9]). The latter plays a crucial role in hyper-
dimensional spaces, since it deﬁnes the optimization approach
necessary to identify the best (or a good) subset of features ac-
cording to the used selection criterion. Since the identiﬁcation of
the optimal solution is computationally unfeasible, techniques
that lead to suboptimal solutions are normally used. Among
the search strategies proposed in the literature, it is worth men-
tioning the basic sequential forward selection (SFS) [10], the
more effective sequential forward ﬂoating selection [11], and
the steepest ascent (SA) techniques [12]. The feature-extraction
approach addresses the problem of feature reduction by trans-
forming the original feature space into a space of a lower di-
mensionality, which contains most of the original information.
In this context, the decision boundary feature extraction (DBFE)
method [13] has proved to be a very effective method, capable
of providing a minimum number of transformed features that
achieve good classiﬁcation accuracy. However, this feature-ex-
traction technique suffers from high computational complexity,
which makes it often unpractical. This problem can be over-
come by coupling with the projection pursuit (PP) algorithm
[14], which plays the role of a preprocessor to the DBFE by
applying a preliminary limited reduction of the feature space
with (hopefully) an almost negligible information loss. An al-
ternative feature-extraction method, whose class-speciﬁc nature
makes it particularly attractive, was proposed by Kumar et al.
[15]. It is based on a combination of subsets of (highly corre-
lated) adjacent bands into fewer features by means of top-down
and bottom-up algorithms. In general, it is evident that even if
feature-reduction techniques take care of limiting the loss of in-
formation, this loss is often unavoidable and may have a nega-
tive impact on classiﬁcation accuracy.
Finally, the approach inherited from spectroscopic methods
in analytical chemistry to deal with hyperspectral data is worth
mentioning. The idea behind this approach is that of looking
at the response from each pixel in the hyperspectral image as
a one-dimensional spectral signal (signature). Each information
class is modeled by some descriptors of the shape of its spectra
[16], [17]. The merit of this approach is that it signiﬁcantly sim-
pliﬁes the formulation of the hyperspectral data classiﬁcation
problem. However, additional work is required to ﬁnd out appro-
priate shape descriptors capable of capturing the spectral shape
variability related to each information class accurately.
Other methods also exist that are not included in the group
of the four main approaches discussed above. In particular, it is
interesting to mention the method based on the combination of
different classiﬁers [18] and that based on cluster-space repre-
sentation [19].
Recently, particular attention has been dedicated to support
vector machines (SVMs) for the classiﬁcation of multispectral
remote sensing images [20]–[22]. SVMs have often been found
to provide higher classiﬁcation accuracies than other widely
used pattern recognition techniques, such as the maximum
likelihood and the multilayer perceptron neural network classi-
ﬁers. Furthermore, SVMs appear to be especially advantageous
in the presence of heterogeneous classes for which only few
training samples are available. In the context of hyperspectral
image classiﬁcation, some pioneering experimental investiga-
tions preliminarily pointed out the effectiveness of SVMs to
analyze hyperspectral data directly in the hyperdimensional
feature space, without the need of any feature-reduction pro-
cedure [23]–[26]. In particular, in [24], the authors found
that a signiﬁcant improvement of classiﬁcation accuracy can
be obtained by SVMs with respect to the results achieved
by the basic minimal-distance-to-means classiﬁer and those
reported in [3]. In order to show its relatively low sensitivity
to the number of training samples, the accuracy of the SVM
classiﬁer was estimated on the basis of different proportions
between the number of training and test samples. As will be
explained in the following section, this mainly depends on the
fact that SVMs implement a classiﬁcation strategy that exploits
a margin-based “geometrical” criterion rather than a purely
“statistical” criterion. In other words, SVMs do not require
an estimation of the statistical distributions of classes to carry
out the classiﬁcation task, but they deﬁne the classiﬁcation
model by exploiting the concept of margin maximization. The
growing interest in SVMs [27]–[30] is conﬁrmed by their suc-
cessful implementation in numerous other pattern recognition
applications such as biomedical imaging [31], image compres-
sion [32], and three-dimensional object recognition [33]. Such
an interest is justiﬁed by three main general reasons: 1) their
intrinsic effectiveness with respect to traditional classiﬁers,
which results in high classiﬁcation accuracies and very good
generalization capabilities; 2) the limited effort required for
architecture design (i.e., they involve few control parameters);
and 3) the possibility of solving the learning problem according
to linearly constrained quadratic programming (QP) methods
(which have been studied intensely in the scientiﬁc literature).
However, a major drawback of SVMs is that, from a theoretical
point of view, they were originally developed to solve binary

----- Page 3 (native) -----
1780
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
classiﬁcation problems. This drawback becomes even more
evident when dealing with data acquired from hyperspectral
sensors, since they are intrinsically designed to discriminate
among a broad range of land-cover classes that may be very
similar from a spectral viewpoint. The implementation of
SVMs in multiclass classiﬁcation problems can be approached
in two ways [23], [24], [34], [35]. The ﬁrst consists of deﬁning
an architecture made up of an ensemble of binary classiﬁers.
The decision is then taken by combining the partial decisions of
the single members of the ensemble. The second is represented
by SVMs formulated directly as a multiclass optimization
problem. Because of the number of classes that are to be
discriminated simultaneously, the number of parameters to
be estimated increases considerably in a multiclass optimiza-
tion formulation. This renders the method less stable and,
accordingly, affects the classiﬁcation performances in terms
of accuracy. For this reason, multiclass optimization has not
been as successful as the approach based on the two-class
optimization.
In this paper, we present a theoretical discussion and an ac-
curate experimental analysis that aim: 1) at assessing the prop-
erties of SVM classiﬁers in hyperdimensional feature spaces
and 2) at evaluating the impact of the multiclass problem in-
volved by SVM classiﬁers when applied to hyperspectral data
by comparing different multiclass strategies. With regard to the
experimental part of the ﬁrst objective, assessment of SVM ef-
fectiveness is carried out through two different experiments. In
the ﬁrst, we propose to compare the performances of SVMs with
those of two other nonparametric classiﬁers applied directly to
the original hyperdimensional feature space: the radial basis
function neural network, which is another kernel-based classi-
ﬁcation method (like SVMs) that uses a different classiﬁcation
strategy based on a “statistical” rather than a “geometrical” cri-
terion; and the K-nearest neighbors classiﬁer, which is widely
used in pattern recognition as a reference classiﬁcation method.
The second experiment consists of a comparison of SVMs with
the classical classiﬁcation approach adopted for hyperspectral
data, i.e., a conventional classiﬁer combined with a feature-re-
duction technique. This also allows to assess the performances
of SVMs in hypersubspaces of various dimensionalities. As re-
gards the second objective of this work, four different multiclass
strategies are analyzed and compared. In particular, the widely
used one-against-all and one-against-one strategies are consid-
ered. In addition, two strategies based on the hierarchical tree
approach are investigated. The experimental studies were car-
ried out on the basis of hyperspectral images acquired by the
Airborne Visible/Infrared Imaging Spectroradiometer (AVIRIS)
sensor in June 1992 on the Indian Pines area (Indiana) [36]. Dif-
ferent performance indicators are used to support our experi-
mental analysis, namely, the classiﬁcation accuracy, the com-
putational time, the stability to parameter setting, and the com-
plexity of the multiclass architecture adopted. Experimental re-
sults conﬁrm the signiﬁcant superiority of the SVM classiﬁers
in the context of hyperspectral data classiﬁcation over the con-
ventional classiﬁcation methodologies, whatever the multiclass
strategy adopted to face the multiclass dilemma.
The rest of this paper is organized in four sections. Section II
recalls the mathematical formulation of SVMs and discusses
their potential properties in hyperspectral feature spaces. Sec-
tion III describes different strategies that can be used to solve
multiclass problems with binary SVMs and that are adopted in
the experiments to assess the impact of the multiclass problem
in a hyperdimensional context. Section IV deals with the exper-
imental phase of the work. Finally, Section V summarizes the
observations and concluding remarks to complete this paper.
II. SVM CLASSIFICATION APPROACH
A. SVM Mathematical Formulation
1) Linear SVM: Linearly Separable Case: Let us consider a
supervised binary classiﬁcation problem. Let us assume that the
training set consists of
vectors from the
-dimensional fea-
ture space
. A target
is associated to each vector
. Let us assume that the two classes
are linearly separable. This means that it is possible to ﬁnd at
least one hyperplane (linear surface) deﬁned by a vector
(normal to the hyperplane) and a bias
that can separate
the two classes without errors. The membership decision rule
can be based on the function sgn
, where
is the dis-
criminant function associated with the hyperplane and deﬁned
as
(1)
In order to ﬁnd such a hyperplane, one should estimate
and
so that
with
(2)
The SVM approach consists in ﬁnding the optimal hyperplane
that maximizes the distance between the closest training sample
and the separating hyperplane. It is possible to express this dis-
tance as equal to
with a simple rescaling of the hyper-
plane parameters
and
such that
(3)
The geometrical margin between the two classes is given by the
quantity
. The concept of margin is central in the SVM
approach, since it is a measure of its generalization capability.
The larger the margin, the higher the expected generalization
[27].
Accordingly, it turns out that the optimal hyperplane can be
determined as the solution of the following convex quadratic
programming problem:
minimize:
subject to:
(4)
This classical linearly constrained optimization problem can be
translated (using a Lagrangian formulation) into the following
dual problem:
maximize:
subject to:
and
(5)

----- Page 4 (native) -----
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
1781
Fig. 1.
Optimal separating hyperplane in SVMs for a linearly nonseparable case. White and black circles refer to the classes “+1” and “ 1,” respectively. Support
vectors are indicated by an extra circle.
The Lagrange multipliers
’s
expressed
in (5) can be estimated using quadratic programming (QP)
methods [27]. The discriminant function associated with the
optimal hyperplane becomes an equation depending both on
the Lagrange multipliers and on the training samples, i.e.,
(6)
where
is the subset of training samples corresponding to the
nonzero Lagrange multipliers
’s. It is worth noting that the
Lagrange multipliers effectively weight each training sample
according to its importance in determining the discriminant
function. The training samples associated to nonzero weights
are called support vectors. These lie at a distance exactly equal
to
from the optimal separating hyperplane.
2) Linear SVM: Linearly Nonseparable Case: The SVM
formulation described in the previous subsection holds only
if data are linearly separable. Such an optimistic condition is
difﬁcult to satisfy in the classiﬁcation of real data. In order to
handle nonseparable data, the concept of optimal separating
hyperplane has been generalized as the solution that minimizes
a cost function that expresses a combination of two criteria:
margin maximization (as in the case of linearly separable data)
and error minimization (to penalize the wrongly classiﬁed
samples). The new cost function is deﬁned as
C
(7)
where the
’s are the so-called slack variables introduced to
account for the nonseparability of data, and the constant C
represents a regularization parameter that allows to control the
penalty assigned to errors. The larger the C value, the higher the
penalty associated to misclassiﬁed samples. The minimization
of the cost function expressed in (7) is subject to the following
constraints:
(8)
(9)
It is worth noting that, in the nonseparable case, two kinds of
support vectors coexist: 1) margin support vectors that lie on
the hyperplane margin and 2) nonmargin support vectors that
fall on the “wrong” side of this margin (Fig. 1).
3) Nonlinear SVM: Kernel Method: A natural way to im-
prove further the separation between two information classes
consists in generalizing the above method to the category of
nonlinear discriminant functions. Accordingly, one may think
of mapping the data through a proper nonlinear transformation
into a higher dimensional feature space
, where a separation between the two classes can be looked
for following the method described in the previous subsections,
i.e., by means of an optimal hyperplane deﬁned by a normal
vector
and a bias
. To identify the latter, one
should solve a dual problem such as the one deﬁned in (5) for
the linearly separable case by replacing the inner products in
the original space
with inner products in the trans-
formed space
. At this point, the main problem
consists of the explicit computation of
, which can prove
expensive and at times unfeasible. The kernel method provides
an elegant and effective way of dealing with this problem. Let
us consider a kernel function that satisﬁes the condition stated
in Mercer’s theorem so as to correspond to some type of inner
product in the transformed (higher) dimensional feature space
[27, pp. 423–424], i.e.,
(10)
This kind of kernel function allows to simplify the solution of
the dual problem considerably, since it avoids the computation

----- Page 5 (native) -----
1782
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
of the inner products in the transformed space
,
i.e., as in
maximize:
subject to:
and
C
(11)
The ﬁnal result is a discriminant function
conveniently ex-
pressed as a function of the data in the original (lower) dimen-
sional feature space
(12)
The shape of the discriminant function depends on the kind of
kernel functions adopted. A common example of kernel type
that fulﬁlls Mercer’s condition is the Gaussian radial basis func-
tion
(13)
where
is a parameter inversely proportional to the width of the
Gaussian kernel. Another extensively used kernel is the polyno-
mial function of order
expressed as
(14)
It is worth underlining that the kernel-based implementation of
SVMs involves the problem of the selection of multiple param-
eters, including the kernel parameters (e.g., the
and
parame-
ters for the Gaussian and polynomial kernels, respectively) and
the regularization parameter C. Recently, two interesting auto-
matic techniques have been developed to deal with this issue
[37], [38]. They are based on the idea of estimating the pa-
rameter values so that: 1) they maximize the margin; and 2)
they minimize the estimate of the expected generalization error.
The latter is expressed in analytical form by the well-known
leave-one-out (LOO) procedure. Optimization of the parame-
ters is then carried out using a gradient descent search over the
space of the parameters.
Since a detailed analysis of the theory of SVMs is beyond the
scope of this paper, we refer the reader to [27]–[30] for greater
detail on SVMs.
B. SVMs in Hyperspectral Feature Spaces
Unlike traditional learning techniques, SVMs do not depend
explicitly on the dimensionality of input spaces. They solve
classical statistical problems such as pattern recognition, regres-
sion, and density estimation in high-dimensional spaces [27]. In
greater detail, as stated in the previous subsection, the input fea-
ture space is mapped by a kernel transformation into a higher
dimensional space, where it is expected to ﬁnd a linear sepa-
ration that maximizes the margin between the two classes. In
order to appreciate the potentialities of SVMs in high-dimen-
sional spaces, it is useful to recall the statistical and geometrical
properties of the data in such spaces.
First, in a hyperspectral space, normally distributed samples
(a reasonable assumption for optically remotely sensed data)
tend to fall toward the tails of the density function with virtu-
ally no samples falling in the central region [39]. This can be
illustrated by a simple geometric example [40]. Let us consider
the ratio
between the volume of a sphere of radius
and
one of a cube deﬁned in the interval
in the
-dimen-
sional space. It is equal to
(15)
where
represents the well-known gamma function. From
(15), it easy to show that the higher the dimensionality of the
space, the lower the volume ratio. Accordingly, the volume of a
hypercube is almost concentrated in its corners. In other words,
turning back to our classiﬁcation problem, the increase in di-
mensionality makes the space almost empty and results in a
“centrifuge” effect such that data have a tendency to concentrate
close to the tails of the distribution where they are very likely
to be in proximity of decision boundaries between the informa-
tion classes. This statistical property is of interest potentially to
pattern recognition approaches, such as SVMs, that deﬁne dis-
criminant functions on the basis of samples situated near the
decision boundaries, since the presence of a larger number of
samples in this region allows to generate more accurate and re-
liable discriminant functions.
In the second place, it is well-known that as the dimension-
ality of the data increases, the distances between the samples
(and consequently between the information classes) increase
[41]. In this situation, local neighborhoods are almost certainly
empty, requiring the bandwidth of estimation to be large and
producing the effect of losing accuracy in density estimation for
a statistical classiﬁer [39]. On the contrary, the “geometrical”
nature of SVMs results in a methodology that is not aimed at
estimating the statistical distributions of classes over the entire
hyperdimensional space. Indeed, SVMs are inspired by the fol-
lowing idea:
If you possess a limited amount of information to solve
a problem, try solving it directly and never solve a more
general problem as an intermediate step. The available in-
formation may be sufﬁcient for a direct solution, though
insufﬁcient to solve a more general intermediate problem.
[27, p. 12]
In other words, SVMs do not involve a density estimation
problem that can lead to the Hughes effect, but they directly
exploit the geometrical behavior of data (space local empti-
ness) as they make it more likely to ﬁnd a decision boundary
between classes that results in a small classiﬁcation error. The
above-discussed properties (statistical and geometrical) render
SVMs potentially less sensitive to the curse of dimensionality.
Another important aspect to be pointed out is the intrinsic
good generalization capability of SVMs, which stems from
the selection of the hyperplane that maximizes the geometrical
margin between classes. In a hyperspectral context, the max-
imum margin solution allows to fully exploit the discrimination
capability of the relatively few training samples available.

----- Page 6 (native) -----
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
1783
Accordingly, this solution deals with some of the major prob-
lems, such as the large spatial variability of the hyperspectral
signature of each information class, in the best way in terms
of generalization capability, given the limited information
present in the training set. However, it is worth noting that to
solve the problem of the spatial variability of the hyperspectral
signature of classes effectively, good generalization properties
of the classiﬁers should be coupled with other data analysis
techniques.
III. SVMS: MULTICLASS STRATEGIES
As stated in the previous section, SVMs are intrinsically
binary classiﬁers. However, the classiﬁcation of hyperspec-
tral remote sensing data usually involves the simultaneous
discrimination of numerous information classes. In this sec-
tion, we describe four different strategies of combination of
SVMs considered to evaluate the impact of the multiclass
problem in the context of hyperspectral data classiﬁcation. Let
be the set of
possible labels (informa-
tion classes) associated with the
-dimensional hyperspectral
image
of the study area. In the multiclass case, the problem is
to associate to each -dimensional sample
the label of the set
that optimizes a predeﬁned classiﬁcation criterion. In order
to carry out this task, the general approach adopted in strategies
based on binary classiﬁers consists of: 1) deﬁning an ensemble
of binary classiﬁers; and 2) combining them according to some
decision rules.
The deﬁnition of the ensemble of binary classiﬁers involves
the deﬁnition of a set of two-class problems, each modeled with
two groups
and
of classes (
and
).
Targets with values
and
are assigned to the samples of
and
, respectively, for each SVM. The selection of these
subsets depends on the kind of approach adopted to combine the
ensemble. Two main approaches can be identiﬁed: the “parallel”
and the “hierarchical tree-based” approaches. In the following,
we describe two multiclass strategies from each approach char-
acterized by different classiﬁcation complexity and computa-
tional cost properties.
A. Parallel Approach
1) One-Against-All Strategy: The one-against-all (OAA)
strategy represents the earliest and most common multiclass
approach used for SVMs [42]. It involves a parallel architecture
made up of
SVMs, one for each class (Fig. 2). Each SVM
solves a two-class problem deﬁned by one information class
(e.g.,
) against all the others, i.e.,
(16)
The “winner-takes-all” rule is used for the ﬁnal decision, i.e.,
the winning class is the one corresponding to the SVM with the
highest output (discriminant function value).
2) One-Against-One Strategy: The main problem of the
OAA strategy is that the discrimination between an information
class and all the others often leads to the estimation of complex
discriminant functions. In addition, a problem with strongly
unbalanced prior probabilities should be solved by each SVM.
The idea behind the one-against-one (OAO) strategy is that
Fig. 2.
Block diagram of a parallel architecture for solving multiclass
problems with binary SVMs. In the OAA strategy, M is equal to T (i.e., the
number of information classes). By contrast, the OAO strategy involves a larger
number of SVMs and M is given by T(T   1)=2.
of a different reasoning, in which simple classiﬁcation tasks
are made possible thanks to a parallel architecture made up
of a large number of SVMs [23], [43]. The OAO strategy
involves
SVMs, which model all possible pair-
wise classiﬁcations. In this case, each SVM carries out a
binary classiﬁcation in which two information classes
and
are analyzed against each other
by means of a discriminant function
. Consequently, the
grouping becomes
(17)
Before the decision process, it is necessary to compute for each
class
a score function
, which sums the favorable
and unfavorable votes expressed for the considered class
sgn
(18)
The ﬁnal decision in the OAO strategy is taken on the basis of
the “winner-takes-all” rule, which corresponds to the following
maximization
(19)
Sometimes, conﬂict situations may occur between two different
classes characterized by the same score. Such ambiguities
can be solved by selecting the class with the highest prior
probability.
B. Hierarchical Tree-Based Approach
The idea of representing the data analysis process with a hier-
archical tree is not new and has been under study in many pattern
recognition application areas. Tree-based classiﬁers have repre-
sented an interesting and effective way to structure and solve
complex classiﬁcation problems [44]–[47]. The organization of
information into a hierarchical tree allows to achieve a faster
processing capability and, at times, a higher accuracy of anal-
ysis. This is mainly explained by the fact that the nodes of the
tree carry out very focused tasks, meaningless when taken indi-
vidually but meaningful when taken as a whole. Turning back

----- Page 7 (native) -----
1784
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
to our problem, the binary hierarchical tree (BHT) can be seen
as an alternative to the OAA and the OAO strategies, since it
allows to reach a good tradeoff between the number of SVMs
to be used and the complexity of the task assigned to each of
them. Furthermore, the BHT does not implement a global deci-
sion scheme after evaluating the local decisions as in the OAA
and OAO strategies. Indeed, the ﬁnal decision is implicitly made
after running through the tree and reaching one of its terminal
nodes.
Many BHT strategies have been proposed in the literature. In
this paper, we investigate two different binary tree hierarchies
aimed at reducing the computational load required by the OAA
and OAO strategies, especially in the operational classiﬁcation
phase (the off-line training phase is less critical from the view-
point of the computational time). This can become particularly
important when large hyperspectral images are considered. As
described in the following, both trees exploit the prior proba-
bilities of the classes to deﬁne the hierarchy of binary SVMs.
It is worth noting that alternative strategies that also exploit the
underlying afﬁnities among the individual classes to deﬁne the
binary trees (like in [46]) could be considered.
1) BHT-Balanced Branches Strategy: In the BHT-balanced
branches (BHT-BB) strategy, the tree is deﬁned in such a way
that each node (SVM) discriminates between two groups of
classes
and
with similar cumulative prior probabilities.
Fig. 3(a) shows an example of tree that can be found with the
BHT-BB strategy for a general
-class classiﬁcation problem.
The algorithm that implements the BHT-BB strategy is de-
scribed as follows:
Step 0: Root Node
—Set level index
—Divide
into two groups
and
such that
Step 1:k-Level Branching
—For
• If Card
, divide
into two groups
and
such that
• If Card
, divide
into two groups
and
such that
—Set
Step 2: Stop Condition
—If
or
such that Card
or
Card
with
, go to
Step 1. Otherwise, Stop.
2) BHT-One Against All Strategy: The second binary tree-
based hierarchy, called BHT-one against all (BHT-OAA), rep-
resents a simpliﬁcation of the OAA strategy obtained through its
implementation in a hierarchical context. To this end, we pro-
pose to deﬁne the tree in such a way that each node discrim-
inates between two groups of classes
and
, where
represents the information class with the highest prior proba-
bility among those belonging to
. This kind of hier-
archy leads to a tree with only one single branch as depicted in
Fig. 3.
Examples of BHTs for a T-class classiﬁcation problem. (a) BHT-BB.
(b) BHT-OAA.
TABLE I
NUMBER OF TRAINING AND TEST SAMPLES USED IN THE EXPERIMENTS
Fig. 3(b). The algorithm of the BHT-OAA strategy is drawn up
in the following:
Step 0: Root Node
—Set level index
—Divide
into two groups
and
such that
and
Step 1: k-Level Branching
—Divide
into two groups
and
such
that
and
—Set
Step 2: Stop Condition
—If Card
, go to Step 1. Otherwise, Stop.
It is worth noting that both BHT strategies allow to reduce the
number of required SVMs from
and
, respectively,
for the OAA and OAO strategies, to
. Since the classiﬁ-
cation time depends linearly on the number of SVMs and since

----- Page 8 (native) -----
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
1785
TABLE II
BEST OVERALL AND CLASS-BY-CLASS ACCURACIES, AND COMPUTATIONAL TIMES ACHIEVED ON THE TEST SET
BY THE DIFFERENT CLASSIFIERS IN THE ORIGINAL HYPERSPECTRAL SPACE
classiﬁcation tasks of medium complexity are assigned to the
SVMs of the tree, we expect a lower classiﬁcation time required
by the two BHT-based strategies with respect to both the OAO
and, especially, the standard OAA strategies.
IV. EXPERIMENTAL RESULTS
A. Dataset Description and Experiment Design
The hyperspectral dataset used in our experiments is a sec-
tion of a scene taken over northwest Indiana’s Indian Pines by
the AVIRIS sensor in 1992 [36]. From the 220 spectral channels
acquired by the AVIRIS sensor, 20 channels were discarded be-
cause affected by atmospheric problems. From the 16 different
land-cover classes available in the original ground truth, seven
were discarded, since only few training samples were available
for them (this makes the experimental analysis more signiﬁcant
from the statistical viewpoint). The remaining nine land-cover
classes were used to generate a set of 4757 training samples
(used for learning the classiﬁers) and a set of 4588 test sam-
ples (exploited for assessing their accuracies) (see Table I). The
experiments were run on a Sun Ultra 80 workstation.
The experimental analysis was organized into three main
experiments. The ﬁrst aims at analyzing the effectiveness
of SVMs in classifying hyperspectral images directly in the
original hyperdimensional feature space. A comparison with
two other nonparametric classiﬁers is provided as well as an
assessment of the stability of these three classiﬁcation methods
versus the setting of their parameters. In the second experiment,
SVMs are compared with the classical approach adopted for
hyperspectral data classiﬁcation, that is a conventional pat-
tern recognition system made up of a classiﬁcation method
combined with a feature-reduction technique. In these two
experiments, we adopted the most popular multiclass strategy
used for SVMs, that is the OAA strategy. Finally, the third
experiment aims at analyzing and comparing the effectiveness
of the different multiclass strategies described in the previous
section, that is the OAA, OAO, BHT-BB, and BHT-OAA
strategies.
B. Results of Experiment 1: Classiﬁcation in the Original
Hyperdimensional Feature Space
SVMs were compared with two widely used nonparametric
classiﬁers: a radial basis functions (RBFs) neural network
trained with the technique described in [48] and a conventional
K-nearest neighbors (K-nn) classiﬁer. The choice of the RBF
classiﬁer is motivated by the fact that it is a kernel-based
TABLE III
ANALYSIS OF THE STABILITY OF THE OVERALL CLASSIFICATION ACCURACY
AND OF THE COMPUTATIONAL TIME VERSUS THE SETTING OF THE
PARAMETERS OF THE DIFFERENT CLASSIFIERS
method (like SVMs), which adopts a different strategy based
on a “statistical” (rather than a “geometrical”) criterion for
deﬁning the discriminant hyperplane in the transformed kernel
space. The K-nn classiﬁer was considered in our experiments,
since it represents a reference classiﬁcation method in pattern
recognition. However, it is worth noting that we expect it to be
sensitive to the curse of dimensionality. For both classiﬁers,
different trials were carried out to determine empirically the
best related parameters, namely, the number of nodes in the
hidden layer and the variable
, respectively.
In the experiments, we considered two different kinds of
SVMs: a linear SVM (SVM-Linear) which corresponds to an
SVM without kernel transformation, and a nonlinear SVM
based on Gaussian radial basis kernel functions (SVM-RBF).
For both SVMs, the regularization parameter C must be es-
timated, since data are not ideally separable. In addition, the
nonlinear SVM requires the determination of the width pa-
rameter
of the Gaussian radial basis kernels, which tunes
the smoothing of the discriminant function. For the considered
dataset, the best values of the parameter C were 50 and 40
for the linear and nonlinear SVMs, respectively. The optimal
kernel width parameter
of the nonlinear SVM was found
equal to 0.25. These values were estimated empirically on the
basis of the available training samples.
The results in terms of classiﬁcation accuracy and computa-
tional time provided by the different classiﬁers are summarized
in Table II. The nonlinear SVM exhibited the best Overall
Accuracy (OA), i.e., the best percentage of correctly classiﬁed
pixels among all the test pixels considered, with a gain of
6.32%, 6.43%, and 9.48% over the linear SVM, the RBF, and
the K-nn classiﬁers, respectively. In terms of class accuracies,
the “corn-min till” class
was the most critical. For this
class, the nonlinear SVM still exhibited the best accuracy
(87.76%), whereas the worst accuracy (61.16%) was obtained
by the K-nn classiﬁer. It is worth noting that, since the K-nn

----- Page 9 (native) -----
1786
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
classiﬁer is based on counting the number of nearest neigh-
boring training samples, it requires the feature space to be ﬁlled
in with a signiﬁcant number of training samples to obtain reli-
able local estimates of the conditional posterior probabilities of
classes. However, in the considered dataset, the small number
of training samples (4757) is not sufﬁcient to ﬁll in a proper
way the emptiness of the hyperdimensional feature space. This
explains the relatively poor classiﬁcation accuracies of the K-nn
classiﬁer. By contrast, SVMs exploit a discriminant model that
is deﬁned on the basis of a particular portion of the training
samples (support vectors). As explained in Section II and
conﬁrmed by the obtained results, the behavior of the class dis-
tributions in hyperdimensional spaces makes it more effective
to apply techniques that deﬁne discriminant functions on the
basis of training samples located near the decision boundaries.
Concerning computational cost, the nonlinear SVM exhibited
a reasonable total computational time (given by the sum of the
training and test times) compared to the other three classiﬁers.
It is worth noting that the long computational time required by
the linear SVM (40342 [s]) expresses the difﬁculties encoun-
tered by this kind of classiﬁer in the training phase to ﬁnd a
reasonable linear separation between information classes.
In order to assess the robustness of each classiﬁer to the pa-
rameter settings, we derived some statistics by looking at the
overall accuracy (OA) and at the total computational time as
random realizations obtained by varying the parameters in a pre-
deﬁned range of values. The results reported in Table III conﬁrm
the superiority of the nonlinear SVM in terms of both mean
overall accuracy (92.64% and 92.51% by varying the param-
eters
and C, respectively) and in terms of stability (it pro-
vided the lowest variances). It is worth noting that the nonlinear
SVM is less sensitive to the choice of the kernel width value
than to the regularization parameter C. The linear SVM showed
the worst stability to the parameter C (overall-accuracy variance
equals 4.94). This is explained by the fact that a linear separa-
tion between classes involves a large number of error samples,
which lie on the wrong side of the separating hyperplane. This
makes it more difﬁcult to apply the regularization mechanism
implemented in the SVM formulation, resulting in signiﬁcant
sensitivity of the classiﬁcation accuracy to the value of the reg-
ularization parameter. Concerning the average total computa-
tional times, the obtained results conﬁrm the conclusions drawn
above on the basis of the total computational times obtained for
the best parameter values of the four considered classiﬁers.
C. Results of Experiment 2: Feature Reduction and
Classiﬁcation
As already discussed in Section I, the traditional approach
adopted to address the problem of the classiﬁcation of hyper-
spectral data consists of two main phases: 1) reducing the di-
mensionality of the feature space; and 2) applying the resulting
subset of features to a conventional classiﬁer. In this experiment,
we propose to assess the effectiveness of SVMs with respect
to a traditional feature-reduction-based approach and to eval-
uate their performances in hypersubspaces of various dimen-
sionalities. To this end, we used the Jeffries–Matusita (JM) in-
terclass distance measure [8] and the steepest ascent (SA) search
strategy [12] to reduce the original hyperdimensional space into
Fig. 4.
Overall accuracy versus the number of features obtained on the test set
by the four different classiﬁers considered in our investigation (i.e., linear and
nonlinear SVMs, RBF and K-nn classiﬁers).
TABLE IV
FIRST- AND SECOND-ORDER STATISTICS OF THE OVERALL ACCURACIES
OBTAINED ON THE TEST SET BY THE DIFFERENT CLASSIFIERS COMBINED
WITH THE SA-BASED FEATURE-SELECTION PROCEDURE FOR A NUMBER
OF FEATURES VARYING FROM 20 TO 200 (WITH A STEP OF 10)
spaces of a lower dimensionality (the number of features was
varied from 20 to 200 with a step of 10). The SA technique
formulates the problem of deﬁning the subset of features that
maximizes the JM distance as a discrete optimization problem
in a -dimensional space, which is viewed as a space of binary
strings. It starts with a binary string randomly initialized, and
performs an iterative local optimization of the adopted criterion
function. At each iteration, the criterion is maximized over a
neighborhood of the current solution under a predeﬁned con-
straint. In our experiments, each subset of selected features was
given as input to all four considered classiﬁers (i.e., linear and
nonlinear SVMs, RBF neural networks, and the K-nn classiﬁer).
Fig. 4 plots the overall accuracy versus the number of selected
features for the four considered classiﬁers. As can be seen, the
obtained results still conﬁrm the strong superiority of nonlinear
SVMs over the other classiﬁers even in lower dimensional fea-
ture spaces, with a gain in overall accuracy (averaged over all the
subsets of features) of
%
%, and
% with re-
spect to the linear SVM, the K-nn, and the RBF neural network
classiﬁers (see Table IV). In order to analyze the sensitivity of
each classiﬁer to the Hughes phenomenon, in the same table we
reported the variance of the overall accuracy exhibited by each
classiﬁcation method when varying the number of features from
20 to 200. The lowest sensitivity was again obtained by the non-
linear SVM classiﬁer with a sharp reduction of the variance with
respect to those achieved by the K-nn, the linear SVM, and the
RBF neural network classiﬁers.

----- Page 10 (native) -----
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
1787
TABLE V
CLASSIFICATION ACCURACIES YIELDED ON THE TEST SET BY THE DIFFERENT CLASSIFIERS WITH THE SUBSET OF THE BEST 30 FEATURES SELECTED ACCORDING
TO THE SA-BASED FEATURE-SELECTION PROCEDURE. THE DIFFERENCE IN OVERALL ACCURACY (DIFF-OA) FOR EACH CLASSIFIER WITH RESPECT TO THE
ACCURACY ACHIEVED IN THE ORIGINAL HYPERDIMENSIONAL SPACE IS ALSO GIVEN
Table V reports the overall and class-by-class accuracies ob-
tained for the hypersubspace made up of the best 30 selected
features. The choice of this subspace is motivated by the fact that
it represents a good compromise between a low dimensionality
of the feature space and a high classiﬁcation accuracy achieved
on average by the four classiﬁers. In particular, one can see the
greater capacity of the nonlinear SVMs to recognize each in-
formation class, with a gain in the average of the class-by-class
accuracies of
%
%, and
% with respect to
the linear SVM, the K-nn, and the RBF neural network classi-
ﬁers. In addition, the same table reports the difference in overall
accuracy (DIFF-OA) for each classiﬁer with respect to the accu-
racy achieved in the original hyperdimensional space. It is inter-
esting to note the lower difference (associated with the expected
lowest sensitivity to the problem of the curse of dimensionality)
achieved by the SVM-RBF classiﬁer (0.93%). The reduction in
the number of features involved a decrease in accuracy of 3.36%
for the linear SVM classiﬁer. By contrast, signiﬁcant increases
in accuracy of 2.96% and 3.46% were obtained by the conven-
tional K-nn and RBF classiﬁers, respectively, conﬁrming a rel-
atively high sensitivity to the curse of dimensionality.
In order to analyze the complexity of the decision bound-
aries produced by the nonlinear SVM classiﬁer, we computed
the number of SVs deﬁned in each binary SVM of the OAA
architecture in both the original hyperspace and the hypersub-
space consisting of the best 30 selected features. These numbers
are represented graphically in Fig. 5. It can be observed in gen-
eral that the numbers of SVs are relatively small, except for the
SVM associated with the class
. This suggests that decision
boundaries of moderate complexity were enough to discrimi-
nate accurately between the information classes. Furthermore,
as discussed in Section II-B, an important property related to
the “geometrical” nature of SVMs seems conﬁrmed, i.e., that
the classiﬁcation complexity does not depend on the dimension
of the feature space, since the number of SVs is almost similar
in both the original and the reduced spaces.
D. Results of Experiment 3: SVM and Multiclass Strategies
The third (and last) experiment addressed the application of
SVMs to the multiclass problem in the hyperdimensional space.
The different multiclass strategies described in Section III (i.e.,
the OAA, OAO, BHT-BB, and BHT-OAA strategies) were de-
signed and trained using nonlinear SVMs based on the Gaussian
radial basis kernel functions. The trees of SVMs deﬁned for the
BHT-BB and the BHT-OAA strategies are illustrated in Fig. 6.
The class prior probabilities necessary to obtain such trees were
computed on the basis of the training set. After the training
Fig. 5.
Number of support vectors that characterize each binary SVM of
the multiclass nonlinear SVM classiﬁer (OAA strategy) in both the original
hyperspace (d = 200) and the hypersubspace made up of the best 30 selected
features (d = 30).
phase, the four strategies were analyzed and compared based on
three parameters: 1) classiﬁcation accuracy; 2) computational
time; and 3) architecture complexity. The obtained results are re-
ported in Tables VI and VII. From the viewpoint of the accuracy,
all four strategies resulted in satisfactory results when compared
with the two other nonparametric classiﬁers (i.e., the RBF neural
networks and the K-nn classiﬁer). In greater detail, the OAO
strategy exhibited the best accuracy with a gain in overall ac-
curacy of
%
%, and
% over the BHT-OAA,
the BHT-BB and the OAA strategies, respectively. This sug-
gests that the decomposition of the multiclass problem into an
ensemble of two-class problems of very low-complexity repre-
sents an effective way of improving overall discrimination capa-
bility. The signiﬁcant reduction in the complexity of the classi-
ﬁcation problem assigned to each SVM of the OAO architecture
is shown by the very small average number of SVs that charac-
terizes each SVM of the same architecture. Indeed, this number
is 130 against 333, 334, and 424 for the BHT-OAA, BHT-BB
and the OAA strategies, respectively (Table VII). These values
explain also why the time required to train the SVMs of the OAO
strategy is the shortest, despite the greater amount of SVMs re-
quired by the same strategy (212 [s] against 311 [s], 410 [s],
and 2361 [s] to train the BHT-BB, BHT-OAA, and OAA strate-
gies, respectively). It is worth noting that the smallest number
of SVs was exhibited by the OAO strategy. Indeed, only nine
SVs were necessary to discriminate between the ﬁfth and ninth
classes (hay-windrowed and woods, respectively) with an accu-
racy of 100%. On the other hand, the larger number of SVMs
involved in the OAO strategy directly affects the computational
time demanded during the classiﬁcation of test samples (554
[s] against 125 [s], 155 [s] and 341 [s] for the BHT-BB, the

----- Page 11 (native) -----
1788
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
TABLE VI
OVERALL AND CLASS-BY-CLASS ACCURACIES OBTAINED ON THE TEST SET BY SVMS WITH THE DIFFERENT MULTICLASS STRATEGIES CONSIDERED
Fig. 6.
Hierarchical trees obtained on the considered dataset by (a) the
BHT-BB strategy and (b) the BHT-OAA strategy.
BHT-OAA, and the OAA strategies, respectively). Thanks to
the small number of required SVMs and to the moderate com-
plexity of the classiﬁcation tasks assigned to each of them, the
two BHT strategies seem particularly interesting in an oper-
ative phase involving the classiﬁcation of large scale images.
Table VIII shows the overall accuracies achieved by each SVM
TABLE VII
COMPUTATIONAL TIME AND CLASSIFICATION COMPLEXITY ASSOCIATED TO
THE DIFFERENT SVM MULTICLASS STRATEGIES CONSIDERED
involved in both the BHT-BB and the BHT-OAA strategies. It is
worth noting that the relatively low accuracy (93.77%) obtained
by the ﬁrst SVM of the BHT-OAA architecture (SVM1) com-
bined with a signiﬁcant depth of its associated tree (involving a
higher risk of error propagation) may explain why this strategy
was slightly less accurate than the BHT-BB strategy. In gen-
eral, from a computational point of view, the two investigated
BHT-BB and BHT-OAA strategies proved effective, resulting
in a signiﬁcant decrease in computational time.
V. DISCUSSION AND CONCLUSION
In this paper, we addressed the problem of the classiﬁcation
of hyperspectral remote sensing data using support vector ma-
chines. In order to assess the effectiveness of this promising
classiﬁcation methodology, we considered two main objectives.
The ﬁrst was aimed at assessing the properties of SVMs in hy-
perdimensional spaces and hypersubspaces of various dimen-
sionalities. In this context, the results obtained on the considered
dataset allow to identify the following three properties: 1) SVMs
are much more effective than other conventional nonparametric
classiﬁers (i.e., the RBF neural networks and the K-nn classiﬁer)
in terms of classiﬁcation accuracy, computational time, and sta-
bility to parameter setting; 2) SVMs seem more effective than
the traditional pattern recognition approach, which is based on
the combination of a feature extraction/selection procedure and
a conventional classiﬁer, as implemented in this paper; and 3)
SVMs exhibit low sensitivity to the Hughes phenomenon, re-
sulting in an excellent approach to avoid the usually time-con-
suming phase required by any feature-reduction method. In-
deed, as shown in the experiments, the improvement in accuracy
obtained on the considered dataset by combining SVMs with a
feature-reduction technique is deﬁnitely insufﬁcient to justify
the use of the latter.
The second objective of the work concerned the assessment
of the effectiveness of strategies based on ensembles of binary
SVMs used to solve multiclass problems in hyperspectral data.
In particular, four different multiclass strategies were investi-
gated and compared. These four strategies differ basically in

----- Page 12 (native) -----
MELGANI AND BRUZZONE: CLASSIFICATION OF HYPERSPECTRAL REMOTE SENSING IMAGES WITH SVMs
1789
TABLE VIII
OVERALL ACCURACY YIELDED ON THE TEST SET BY EACH SINGLE SVM OF THE BHT-BB AND BHT-OAA STRATEGIES
the manner in which the classiﬁcation problem complexity is
distributed over the single members (SVMs) of the architecture.
Compared with each other, the parallel architectures (OAA and
OAO) showed a better discrimination capability than the hierar-
chical tree-based architectures (BHT-BB and BHT-OAA). This
can be explained by the fact that the BHT strategies may involve
the risk of propagation of errors, since the ﬁnal decision is the
result of several hierarchical exchanges of partial decisions that
may accumulate errors. Accordingly, one may observe that the
design of a BHT strategy should favor a large number of ramiﬁ-
cations at the expense of a lower ramiﬁcation depth, to attenuate
such a risk. Another reason that justiﬁes the lower discrimina-
tion capability of the two proposed BHT strategies can be found
in the kind of information used to construct the tree. Indeed,
the use of simple information, such as the class prior probabil-
ities, cannot take into proper account the underlying afﬁnities
among individual classes (or metaclasses). However, from the
viewpoint of computational time, the BHT-BB and BHT-OAA
strategies proved the most effective. Consequently, depending
on the considered application, the multiclass strategy should be
selected according to a proper tradeoff between classiﬁcation
accuracy and computational time. As a ﬁnal remark, it is impor-
tant to point out that the classiﬁcation accuracies exhibited by
all four strategies suggest that the multiclass problem does not
signiﬁcantly affect the performances of SVMs in the analysis of
hyperspectral data. Indeed, all the strategies exhibited accura-
cies sharply higher than those of the nonparametric classiﬁers
considered in our experimental analysis.
ACKNOWLEDGMENT
Support by the Italian Ministry of Education, Research and
University (MIUR) for this work is gratefully acknowledged.
The authors would like to thank D. Landgrebe for providing
the AVIRIS data and T. Joachims for supplying the software
SVM
(http://svmlight.joachims.org/) used in the context of
this work.
REFERENCES
[1] G. F. Hughes, “On the mean accuracy of statistical pattern recognizers,”
IEEE Trans. Inform. Theory, vol. IT-14, pp. 55–63, 1968.
[2] J. P. Hoffbeck and D. A. Landgrebe, “Covariance matrix estimation and
classiﬁcation with limited training data,” IEEE Trans. Pattern Anal. Ma-
chine Intell., vol. 18, pp. 763–767, July 1996.
[3] S. Tadjudin and D. A. Landgrebe, “Covariance estimation with limited
training samples,” IEEE Trans. Geosci. Remote. Sensing, vol. 37, pp.
2113–2118, July 1999.
[4] Q. Jackson and D. A. Landgrebe, “An adaptive classiﬁer design for high-
dimensional data analysis with a limited training data set,” IEEE Trans.
Geosci. Remote. Sensing, vol. 39, pp. 2664–2679, Dec. 2001.
[5] B. M. Shahshahani and D. A. Landgrebe, “The effect of unlabeled
samples in reducing the small sample size problem and mitigating the
Hughes phenomenon,” IEEE Trans. Geosci. Remote. Sensing, vol. 32,
pp. 1087–1095, Sept. 1994.
[6] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
from incomplete data via the EM algorithm,” J. R. Statist. Soc., vol. 19,
pp. 1–38, 1977.
[7] T. K. Moon, “The expectation-maximization algorithm,” Signal Process.
Mag., vol. 13, pp. 47–60, 1996.
[8] J. A. Richards and X. Jia, Remote Sensing Digital Image Analysis,
Berlin, Germany: Springer-Verlag, 1999.
[9] L. Bruzzone, F. Roli, and S. B. Serpico, “An extension to multiclass
cases of the Jeffries–Matusita distance,” IEEE Trans. Geosci. Remote
Sensing, vol. 33, pp. 1318–1321, Nov. 1995.
[10] J. Kittler, “Feature set search algorithm,” in Pattern Recognition and
Signal Processing, C. H. Chen, Ed.
Alphen aan den Rijn, Netherlands:
Sijthoff and Noordhoff, 1978, pp. 41–60.
[11] P. Pudil, J. Novovicova, and J. Kittler, “Floating search methods in fea-
ture selection,” Pattern Recognit. Lett., vol. 15, pp. 1119–1125, 1994.
[12] S. B. Serpico and L. Bruzzone, “A new search algorithm for feature
selection in hyperspectral remote sensing images,” IEEE Trans. Geosci.
Remote Sensing, vol. 39, pp. 1360–1367, July 2001.
[13] C. Lee and D. A. Landgrebe, “Feature extraction based on decision
boundaries,” IEEE Trans. Pattern Anal. Machine Intell., vol. 15, pp.
388–400, Apr. 1993.
[14] L. O. Jimenez and D. A. Landgrebe, “Hyperspectral data analysis and
feature reduction via projection pursuit,” IEEE Trans. Geosci. Remote
Sensing, vol. 37, pp. 2653–2667, Nov. 1999.
[15] S. Kumar, J. Ghosh, and M. M. Crawford, “Best-bases feature extraction
algorithms for classiﬁcation of hyperspectral data,” IEEE Trans. Geosc.
Remote. Sensing, vol. 39, pp. 1368–1379, May 2001.
[16] J. P. Hoffbeck and D. A. Landgrebe, “Classiﬁcation of remote sensing
images having high-spectral resolution,” Remote Sens. Environ., vol. 57,
pp. 119–126, 1996.
[17] F. Tsai and W. D. Philpot, “A derivative-aided hyperspectral image anal-
ysis system for land-cover classiﬁcation,” IEEE Trans. Geosc. Remote.
Sensing, vol. 40, pp. 416–425, Feb. 2002.
[18] J. A. Benediktsson and I. Kanellopoulos, “Classiﬁcation of multisource
and hyperspectral data based on decision fusion,” IEEE Trans. Geosci.
Remote Sensing, vol. 37, pp. 1367–1377, May 1999.
[19] X. Jia and J. A. Richards, “Cluster-space representation of hyperspectral
data classiﬁcation,” IEEE Trans. Geosci. Remote Sensing, vol. 40, pp.
593–598, Mar. 2002.
[20] L. Hermes, D. Frieauff, J. Puzicha, and J. M. Buhmann, “Support vector
machines for land usage classiﬁcation in landsat TM imagery,” in Proc.
IGARSS, Hamburg, Germany, 1999, pp. 348–350.
[21] F. Roli and G. Fumera, “Support vector machines for remote-sensing
image classiﬁcation,” Proc. SPIE, vol. 4170, pp. 160–166, 2001.
[22] C. Huang, L. S. Davis, and J. R. G. Townshend, “An assessment of sup-
port vector machines for land cover classiﬁcation,” Int. J. Remote Sens.,
vol. 23, pp. 725–749, 2002.
[23] J. A. Gualtieri and R. F. Cromp, “Support vector machines for hy-
perspectral remote sensing classiﬁcation,” Proc. SPIE, vol. 3584, pp.
221–232, 1998.
[24] J. A. Gualtieri, S. R. Chettri, R. F. Cromp, and L. F. Johnson, “Support
vector machine classiﬁers as applied to AVIRIS data,” in Summaries
8th JPL Airborne Earth Science Workshop, 1999, JPL Pub. 99-17, pp.
217–227. Online. [Available]: ftp://popo.jpl.nasa.gov/pub/docs/work-
shops/99_docs/toc.html.
[25] J. A. Gualtieri and S. Chettri, “Support vector machines for classiﬁca-
tion of hyperspectral data,” in Proc. IGARSS, Honolulu, HI, 2000, pp.
813–815.
[26] F. Melgani and L. Bruzzone, “Support vector machines for classiﬁcation
of hyperspectral remote-sensing images,” in Proc. IGARSS, Toronto,
ON, Canada, 2002, pp. 506–508.
[27] V. N. Vapnik, Statistical Learning Theory.
New York: Wiley, 1998.
[28] C. B. E. Boser, I. M. Guyon, and V. N. Vapnik, “A training algorithm for
optimal margin classiﬁers,” in Proc. 5th Annu. ACM Workshop Compu-
tational Learning Theory, 1992, pp. 144–152.

----- Page 13 (native) -----
1790
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 42, NO. 8, AUGUST 2004
[29] C. J. C. Burges, “A tutorial on support vector machines for pattern recog-
nition,” Data Mining Knowl. Discov., vol. 2, pp. 121–167, 1998.
[30] Set of tutorials on SVM’s and kernel methods [Online]. Available:
http://www.kernel-machines.org/tutorial.html.
[31] I. El-Naqa, Y. Yongyi, M. N. Wernick, N. P. Galatsanos, and R. M.
Nishikawa, “A support vector machine approach for detection of micro-
calciﬁcations,” IEEE Trans. Med. Imag., vol. 21, pp. 1552–1563, Dec.
2002.
[32] J. Robinson and V. Kecman, “Combining support vector machine
learning with the discrete cosine transform in image compression,”
IEEE Trans. Neural Networks, vol. 14, pp. 950–958, July 2003.
[33] M. Pontil and A. Verri, “Support vector machines for 3D object recogni-
tion,” IEEE Trans. Pattern Anal. Machine Intell., vol. 20, pp. 637–646,
June 1998.
[34] D. J. Sebald and J. A. Bucklew, “Support vector machines and the mul-
tiple hypothesis test problem,” IEEE Trans. Signal Processing, vol. 49,
pp. 2865–2872, Nov. 2001.
[35] C.-W. Hsu and C.-J. Lin, “A comparison of methods for multiclass
support vector machines,” IEEE Trans. Neural Networks, vol. 13, pp.
415–425, Mar. 2002.
[36] AVIRIS NW Indiana’s Indian Pines 1992 data set [Online]. Available:
ftp://ftp.ecn.purdue.edu/biehl/MultiSpec/92AV3C (original ﬁles) and
ftp://ftp.ecn.purdue.edu/biehl/PC_MultiSpec/ThyFiles.zip
(ground
truth).
[37] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee, “Choosing mul-
tiple parameters for support vector machines,” Mach. Learn., vol. 46, pp.
131–159, 2002.
[38] K.-M. Chung, W.-C. Kao, T. Sun, L.-L. Wang, and C.-J. Lin, “Radius
margin bounds for support vector machines with the RBF kernel,”
Neural. Comput., vol. 15, pp. 2643–2681, 2003.
[39] L. O. Jimenez and D. A. Landgrebe, “Supervised classiﬁcation in high-
dimensional space: Geometrical, statistical, and asymptotic properties
of multivariate data,” IEEE Trans. Syst., Man, Cybern. C, vol. 28, pp.
39–54, Jan. 1998.
[40] M. G. Kendall, A Course in the Geometry of n-Dimensions.
New York:
Hafner, 1961.
[41] K. Fukunaga, Introduction to Statistical Pattern Recognition, 2nd
ed.
New York: Academic, 1990.
[42] L. Bottou, C. Cortes, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y.
LeCun, U. Muller, E. Sackinger, P. Simard, and V. Vapnik, “Comparison
of classiﬁer methods: A case study in handwriting digit recognition,” in
Proc. Int. Conf. Pattern Recognition, 1994, pp. 77–87.
[43] U. H.-G. Kreßel, “Pairwise classiﬁcation and support vector machines,”
in Advances in Kernel Methods: Support Vector Learning, B. Schölkopf,
C. J. C. Burges, and A. J. Smola, Eds.
Cambridge, MA: MIT Press,
1999, pp. 255–268.
[44] P. H. Swain and H. Hauska, “The decision tree classiﬁer: Design and po-
tential,” IEEE Trans. Geosci. Electron., vol. GE-15, pp. 142–147, 1977.
[45] B. Kim and D. A. Landgrebe, “Hierarchical classiﬁer design in high-di-
mensional, numerous class cases,” IEEE Trans. Geosci. Remote Sensing,
vol. 29, pp. 518–528, July 1991.
[46] J. T. Morgan, A. Henneguelle, M. M. Crawford, J. Ghosh, and A. Neuen-
schwander, “Adaptive feature spaces for land cover classiﬁcation with
limited ground truth data,” in Proc. 3rd Int. Workshop on Multiple Clas-
siﬁer Systems—MCS 2002, Cagliari, Italy, June 2002, pp. 189–200.
[47] M. Datcu, F. Melgani, A. Piardi, and S. B. Serpico, “Multisource data
classiﬁcation with dependence trees,” IEEE Trans. Geosci. Remote
Sensing, vol. 40, pp. 609–617, Mar. 2002.
[48] L. Bruzzone and D. F. Prieto, “A technique for the selection of kernel-
function parameters in RBF neural networks for classiﬁcation of re-
mote-sensing images,” IEEE Trans. Geosci Remote. Sensing, vol. 37,
pp. 1179–1184, Mar. 1999.
Farid Melgani (M’04) received the State Engineer
degree in electronics from the University of Batna,
Batna, Algeria, in 1994, the M.Sc. degree in elec-
trical engineering from the University of Baghdad,
Baghdad, Iraq, in 1999, and the Ph.D. degree in elec-
tronic and computer engineering from the University
of Genoa, Genoa, Italy, in 2003.
From 1999 to 2002, he cooperated with the Signal
Processing and Telecommunications Group, Depart-
ment of Biophysical and Electronic Engineering,
University of Genoa. He is currently an Assistant
Professor of telecommunications at the University of Trento, Trento, Italy,
where he teaches pattern recognition, radar remote sensing systems, and
digital transmission. His research interests are in the area of processing and
pattern recognition techniques applied to remote sensing images (classiﬁcation,
multitemporal analysis, and data fusion). He is coauthor of more than 30
scientiﬁc publications.
Dr. Melgani served on the Scientiﬁc Committee of the SPIE Interna-
tional Conferences on Signal and Image Processing for Remote Sensing VI
(Barcelona, Spain, 2000), VII (Toulouse, France, 2001), VIII (Crete, 2002),
and IX (Barcelona, Spain, 2003) and is a referee for the IEEE TRANSACTIONS
ON GEOSCIENCE AND REMOTE SENSING.
Lorenzo Bruzzone (S’95–M’99–SM’03) received
the laurea (M.S.) degree in electronic engineering
(summa cum laude) and the Ph.D. degree in telecom-
munications, both from the University of Genoa,
Genoa, Italy, in 1993 and 1998, respectively.
He is currently Head of the Remote Sensing Lab-
oratory in the Department of Information and Com-
munication Technologies at the University of Trento,
Trento, Italy. From 1998 to 2000, he was a Postdoc-
toral Researcher at the University of Genoa. From
2000 to 2001, he was an Assistant Professor at the
University of Trento, where he has been an Associate Professor of telecommu-
nications since November 2001. He currently teaches remote sensing, pattern
recognition, and electrical communications. His current research interests are
in the area of remote sensing image processing and recognition (analysis of
multitemporal data, feature selection, classiﬁcation, data fusion, and neural net-
works). He conducts and supervises research on these topics within the frame-
works of several national and international projects. He is the author (or coau-
thor) of more than 100 scientiﬁc publications, including journals, book chapters,
and conference proceedings. He is a referee for many international journals and
has served on the Scientiﬁc Committees of several international conferences.
Dr. Bruzzone ranked ﬁrst place in the Student Prize Paper Competition of the
1998 IEEE International Geoscience and Remote Sensing Symposium (Seattle,
July 1998). He is the Delegate in the scientiﬁc board for the University of Trento
of the Italian Consortium for Telecommunications (CNIT) and a member of
the Scientiﬁc Committee of the India–Italy Center for Advanced Research. He
was a recipient of the Recognition of IEEE Transactions on Geoscience and
Remote Sensing Best Reviewers in 1999 and was a Guest Editor of a Special
Issue of the IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING on
the subject of the analysis of multitemporal remote sensing images (November
2003). He was the General Co-chair of the First and Second IEEE Interna-
tional Workshop on the Analysis of Multi-temporal Remote-Sensing Images
(Trento, Italy, September 2001—Ispra, Italy, July 2003). Since 2003, he has
been the Chair of the SPIE Conference on Image and Signal Processing for
Remote Sensing (Barcelona, Spain, September 2003—Maspalomas, Gran Ca-
naria, September 2004). He is an Associate Editor of the IEEE GEOSCIENCE AND
REMOTE SENSING LETTERS. He is a member of the International Association for
Pattern Recognition (IAPR) and of the Italian Association for Remote Sensing
(AIT).