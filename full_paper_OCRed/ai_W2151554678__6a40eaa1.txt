

----- Page 1 (native) -----
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
67
No Free Lunch Theorems for Optimization
David H. Wolpert and William G. Macready
Abstract—A framework is developed to explore the connection
between effective optimization algorithms and the problems they
are solving. A number of “no free lunch” (NFL) theorems are
presented which establish that for any algorithm, any elevated
performance over one class of problems is offset by perfor-
mance over another class. These theorems result in a geometric
interpretation of what it means for an algorithm to be well
suited to an optimization problem. Applications of the NFL
theorems to information-theoretic aspects of optimization and
benchmark measures of performance are also presented. Other
issues addressed include time-varying optimization problems and
a priori “head-to-head” minimax distinctions between optimiza-
tion algorithms, distinctions that result despite the NFL theorems’
enforcing of a type of uniformity over all algorithms.
Index Terms— Evolutionary algorithms, information theory,
optimization.
I. INTRODUCTION
T
HE past few decades have seen an increased interest
in general-purpose “black-box” optimization algorithms
that exploit limited knowledge concerning the optimization
problem on which they are run. In large part these algorithms
have drawn inspiration from optimization processes that occur
in nature. In particular, the two most popular black-box
optimization strategies, evolutionary algorithms [1]–[3] and
simulated annealing [4], mimic processes in natural selection
and statistical mechanics, respectively.
In light of this interest in general-purpose optimization
algorithms, it has become important to understand the rela-
tionship between how well an algorithm
performs and the
optimization problem
on which it is run. In this paper
we present a formal analysis that contributes toward such
an understanding by addressing questions like the following:
given the abundance of black-box optimization algorithms and
of optimization problems, how can we best match algorithms
to problems (i.e., how best can we relax the black-box nature
of the algorithms and have them exploit some knowledge
concerning the optimization problem)? In particular, while
serious optimization practitioners almost always perform such
matching, it is usually on a heuristic basis; can such matching
be formally analyzed? More generally, what is the underlying
mathematical “skeleton” of optimization theory before the
“ﬂesh” of the probability distributions of a particular context
and set of optimization problems are imposed? What can
Manuscript received August 15, 1996; revised December 30, 1996. This
work was supported by the Santa Fe Institute and TXN Inc.
D. H. Wolpert is with IBM Almaden Research Center, San Jose, CA 95120-
6099 USA.
W. G. Macready was with Santa Fe Institute, Santa Fe, NM 87501 USA.
He is now with IBM Almaden Research Center, San Jose, CA 95120-6099
USA.
Publisher Item Identiﬁer S 1089-778X(97)03422-X.
information theory and Bayesian analysis contribute to an
understanding of these issues? How a priori generalizable are
the performance results of a certain algorithm on a certain
class of problems to its performance on other classes of
problems? How should we even measure such generalization?
How should we assess the performance of algorithms on
problems so that we may programmatically compare those
algorithms?
Broadly speaking, we take two approaches to these ques-
tions. First, we investigate what a priori restrictions there are
on the performance of one or more algorithms as one runs
over the set of all optimization problems. Our second approach
is to instead focus on a particular problem and consider the
effects of running over all algorithms. In the current paper
we present results from both types of analyses but concentrate
largely on the ﬁrst approach. The reader is referred to the
companion paper [5] for more types of analysis involving the
second approach.
We begin in Section II by introducing the necessary nota-
tion. Also discussed in this section is the model of computation
we adopt, its limitations, and the reasons we chose it.
One might expect that there are pairs of search algorithms
and
such that
performs better than
on average, even if
sometimes outperforms
. As an example, one might expect
that hill climbing usually outperforms hill descending if one’s
goal is to ﬁnd a maximum of the cost function. One might also
expect it would outperform a random search in such a context.
One of the main results of this paper is that such expecta-
tions are incorrect. We prove two “no free lunch” (NFL) the-
orems in Section III that demonstrate this and more generally
illuminate the connection between algorithms and problems.
Roughly speaking, we show that for both static and time-
dependent optimization problems, the average performance
of any pair of algorithms across all possible problems is
identical. This means in particular that if some algorithm
’s
performance is superior to that of another algorithm
over
some set of optimization problems, then the reverse must be
true over the set of all other optimization problems. (The reader
is urged to read this section carefully for a precise statement
of these theorems.) This is true even if one of the algorithms
is random; any algorithm
performs worse than randomly
just as readily (over the set of all optimization problems) as
it performs better than randomly. Possible objections to these
results are addressed in Sections III-A and III-B.
In Section IV we present a geometric interpretation of the
NFL theorems. In particular, we show that an algorithm’s
average performance is determined by how “aligned” it is
with the underlying probability distribution over optimization
problems on which it is run. This section is critical for an
1089–778X/97$10.00 1997 IEEE

----- Page 2 (native) -----
68
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
understanding of how the NFL results are consistent with the
well-accepted fact that many search algorithms that do not take
into account knowledge concerning the cost function work
well in practice.
Section V-A demonstrates that the NFL theorems allow
one to answer a number of what would otherwise seem to
be intractable questions. The implications of these answers
for measures of algorithm performance and of how best to
compare optimization algorithms are explored in Section V-B.
In Section VI we discuss some of the ways in which,
despite the NFL theorems, algorithms can have a priori
distinctions that hold even if nothing is speciﬁed concerning
the optimization problems. In particular, we show that there
can be “head-to-head” minimax distinctions between a pair of
algorithms, i.e., that when considering one function at a time,
a pair of algorithms may be distinguishable, even if they are
not when one looks over all functions.
In Section VII we present an introduction to the alternative
approach to the formal analysis of optimization in which
problems are held ﬁxed and one looks at properties across
the space of algorithms. Since these results hold in general,
they hold for any and all optimization problems and thus
are independent of the types of problems one is more or
less likely to encounter in the real world. In particular,
these results show that there is no a priori justiﬁcation for
using a search algorithm’s observed behavior to date on a
particular cost function to predict its future behavior on that
function. In fact when choosing between algorithms based on
their observed performance it does not sufﬁce to make an
assumption about the cost function; some (currently poorly
understood) assumptions are also being made about how the
algorithms in question are related to each other and to the
cost function. In addition to presenting results not found in
[5], this section serves as an introduction to the perspective
adopted in [5].
We conclude in Section VIII with a brief discussion, a
summary of results, and a short list of open problems.
We have conﬁned all proofs to appendixes to facilitate the
ﬂow of the paper. A more detailed, and substantially longer,
version of this paper, a version that also analyzes some issues
not addressed in this paper, can be found in [6].
II. PRELIMINARIES
We restrict attention to combinatorial optimization in which
the search space
, though perhaps quite large, is ﬁnite.
We further assume that the space of possible “cost” values
is also ﬁnite. These restrictions are automatically met
for optimization algorithms run on digital computers where
typically
is some 32 or 64 bit representation of the real
numbers.
The size of the spaces
and
are indicated by
and
,
respectively. An optimization problem
(sometimes called
a “cost function” or an “objective function” or an “energy
function”) is represented as a mapping
and
indicates the space of all possible problems.
is of size
—a large but ﬁnite number. In addition to
static
, we are also interested in optimization problems that
depend explicitly on time. The extra notation required for such
time-dependent problems will be introduced as needed.
It is common in the optimization community to adopt
an oracle-based view of computation. In this view, when
assessing the performance of algorithms, results are stated
in terms of the number of function evaluations required to
ﬁnd a given solution. Practically though, many optimization
algorithms are wasteful of function evaluations. In particular,
many algorithms do not remember where they have already
searched and therefore often revisit the same points. Although
any algorithm that is wasteful in this fashion can be made
more efﬁcient simply by remembering where it has been (cf.
tabu search [7], [8]), many real-world algorithms elect not to
employ this stratagem. From the point of view of the oracle-
based performance measures, these revisits are “artifacts”
distorting the apparent relationship between many such real-
world algorithms.
This difﬁculty is exacerbated by the fact that the amount
of revisiting that occurs is a complicated function of both
the algorithm and the optimization problem and therefore
cannot be simply “ﬁltered out” of a mathematical analysis.
Accordingly, we have elected to circumvent the problem
entirely by comparing algorithms based on the number of
distinct function evaluations they have performed. Note that
this does not mean that we cannot compare algorithms that
are wasteful of evaluations—it simply means that we compare
algorithms by counting only their number of distinct calls to
the oracle.
We call a time-ordered set of
distinct visited points
a “sample” of size
. Samples are denoted by
. The points in a
sample are ordered according to the time at which they
were generated. Thus
indicates the
value of the
th successive element in a sample of size
and
is
its associated cost or
value.
will be used to indicate the ordered set of cost values. The
space of all samples of size
is
(so
) and the set of all possible samples of arbitrary
size is
.
As an important clariﬁcation of this deﬁnition, consider a
hill-descending algorithm. This is the algorithm that examines
a set of neighboring points in
and moves to the one having
the lowest cost. The process is then iterated from the newly
chosen point. (Often, implementations of hill descending stop
when they reach a local minimum, but they can easily be
extended to run longer by randomly jumping to a new unvis-
ited point once the neighborhood of a local minimum has been
exhausted.) The point to note is that because a sample contains
all the previous points at which the oracle was consulted, it
includes the
values of all the neighbors of the current
point, and not only the lowest cost one that the algorithm
moves to. This must be taken into account when counting the
value of
.
An optimization algorithm
is represented as a mapping
from previously visited sets of points to a single new (i.e.,
previously unvisited) point in
. Formally,
. Given our decision to only measure distinct
function evaluations even if an algorithm revisits previously

----- Page 3 (native) -----
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
69
searched points, our deﬁnition of an algorithm includes all
common black-box optimization techniques like simulated an-
nealing and evolutionary algorithms. (Techniques like branch
and bound [9] are not included since they rely explicitly on
the cost structure of partial solutions.)
As deﬁned above, a search algorithm is deterministic; every
sample maps to a unique new point. Of course, essentially, all
algorithms implemented on computers are deterministic,1 and
in this our deﬁnition is not restrictive. Nonetheless, it is worth
noting that all of our results are extensible to nondeterministic
algorithms, where the new point is chosen stochastically from
the set of unvisited points. This point is returned to later.
Under the oracle-based model of computation any measure
of the performance of an algorithm after
iterations is a
function of the sample
. Such performance measures will
be indicated by
. As an example, if we are trying
to ﬁnd a minimum of
, then a reasonable measure of the
performance of
might be the value of the lowest
value in
:
. Note that measures
of performance based on factors other than
(e.g., wall clock
time) are outside the scope of our results.
We shall cast all of our results in terms of probability
theory. We do so for three reasons. First, it allows simple
generalization of our results to stochastic algorithms. Second,
even when the setting is deterministic, probability theory
provides a simple consistent framework in which to carry out
proofs. The third reason for using probability theory is perhaps
the most interesting. A crucial factor in the probabilistic
framework is the distribution
.
This distribution, deﬁned over
, gives the probability that
each
is the actual optimization problem at hand.
An approach based on this distribution has the immediate
advantage that often knowledge of a problem is statistical in
nature and this information may be easily encodable in
.
For example, Markov or Gibbs random ﬁeld descriptions [10]
of families of optimization problems express
exactly.
Exploiting
, however, also has advantages even when
we are presented with a single uniquely speciﬁed cost function.
One such advantage is the fact that although it may be fully
speciﬁed, many aspects of the cost function are effectively
unknown (e.g., we certainly do not know the extrema of the
function). It is in many ways most appropriate to have this
effective ignorance reﬂected in the analysis as a probability
distribution. More generally, optimization practitioners usually
act as though the cost function is partially unknown, in that the
same algorithm is used for all cost functions in a class of such
functions (e.g., in the class of all traveling salesman problems
having certain characteristics). In so doing, the practitioner
implicitly acknowledges that distinctions between the cost
functions in that class are irrelevant or at least unexploitable.
In this sense, even though we are presented with a single
particular problem from that class, we act as though we are
presented with a probability distribution over cost functions,
a distribution that is nonzero only for members of that class
of cost functions.
is thus a prior speciﬁcation of the
class of the optimization problem at hand, with different
1In particular, note that pseudorandom number generators are deterministic
given a seed.
classes of problems corresponding to different choices of
what algorithms we will use, and giving rise to different
distributions
.
Given our choice to use probability theory, the perfor-
mance of an algorithm
iterated
times on a cost function
is measured with
. This is the conditional
probability of obtaining a particular sample
under the
stated conditions. From
performance measures
can be found easily.
In the next section we analyze
and in par-
ticular how it varies with the algorithm
. Before proceeding
with that analysis, however, it is worth brieﬂy noting that there
are other formal approaches to the issues investigated in this
paper. Perhaps the most prominent of these is the ﬁeld of com-
putational complexity. Unlike the approach taken in this paper,
computational complexity largely ignores the statistical nature
of search and concentrates instead on computational issues.
Much, though by no means all, of computational complexity is
concerned with physically unrealizable computational devices
(e.g., Turing machines) and the worst-case resource usage
required to ﬁnd optimal solutions. In contrast, the analysis
in this paper does not concern itself with the computational
engine used by the search algorithm, but rather concentrates
exclusively on the underlying statistical nature of the search
problem. The current probabilistic approach is complimentary
to computational complexity. Future work involves combining
our analysis of the statistical nature of search with practical
concerns for computational resources.
III. THE NFL THEOREMS
In this section we analyze the connection between algo-
rithms and cost functions. We have dubbed the associated
results NFL theorems because they demonstrate that if an
algorithm performs well on a certain class of problems then
it necessarily pays for that with degraded performance on the
set of all remaining problems. Additionally, the name em-
phasizes a parallel with similar results in supervised learning
[11], [12].
The precise question addressed in this section is: “How does
the set of problems
for which algorithm
performs
better than algorithm
compare to the set
for which
the reverse is true?” To address this question we compare the
sum over all
of
to the sum over all
of
. This comparison constitutes a major result of
this paper:
is independent of
when averaged
over all cost functions.
Theorem 1: For any pair of algorithms
and
A proof of this result is found in Appendix A. An immediate
corollary of this result is that for any performance measure
, the average over all
of
is inde-
pendent of
. The precise way that the sample is mapped to
a performance measure is unimportant.
This theorem explicitly demonstrates that what an algorithm
gains in performance on one class of problems is necessarily

----- Page 4 (native) -----
70
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
offset by its performance on the remaining problems; that is
the only way that all algorithms can have the same
-averaged
performance.
A result analogous to Theorem 1 holds for a class of time-
dependent cost functions. The time-dependent functions we
consider begin with an initial cost function
that is present
at the sampling of the ﬁrst
value. Before the beginning of
each subsequent iteration of the optimization algorithm, the
cost function is deformed to a new function, as speciﬁed by a
mapping
.2 We indicate this mapping with the
notation
. So the function present during the th iteration is
.
is assumed to be a (potentially -dependent)
bijection between
and
. We impose bijectivity because if
it did not hold, the evolution of cost functions could narrow
in on a region of
’s for which some algorithms may perform
better than others. This would constitute an a priori bias in
favor of those algorithms, a bias whose analysis we wish to
defer to future work.
How best to assess the quality of an algorithm’s perfor-
mance on time-dependent cost functions is not clear. Here we
consider two schemes based on manipulations of the deﬁnition
of the sample. In scheme 1 the particular
value in
corresponding to a particular
value
is given by the
cost function that was present when
was sampled. In
contrast, for scheme 2 we imagine a sample
given by the
values from the present cost function for each of the
values in
. Formally if
, then in scheme 1
we have
, and
in scheme 2 we have
where
is the ﬁnal cost function.
In some situations it may be that the members of the
sample “live” for a long time, compared to the time scale
of the dynamics of the cost function. In such situations it may
be appropriate to judge the quality of the search algorithm
by
; all those previous elements of the sample that are
still “alive” at time
, and therefore their current cost is of
interest. On the other hand, if members of the sample live
for only a short time on the time scale of the dynamics of
the cost function, one may instead be concerned with things
like how well the “living” member(s) of the sample track
the changing cost function. In such situations, it may make
more sense to judge the quality of the algorithm with the
sample.
Results similar to Theorem 1 can be derived for both
schemes. By analogy with that theorem, we average over all
possible ways a cost function may be time dependent, i.e., we
average over all
(rather than over all
). Thus we consider
where
is the initial cost function.
Since
only takes effect for
, and since
is ﬁxed,
there are a priori distinctions between algorithms as far as
the ﬁrst member of the sample is concerned. After redeﬁning
samples, however, to only contain those elements added after
the ﬁrst iteration of the algorithm, we arrive at the following
result, proven in Appendix B.
2An obvious restriction would be to require that T does not vary with time,
so that it is a mapping simply from F to F. An analysis for T’s limited in
this way is beyond the scope of this paper.
Theorem 2: For all
, algorithms
and
, and initial cost functions
and
So, in particular, if one algorithm outperforms another for
certain kinds of cost function dynamics, then the reverse must
be true on the set of all other cost function dynamics.
Although this particular result is similar to the NFL result
for the static case, in general the time-dependent situation
is more subtle. In particular, with time dependence there
are situations in which there can be a priori distinctions
between algorithms even for those members of the sample
arising after the ﬁrst. For example, in general there will be
distinctions between algorithms when considering the quantity
. To see this, consider the case where
is a set of contiguous integers and for all iterations
is a
shift operator, replacing
by
for all
[with
]. For such a case we can construct
algorithms which behave differently a priori. For example,
take
to be the algorithm that ﬁrst samples
at
, next
at
, and so on, regardless of the values in the sample.
Then for any
,
is always made up of identical
values.
Accordingly,
is nonzero only for
for
which all values
are identical. Other search algorithms,
even for the same shift
, do not have this restriction on
values. This constitutes an
a priori distinction between
algorithms.
A. Implications of the NFL Theorems
As emphasized above, the NFL theorems mean that if an
algorithm does particularly well on average for one class of
problems then it must do worse on average over the remaining
problems. In particular, if an algorithm performs better than
random search on some class of problems then in must
perform worse than random search on the remaining problems.
Thus comparisons reporting the performance of a particular
algorithm with a particular parameter setting on a few sample
problems are of limited utility. While such results do indicate
behavior on the narrow range of problems considered, one
should be very wary of trying to generalize those results to
other problems.
Note, however, that the NFL theorems need not be viewed
as a way of comparing function classes
and
(or
classes of evolution operators
and
, as the case might
be). They can be viewed instead as a statement concerning
any algorithm’s performance when
is not ﬁxed, under the
uniform prior over cost functions,
. If we wish
instead to analyze performance where
is not ﬁxed, as in this
alternative interpretation of the NFL theorems, but in contrast
with the NFL case
is now chosen from a nonuniform prior,
then we must analyze explicitly the sum
(1)

----- Page 5 (native) -----
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
71
Since it is certainly true that any class of problems faced by
a practitioner will not have a ﬂat prior, what are the practical
implications of the NFL theorems when viewed as a statement
concerning an algorithm’s performance for nonﬁxed
? This
question is taken up in greater detail in Section IV but we
offer a few comments here.
First, if the practitioner has knowledge of problem charac-
teristics but does not incorporate them into the optimization
algorithm, then
is effectively uniform. (Recall that
can be viewed as a statement concerning the practitioner’s
choice of optimization algorithms.) In such a case, the NFL
theorems establish that there are no formal assurances that the
algorithm chosen will be at all effective.
Second, while most classes of problems will certainly have
some structure which, if known, might be exploitable, the
simple existence of that structure does not justify choice
of a particular algorithm; that structure must be known and
reﬂected directly in the choice of algorithm to serve as such a
justiﬁcation. In other words, the simple existence of structure
per se, absent a speciﬁcation of that structure, cannot provide a
basis for preferring one algorithm over another. Formally, this
is established by the existence of NFL-type theorems in which
rather than average over speciﬁc cost functions
, one averages
over speciﬁc “kinds of structure,” i.e., theorems in which one
averages
over distributions
. That such
theorems hold when one averages over all
means that
the indistinguishability of algorithms associated with uniform
is not some pathological, outlier case. Rather, uniform
is a “typical” distribution as far as indistinguishability
of algorithms is concerned. The simple fact that the
at
hand is nonuniform cannot serve to determine one’s choice of
optimization algorithm.
Finally, it is important to emphasize that even if one is
considering the case where
is not ﬁxed, performing the
associated average according to a uniform
is not essential
for NFL to hold. NFL can also be demonstrated for a range
of nonuniform priors. For example, any prior of the form
(where
is the distribution of
values) will also give NFL theorems. The
-average can also
enforce correlations between costs at different
values and
NFL-like results will still be obtained. For example, if costs
are rank ordered (with ties broken in some arbitrary way) and
we sum only over all cost functions given by permutations of
those orderings, then NFL remains valid.
The choice of uniform
was motivated more from
theoretical rather than pragmatic concerns, as a way of an-
alyzing the theoretical structure of optimization. Nevertheless,
the cautionary observations presented above make clear that
an analysis of the uniform
case has a number of
ramiﬁcations for practitioners.
B. Stochastic Optimization Algorithms
Thus far we have considered the case in which algorithms
are deterministic. What is the situation for stochastic algo-
rithms? As it turns out, NFL results hold even for these
algorithms.
The proof is straightforward. Let
be a stochastic “nonpo-
tentially revisiting” algorithm. Formally, this means that
is
a mapping taking any sample
to a
-dependent distribution
over
that equals zero for all
. In this sense
is
what in statistics community is known as a “hyper-parameter,”
specifying the function
for all
and
. One can now reproduce the derivation of the NFL
result for deterministic algorithms, only with
replaced by
throughout. In so doing, all steps in the proof remain valid.
This establishes that NFL results apply to stochastic algorithms
as well as deterministic ones.
IV. A GEOMETRIC PERSPECTIVE ON THE NFL THEOREMS
Intuitively, the NFL theorem illustrates that if knowledge
of
, perhaps speciﬁed through
, is not incorporated into
, then there are no formal assurances that
will be effective.
Rather, in this case effective optimization relies on a fortuitous
matching between
and
. This point is formally established
by viewing the NFL theorem from a geometric perspective.
Consider the space
of all possible cost functions. As pre-
viously discussed in regard to (1), the probability of obtaining
some
is
where
is the prior probability that the optimization
problem at hand has cost function
. This sum over functions
can be viewed as an inner product in
. Deﬁning the
-space
vectors
and
by their
components
and
, respectively
(2)
This equation provides a geometric interpretation of the op-
timization process.
can be viewed as ﬁxed to the sample
that is desired, usually one with a low cost value, and
is a measure of the computational resources that can be
afforded. Any knowledge of the properties of the cost function
goes into the prior over cost functions
. Then (2) says the
performance of an algorithm is determined by the magnitude
of its projection onto
, i.e., by how aligned
is with
the problems . Alternatively, by averaging over
, it is easy
to see that
is an inner product between
and
. The expectation of any performance measure
can be written similarly.
In any of these cases,
or
must “match” or be aligned
with
to get the desired behavior. This need for matching
provides a new perspective on how certain algorithms can
perform well in practice on speciﬁc kinds of problems. For
example, it means that the years of research into the traveling
salesman problem (TSP) have resulted in algorithms aligned
with the (implicit)
describing traveling salesman problems
of interest to TSP researchers.
Taking
the
geometric
view,
the
NFL
result
that
is independent of
has the interpretation
that for any particular
and
, all algorithms
have the
same projection onto the uniform
, represented by the
diagonal vector
. Formally,
where
is some constant depending only upon
and
. For
deterministic algorithms, the components of
(i.e., the

----- Page 6 (native) -----
72
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
Fig. 1.
Schematic view of the situation in which function space F is three
dimensional. The uniform prior over this space, ~1~1~1, lies along the diagonal.
Different algorithms a give different vectors v lying in the cone surrounding
the diagonal. A particular problem is represented by its prior ~p~p~p lying on the
simplex. The algorithm that will perform best will be the algorithm in the
cone having the largest inner product with ~p~p~p.
probabilities that algorithm
gives sample
on cost function
after
distinct cost evaluations) are all either zero or one,
so NFL also implies that
.
Geometrically, this means that the length of
is
independent of
. Different algorithms thus generate different
vectors
all having the same length and lying on a
cone with constant projection onto
. A schematic of this
situation is shown in Fig. 1 for the case where
is three
dimensional. Because the components of
are binary,
we might equivalently view
as lying on the subset
of Boolean hypercube vertices having the same hamming
distance from
.
Now restrict attention to algorithms having the same prob-
ability of some particular
. The algorithms in this set lie in
the intersection of two cones—one about the diagonal, set by
the NFL theorem, and one set by having the same probability
for
. This is in general an
dimensional manifold.
Continuing, as we impose yet more
-based restrictions on
a set of algorithms, we will continue to reduce the dimension-
ality of the manifold by focusing on intersections of more and
more cones.
This geometric view of optimization also suggests measures
for determining how “similar” two optimization algorithms
are. Consider again (2). In that the algorithm only gives
, perhaps the most straightforward way to compare
two algorithms
and
would be by measuring how similar
the vectors
and
are, perhaps by evaluating
the dot product of those vectors. Those vectors, however, occur
on the right-hand side of (2), whereas the performance of the
algorithms—which is after all our ultimate concern—occurs on
the left-hand side. This suggests measuring the similarity of
two algorithms not directly in terms of their vectors
,
but rather in terms of the dot products of those vectors with .
For example, it may be the case that algorithms behave very
similarly for certain
but are quite different for other
. In many respects, knowing this about two algorithms
is of more interest than knowing how their vectors
compare.
As another example of a similarity measure suggested by the
geometric perspective, we could measure similarity between
algorithms based on similarities between
’s. For example,
for two different algorithms, one can imagine solving for the
that optimizes
for those algorithms, in
some nontrivial sense.3 We could then use some measure of
distance between those two
distributions as a gauge of
how similar the associated algorithms are.
Unfortunately, exploiting the inner product formula in prac-
tice, by going from a
to an algorithm optimal for
that
, appears to often be quite difﬁcult. Indeed, even
determining a plausible
for the situation at hand is
often difﬁcult. Consider, for example, TSP problems with
cities. To the degree that any practitioner attacks all
-city
TSP cost functions with the same algorithm, he/she implicitly
ignores distinctions between such cost functions. In this, that
practitioner has implicitly agreed that the problem is one of
how their ﬁxed algorithm does across the set of all
-city
TSP cost functions. But the detailed nature of the
that
is uniform over this class of problems appears to be difﬁcult
to elucidate.
On the other hand, there is a growing body of work that does
rely explicitly on enumeration of
. For example, appli-
cations of Markov random ﬁelds [10], [13] to cost landscapes
yield
directly as a Gibbs distribution.
V. CALCULATIONAL APPLICATIONS OF THE NFL THEOREMS
In this section, we explore some of the applications of
the NFL theorems for performing calculations concerning
optimization. We will consider both calculations of practical
and theoretical interest and begin with calculations of theo-
retical interest, in which information-theoretic quantities arise
naturally.
A. Information-Theoretic Aspects of Optimization
For expository purposes, we simplify the discussion slightly
by considering only the histogram of number of instances of
each possible cost value produced by a run of an algorithm, and
not the temporal order in which those cost values were gener-
ated. Many real-world performance measures are independent
of such temporal information. We indicate that histogram with
the symbol ;
has
components
, where
is the number of times cost value
occurs in the sample
.
Now consider any question like the following: “What frac-
tion of cost functions give a particular histogram
of cost
values after
distinct cost evaluations produced by using a
particular instantiation of an evolutionary algorithm?”
At ﬁrst glance this seems to be an intractable question,
but the NFL theorem provides a way to answer it. This is
because—according to the NFL theorem—the answer must be
independent of the algorithm used to generate . Consequently,
3In particular, one may want to impose restrictions on P (f). For instance,
one may wish to only consider P (f) that are invariant under at least partial
relabeling of the elements in X, to preclude there being an algorithm that will
assuredly “luck out” and land on minx2X f(x) on its very ﬁrst query.

----- Page 7 (native) -----
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
73
we can choose an algorithm for which the calculation is
tractable.
Theorem 3: For any algorithm, the fraction of cost func-
tions that result in a particular histogram
is
For large enough
, this can be approximated as
where
is the entropy of the distribution
, and
is a constant that does not depend on
.
This theorem is derived in Appendix C. If some of the
are
zero, the approximation still holds, only with
redeﬁned to
exclude the ’s corresponding to the zero-valued
. However,
is deﬁned and the normalization constant of (3) can be found
by summing over all
lying on the unit simplex [14].
A related question is the following: “For a given cost
function, what is the fraction
of all algorithms that give
rise to a particular
?” It turns out that the only feature of
relevant for this question is the histogram of its cost values
formed by looking across all
. Specify the fractional form
of this histogram by
so that there are
points in
for which
has the th
value.
In Appendix D it is shown that to leading order,
depends on yet another information-theoretic quantity, the
Kullback–Liebler distance [15] between
and
.
Theorem 4: For a given
with histogram
, the
fraction of algorithms that give rise to a histogram
is given by
(3)
For large enough
, this can be written as
where
is the Kullback–Liebler distance between
the distributions
and
.
As before,
can be calculated by summing
over the unit
simplex.
B. Measures of Performance
We now show how to apply the NFL framework to calculate
certain benchmark performance measures. These allow both
the programmatic assessment of the efﬁcacy of any individual
optimization algorithm and principled comparisons between
algorithms.
Without loss of generality, assume that the goal of the search
process is ﬁnding a minimum. So we are interested in the -
dependence of
, by which we mean
the probability that the minimum cost an algorithm
ﬁnds
on problem
in
distinct evaluations is larger than
. At
least three quantities related to this conditional probability can
be used to gauge an algorithm’s performance in a particular
optimization run:
i) the uniform average of
over all
cost functions;
ii) the form
takes for the random
algorithm, which uses no information from the sample
;
iii) the fraction of algorithms which, for a particular
and
, result in a
whose minimum exceeds .
These measures give benchmarks which any algorithm run
on a particular cost function should surpass if that algorithm is
to be considered as having worked well for that cost function.
Without loss of generality assume that the th cost value
(i.e.,
) equals . So cost values range from minimum of one
to a maximum of
, in integer increments. The following
results are derived in Appendix E.
Theorem 5:
where
is the fraction of cost lying above .
In the limit of
, this distribution obeys the following
relationship:
Unless one’s algorithm has its best-cost-so-far drop faster
than the drop associated with these results, one would be hard
pressed indeed to claim that the algorithm is well suited to the
cost function at hand. After all, for such a performance the
algorithm is doing no better than one would expect it to when
run on a randomly chosen cost function.
Unlike the preceding measure, the measures analyzed below
take into account the actual cost function at hand. This is
manifested in the dependence of the values of those measures
on the vector
given by the cost function’s
histogram.
Theorem 6: For the random algorithm
(4)
where
is the fraction of points in
for
which
. To ﬁrst order in
(5)
This result allows the calculation of other quantities of
interest for measuring performance, for example the quantity
Note that for many cost functions of both practical and
theoretical interest, cost values are approximately distributed

----- Page 8 (native) -----
74
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
Gaussianly. For such cases, we can use the Gaussian nature
of the distribution to facilitate our calculations. In particular,
if the mean and variance of the Gaussian are
and
,
respectively, then we have
,
where erfc is the complimentary error function.
To calculate the third performance measure, note that for
ﬁxed
and
, for any (deterministic) algorithm
is either one or zero. Therefore the fraction of
algorithms that result in a
whose minimum exceeds
is
given by
Expanding in terms of
, we can rewrite the numerator of
this ratio as
. The
ratio of this quantity to
, however, is exactly what was
calculated when we evaluated measure ii) [see the beginning
of the argument deriving (4)]. This establishes the following
theorem.
Theorem 7: For ﬁxed
and
, the fraction of algorithms
which result in a
whose minimum exceeds
is given by the
quantity on the right-hand sides of (4) and (5).
As a particular example of applying this result, consider
measuring the value of
produced in a particular run
of an algorithm. Then imagine that when it is evaluated for
equal to this value, the quantity given in (5) is less than 1/2.
In such a situation the algorithm in question has performed
worse than over half of all search algorithms, for the
and
at hand, hardly a stirring endorsement.
None of the above discussion explicitly concerns the dy-
namics of an algorithm’s performance as
increases. Many
aspects of such dynamics may be of interest. As an example,
let us consider whether, as
grows, there is any change in
how well the algorithm’s performance compares to that of the
random algorithm.
To this end, let the sample generated by the algorithm
after
steps be
, and deﬁne
. Let
be
the number of additional steps it takes the algorithm to ﬁnd
an
such that
. Now we can estimate the number
of steps it would have taken the random search algorithm to
search
and ﬁnd a point whose
was less than
. The
expected value of this number of steps is
, where
is the fraction of
for which
. Therefore
is how much worse
did than the random
algorithm, on average.
Next, imagine letting
run for many steps over some ﬁtness
function
and plotting how well
did in comparison to the
random algorithm on that run, as
increased. Consider the
step where
ﬁnds its
th new value of
. For that step,
there is an associated
[the number of steps until the next
] and
. Accordingly, indicate that step on our
plot as the point
. Put down as many points
on our plot as there are successive values of
in the
run of
over
.
If throughout the run
is always a better match to
than
is the random search algorithm, then all the points in the plot
will have their ordinate values lie below zero. If the random
algorithm won for any of the comparisons however, that would
give a point lying above zero. In general, even if the points
all lie to one side of zero, one would expect that as the
search progresses there would be a corresponding (perhaps
systematic) variation in how far away from zero the points lie.
That variation indicates when the algorithm is entering harder
or easier parts of the search.
Note that even for a ﬁxed
, by using different starting
points for the algorithm one could generate many of these
plots and then superimpose them. This allows a plot of the
mean value of
as a function of
along with
an associated error bar. Similarly, the single number
characterizing the random algorithm could be replaced with
a full distribution over the number of required steps to ﬁnd a
new minimum. In these and similar ways, one can generate
a more nuanced picture of an algorithm’s performance than
is provided by any of the single numbers given by the
performance measure discussed above.
VI. MINIMAX DISTINCTIONS BETWEEN ALGORITHMS
The NFL theorems do not directly address minimax prop-
erties of search. For example, say we are considering two
deterministic algorithms
and
. It may very well be that
there exist cost functions
such that
’s histogram is much
better (according to some appropriate performance measure)
than
’s, but no cost functions for which the reverse is true.
For the NFL theorem to be obeyed in such a scenario, it would
have to be true that there are many more
for which
’s
histogram is better than
’s than vice-versa, but it is only
slightly better for all those
. For such a scenario, in a certain
sense
has better “head-to-head” minimax behavior than
;
there are
for which
beats
badly, but none for which
does substantially worse than
.
Formally, we say that there exists head-to-head minimax
distinctions between two algorithms
and
iff there exists
a
such that for at least one cost function
, the difference
, but there is no other
for which
. A similar
deﬁnition can be used if one is instead interested in
or
, rather than
.
It appears that analyzing head-to-head minimax properties
of algorithms is substantially more difﬁcult than analyzing
average behavior as in the NFL theorem. Presently, very
little is known about minimax behavior involving stochastic
algorithms. In particular, it is not known if there are any senses
in which a stochastic version of a deterministic algorithm
has better/worse minimax behavior than that deterministic
algorithm. In fact, even if we stick completely to deterministic
algorithms, only an extremely preliminary understanding of
minimax issues has been reached.
What is known is the following. Consider the quantity
for deterministic algorithms
and
. (By
is meant
the distribution of a random variable
evaluated at
.)
For deterministic algorithms, this quantity is just the number
of
such that it is both true that
produces a sample

----- Page 9 (native) -----
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
75
with
components
and that
produces a sample with
components
.
In Appendix F, it is proven by example that this quantity
need not be symmetric under interchange of
and
.
Theorem 8: In general
This means that under certain circumstances, even knowing
only the
components of the samples produced by two algo-
rithms run on the same unknown
, we can infer something
concerning which algorithm produced each population.
Now consider the quantity
again for deterministic algorithms
and
. This quantity is
just the number of
such that it is both true that
produces
a histogram
and that
produces a histogram
. It too
need not be symmetric under interchange of
and
(see
Appendix F). This is a stronger statement than the asymmetry
of
’s statement, since any particular histogram corresponds
to multiple samples.
It would seem that neither of these two results directly
implies that there are algorithms
and
such that for
some
’s histogram is much better than
’s, but for
no
’s is the reverse is true. To investigate this problem
involves looking over all pairs of histograms (one pair for
each
) such that there is the same relationship between (the
performances of the algorithms, as reﬂected in) the histograms.
Simply having an inequality between the sums presented above
does not seem to directly imply that the relative performances
between the associated pair of histograms is asymmetric. (To
formally establish this would involve creating scenarios in
which there is an inequality between the sums, but no head-
to-head minimax distinctions. Such an analysis is beyond the
scope of this paper.)
On the other hand, having the sums be equal does carry ob-
vious implications for whether there are head-to-head minimax
distinctions. For example, if both algorithms are deterministic,
then for any particular
equals one for one
pair and zero for all others. In such
a case,
is just the number
of
that result in the pair
. So
implies
that there are no head-to-head minimax distinctions between
and
. The converse, however, does not appear to hold.4
4Consider the grid of all (z; z0) pairs. Assign to each grid point the number
of f that result in that grid point’s (z; z0) pair. Then our constraints are i)
by the hypothesis that there are no head-to-head minimax distinctions, if grid
point (z1; z2) is assigned a nonzero number, then so is (z2; z1) and ii) by
the no-free-lunch theorem, the sum of all numbers in row z equals the sum
of all numbers in column z. These two constraints do not appear to imply
that the distribution of numbers is symmetric under interchange of rows and
columns. Although again, like before, to formally establish this point would
involve explicitly creating search scenarios in which it holds.
As a preliminary analysis of whether there can be head-
to-head minimax distinctions, we can exploit the result in
Appendix F, which concerns the case where
.
First, deﬁne the following performance measures of two-
element samples,
.
i)
.
ii)
.
iii)
of any other argument
.
In Appendix F we show that for this scenario there exist
pairs of algorithms
and
such that for one
gen-
erates the histogram
and
generates the histogram
, but there is no
for which the reverse occurs (i.e.,
there is no
such that
generates the histogram
and
generates
).
So in this scenario, with our deﬁned performance measure,
there are minimax distinctions between
and
. For one
the performance measures of algorithms
and
are,
respectively, zero and two. The difference in the
values
for the two algorithms is two for that
. There are no other
, however, for which the difference is
2. For this
then,
algorithm
is minimax superior to algorithm
.
It is not currently known what restrictions on
are
needed for there to be minimax distinctions between the
algorithms. As an example, it may well be that for
there are no minimax distinctions between al-
gorithms.
More generally, at present nothing is known about “how
big a problem” these kinds of asymmetries are. All of the
examples of asymmetry considered here arise when the set
of
values
has visited overlaps with those that
has
visited. Given such overlap, and certain properties of how the
algorithms generated the overlap, asymmetry arises. A precise
speciﬁcation of those “certain properties” is not yet in hand.
Nor is it known how generic they are, i.e., for what percentage
of pairs of algorithms they arise. Although such issues are
easy to state (see Appendix F), it is not at all clear how best
to answer them.
Consider, however, the case where we are assured that,
in
steps, the samples of two particular algorithms have
not overlapped. Such assurances hold, for example, if we are
comparing two hill-climbing algorithms that start far apart (on
the scale of
) in
. It turns out that given such assurances,
there are no asymmetries between the two algorithms for
-
element samples. To see this formally, go through the argument
used to prove the NFL theorem, but apply that argument to
the quantity
rather than
. Doing this establishes the following theorem.
Theorem 9: If there is no overlap between
and
,
then
An immediate consequence of this theorem is that under
the no-overlap conditions, the quantity
is symmetric under interchange of
and
, as

----- Page 10 (native) -----
76
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
are all distributions determined from this one over
and
(e.g., the distribution over the difference between those
’s
extrema).
Note that with stochastic algorithms, if they give nonzero
probability to all
, there is always overlap to consider.
So there is always the possibility of asymmetry between
algorithms if one of them is stochastic.
VII.
-INDEPENDENT RESULTS
All work to this point has largely considered the behavior
of various algorithms across a wide range of problems. In this
section we introduce the kinds of results that can be obtained
when we reverse roles and consider the properties of many
algorithms on a single problem. More results of this type are
found in [5]. The results of this section, although less sweeping
than the NFL results, hold no matter what the real world’s
distribution over cost functions is.
Let
and
be two search algorithms. Deﬁne a “choosing
procedure” as a rule that examines the samples
and
, produced by
and
, respectively, and based on those
samples, decides to use either
or
for the subsequent part of
the search. As an example, one “rational” choosing procedure
is to use
for the subsequent part of the search if and only if
it has generated a lower cost value in its sample than has
.
Conversely we can consider an “irrational” choosing procedure
that uses the algorithm that had not generated the sample with
the lowest cost solution.
At the point that a choosing procedure takes effect, the
cost function will have been sampled at
.
Accordingly, if
refers to the samples of the cost function
that come after using the choosing algorithm, then the user is
interested in the remaining sample
. As always, without
loss of generality, it is assumed that the search algorithm
selected by the choosing procedure does not return to any
points in
.5
The following theorem, proven in Appendix G, establishes
that there is no a priori justiﬁcation for using any particular
choosing procedure. Loosely speaking, no matter what the
cost function, without special consideration of the algorithm
at hand, simply observing how well that algorithm has done
so far tells us nothing a priori about how well it would
do if we continue to use it on the same cost function. For
simplicity, in stating the result we only consider deterministic
algorithms.
Theorem 10: Let
and
be two ﬁxed samples of
size
, that are generated when the algorithms
and
,
respectively, are run on the (arbitrary) cost function at hand.
Let
and
be two different choosing procedures. Let
be
5a can know to avoid the elements it has seen before. However a priori, a
has no way to avoid the elements observed by a0 has (and vice-versa). Rather
than have the deﬁnition of a somehow depend on the elements in d0   d
(and similarly for a0), we deal with this problem by deﬁning c>m to be set
only by those elements in d>m that lie outside of d[. (This is similar to the
convention we exploited above to deal with potentially retracing algorithms.)
Formally, this means that the random variable c>m is a function of d[ as
well as of d>m. It also means there may be fewer elements in the histogram
c>m than there are in the sample d>m.
the number of elements in
. Then
Implicit in this result is the assumption that the sum excludes
those algorithms
and
that do not result in
and
respectively when run on
.
In the precise form it is presented above, the result may
appear misleading, since it treats all samples equally, when
for any given
some samples will be more likely than others.
Even if one weights samples according to their probability
of occurrence, however, it is still true that, on average, the
choosing procedure one uses has no effect on likely
. This
is established by the following result, proven in Appendix H.
Theorem 11: Under the conditions given in the preceding
theorem
These results show that no assumption for
alone
justiﬁes using some choosing procedure as far as subsequent
search is concerned. To have an intelligent choosing procedure,
one must take into account not only
but also the search
algorithms one is choosing among. This conclusion may be
surprising. In particular, note that it means that there is no
intrinsic advantage to using a rational choosing procedure,
which continues with the better of
and
, rather than using
an irrational choosing procedure which does the opposite.
These results also have interesting implications for degen-
erate choosing procedures
always use algorithm
and
always use algorithm
. As applied to this case, they
mean that for ﬁxed
and
, if
does better (on average)
with the algorithms in some set
, then
does better (on
average) with the algorithms in the set of all other algorithms.
In particular, if for some favorite algorithms a certain “well-
behaved”
results in better performance than does the random
, then that well-behaved
gives worse than random behavior
on the set all remaining algorithms. In this sense, just as there
are no universally efﬁcacious search algorithms, there are no
universally benign
which can be assured of resulting in better
than random performance regardless of one’s algorithm.
In fact, things may very well be worse than this. In super-
vised learning, there is a related result [11]. Translated into
the current context, that result suggests that if one restricts
sums to only be over those algorithms that are a good match
to
, then it is often the case that “stupid” choosing
procedures—like the irrational procedure of choosing the
algorithm with the less desirable
—outperform “intelligent”
ones. What the set of algorithms summed over must be in
order for a rational choosing procedure to be superior to an
irrational procedure is not currently known.

----- Page 11 (native) -----
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
77
VIII. CONCLUSIONS
A framework has been presented in which to compare
general-purpose optimization algorithms. A number of NFL
theorems were derived that demonstrate the danger of com-
paring algorithms by their performance on a small sample of
problems. These same results also indicate the importance of
incorporating problem-speciﬁc knowledge into the behavior of
the algorithm. A geometric interpretation was given showing
what it means for an algorithm to be well suited to solving
a certain class of problems. The geometric perspective also
suggests a number of measures to compare the similarity of
various optimization algorithms.
More direct calculational applications of the NFL theo-
rem were demonstrated by investigating certain information-
theoretic aspects of search, as well as by developing a number
of benchmark measures of algorithm performance. These
benchmark measures should prove useful in practice.
We provided an analysis of the ways that algorithms can
differ a priori despite the NFL theorems. We have also
provided an introduction to a variant of the framework that
focuses on the behavior of a range of algorithms on spe-
ciﬁc problems (rather than speciﬁc algorithms over a range
of problems). This variant leads directly to reconsideration
of many issues addressed by computational complexity, as
detailed in [5].
Much future work clearly remains. Most important is the
development of practical applications of these ideas. Can the
geometric viewpoint be used to construct new optimization
techniques in practice? We believe the answer to be yes. At
a minimum, as Markov random ﬁeld models of landscapes
become more wide spread, the approach embodied in this
paper should ﬁnd wider applicability.
APPENDIX A
NFL PROOF FOR STATIC COST FUNCTIONS
We show that
has no dependence on
. Conceptually, the proof is quite simple but necessary
bookkeeping complicates things, lengthening the proof con-
siderably. The intuition behind the proof is straightforward:
by summing over all
we ensure that the past perfor-
mance of an algorithm has no bearing on its future per-
formance. Accordingly, under such a sum, all algorithms
perform equally.
The proof is by induction. The induction is based on
, and the inductive step is based on breaking
into
two independent parts, one for
and one for
.
These are evaluated separately, giving the desired result.
For
, we write the ﬁrst sample as
where
is set by
. The only possible value for
is
,
so we have
where
is the Kronecker delta function.
Summing over all possible cost functions,
is
one only for those functions which have cost
at point
.
Therefore that sum equals
, independent of
which is independent of
. This bases the induction.
The inductive step requires that if
is
independent of
for all
, then so also is
. Establishing this step completes the proof.
We begin by writing
and thus
The new
value,
, will depend on the new
value,
, and nothing else. So we expand over these possible
values, obtaining
Next note that since
, it does not depend
directly on
. Consequently we expand in
to remove the
dependence in
where use was made of the fact that
and the fact that
.
The sum over cost functions
is done ﬁrst. The cost
function is deﬁned both over those points restricted to
and
those points outside of
will depend on the
values deﬁned over points inside
while

----- Page 12 (native) -----
78
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
depends only on the
values deﬁned over
points outside
. (Recall that
.) So we have
(6)
The sum
contributes a constant,
,
equal to the number of functions deﬁned over points not in
passing through
. So
By hypothesis, the right-hand side of this equation is indepen-
dent of
, so the left-hand side must also be. This completes
the proof.
APPENDIX B
NFL PROOF FOR TIME-DEPENDENT COST FUNCTIONS
In analogy with the proof of the static NFL theorem, the
proof for the time-dependent case proceeds by establishing the
-independence of the sum
, where here
is either
or
.
To begin, replace each
in this sum with a set of cost
functions,
, one for each iteration of the algorithm. To do
this, we start with the following:
where the sequence of cost functions,
, has been indicated
by the vector
. In the next step, the sum over
all possible
is decomposed into a series of sums. Each sum
in the series is over the values
can take for one particular
iteration of the algorithm. More formally, using
,
we write
Note that
is independent of the values
of
, so those values can be absorbed into an overall
-independent proportionality constant.
Consider the innermost sum over
, for ﬁxed values of
the outer sum indexes
. For ﬁxed values of the
outer indexes,
is just a particular
ﬁxed cost function. Accordingly, the innermost sum over
is simply the number of bijections of
that map that
ﬁxed cost function to
. This is the constant,
.
Consequently, evaluating the
sum yields
The sum over
can be accomplished in the same manner
is summed over. In fact, all the sums over all
can
be done, leaving
(7)
In this last step, the statistical independence of
and
has
been used.
Further progress depends on whether
represents
or
. We begin with analysis of the
case. For this case
, since
only reﬂects cost
values from the last cost function,
. Using this result gives
The ﬁnal sum over
is a constant equal to the number of
ways of generating the sample
from cost values drawn
from
. The important point is that it is independent of
the particular
. Because of this the sum over
can be
evaluated eliminating the
dependence
This completes the proof of Theorem 2 for the case of
.
The proof of Theorem 2 is completed by turning to the
case. This is considerably more difﬁcult since
cannot be simpliﬁed so that the sums over
cannot be

----- Page 13 (native) -----
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
79
decoupled. Nevertheless, the NFL result still holds. This is
proven by expanding (7) over possible
values
(8)
The innermost sum over
only has an effect on the
,
term so it contributes
,
. This is a constant, equal to
. This
leaves
The sum over
is now simple
The above equation is of the same form as (8), only with a
remaining sample of size
rather than
. Consequently, in
an analogous manner to the scheme used to evaluate the sums
over
and
that existed in (8), the sums over
and
can be evaluated. Doing so simply generates
more
-independent proportionality constants. Continuing in
this manner, all sums over the
can be evaluated to ﬁnd
There is algorithm dependence in this result, but it is the
trivial dependence discussed previously. It arises from how
the algorithm selects the ﬁrst
point in its sample,
.
Restricting interest to those points in the sample that are gen-
erated subsequent to the ﬁrst, this result shows that there are no
distinctions between algorithms. Alternatively, summing over
the initial cost function
, all points in the sample could be
considered while still retaining an NFL result.
APPENDIX C
PROOF OF
RESULT
As noted in the discussion leading up to Theorem 3, the
fraction of functions giving a speciﬁed histogram
is independent of the algorithm. Consequently, a simple algo-
rithm is used to prove the theorem. The algorithm visits points
in
in some canonical order, say
. Recall
that the histogram
is speciﬁed by giving the frequencies
of occurrence, across the
, for each of the
possible cost values. The number of
’s giving the desired
histogram under this algorithm is just the multinomial giving
the number of ways of distributing the cost values in . At the
remaining
points in
the cost can assume any of
the
values giving the ﬁrst result of Theorem 3.
The expression of
in terms of the entropy of
follows from an application of Stirling’s approximation to
order
, which is valid when all of the
are large.
In this case the multinomial is written
from which the theorem follows by exponentiating this result.
APPENDIX D
PROOF OF
RESULT
In this section the proportion of all algorithms that give a
particular
for a particular
is calculated. The calculation
proceeds in several steps
Since
is ﬁnite there are a ﬁnite number of different
samples. Therefore any (deterministic)
is a huge, but ﬁnite,
list indexed by all possible
’s. Each entry in the list is the
that the
in question outputs for that
-index.
Consider any particular unordered set of
pairs
where no two of the pairs share the same
value. Such a
set is called an unordered path
. Without loss of generality,
from now on we implicitly restrict the discussion to unordered
paths of length
. A particular
is in or from a particular
if there is a unordered set of
pairs identical to
.
The numerator on the right-hand side of (3) is the number of
unordered paths in the given
that give the desired .
The number of unordered paths in
that give the desired
, the numerator on the right-hand side of (3), is proportional
to the number of
’s that give the desired
for
and the
proof of this claim constitutes a proof of (3). Furthermore, the
proportionality constant is independent of
and .
Proof: The proof is established by constructing a map-
ping
taking in an
that gives the desired
for
, and producing a
that is in
and gives the desired
.
Showing that for any
the number of algorithms
such that

----- Page 14 (native) -----
80
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
is a constant, independent of
, and
. and that
is single valued will complete the proof.
Recalling that every
value in an unordered path is distinct,
any unordered path
gives a set of
different ordered paths.
Each such ordered path
in turn provides a set of
successive
’s (if the empty
is included) and a following
. Indicate by
this set of the ﬁrst
’s provided by
.
From any ordered path
a “partial algorithm” can be
constructed. This consists of the list of an
, but with only the
entries in the list ﬁlled in, the remaining entries are
blank. Since there are
distinct partial
’s for each
(one
for each ordered path corresponding to
), there are
such
partially ﬁlled-in lists for each
. A partial algorithm may or
may not be consistent with a particular full algorithm. This
allows the deﬁnition of the inverse of
: for any
that is in
and gives
(the set of all
that are consistent
with at least one partial algorithm generated from
and that
give
when run on
).
To complete the ﬁrst part of the proof, it must be shown that
for all
that are in
and give
contains the same
number of elements, regardless of
, or . To that end, ﬁrst
generate all ordered paths induced by
and then associate each
such ordered path with a distinct
-element partial algorithm.
Now how many full algorithm lists are consistent with at least
one of these partial algorithm partial lists? How this question is
answered is the core of this appendix. To answer this question,
reorder the entries in each of the partial algorithm lists by
permuting the indexes
of all the lists. Obviously such a
reordering will not change the answer to our question.
Reordering is accomplished by interchanging pairs of
indexes. First, interchange any
index of the form
whose
entry is ﬁlled in any of our partial algorithm lists with
, where
is some
arbitrary constant
value and
refers to the
th element
of
. Next, create some arbitrary but ﬁxed ordering of
all
: (
. Then interchange any
index
of the form
whose entry
is ﬁlled in any of our (new) partial algorithm lists with
. Recall that all the
must
be distinct. By construction, the resultant partial algorithm lists
are independent of
and
, as is the number of such lists
(it is
). Therefore the number of algorithms consistent with
at least one partial algorithm list in
is independent of
and
. This completes the ﬁrst part of the proof.
For the second part, ﬁrst choose any two unordered paths
that differ from one another,
and
. There is no ordered
path
constructed from
that equals an ordered path
constructed from
. So choose any such
and any such
. If they disagree for the null
, then we know that there
is no (deterministic)
that agrees with both of them. If they
agree for the null , then since they are sampled from the same
, they have the same single-element
. If they disagree for
that
, then there is no
that agrees with both of them. If they
agree for that
, then they have the same double-element
.
Continue in this manner all the up to the
-element
.
Since the two ordered paths differ, they must have disagreed
at some point by now, and therefore there is no
that agrees
with both of them. Since this is true for any
from
and
any
from
, we see that there is no
in
that is
also in
. This completes the proof.
To show the relation to the Kullback–Liebler distance the
product of binomials is expanded with the aid of Stirling’s
approximation when both
and
are large
It has been assumed that
, which is reasonable when
. Expanding
, to second
order gives
Using
then in terms of
and
one ﬁnds
where
is the Kullback–Liebler
distance between the distributions
and
. Exponentiating
this expression yields the second result in Theorem 4.
APPENDIX E
BENCHMARK MEASURES OF PERFORMANCE
The result for each benchmark measure is established in
turn.
The ﬁrst measure is
. Consider
(9)
for which the summand equals zero or one for all
and
deterministic
. It is one only if
i)
ii)
iii)
and so on. These restrictions will ﬁx the value of
at
points while
remains free at all other points. Therefore

----- Page 15 (native) -----
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZATION
81
Using this result in (9) we ﬁnd
which is the result quoted in Theorem 5.
In the limit as
gets large write
and substitute in for
. Replacing
with
turns the sum into
. Next, write
for some
and multiply and divide the summand by
. Since
then
. To take the limit of
, apply L’hopital’s rule
to the ratio in the summand. Next use the fact that
is going
to zero to cancel terms in the summand. Carrying through the
algebra and dividing by
, we get a Riemann sum of the
form
. Evaluating the integral gives
the second result in Theorem 5.
The second benchmark concerns the behavior of the random
algorithm. Summing over the
values of different histograms
, the performance of
is
Now
is the probability of obtaining histogram
in
random draws from the histogram
of the function
. This can be viewed as the deﬁnition of
. This probability
has been calculated previously as
. So
which is (4) of Theorem 6.
APPENDIX F
PROOF RELATED TO MINIMAX
DISTINCTIONS BETWEEN ALGORITHMS
This proof is by example. Consider three points in
, and
, and three points in
, and
.
1) Let the ﬁrst point
visits be
and the ﬁrst point
visits be
.
2) If at its ﬁrst point
sees a
or a
, it jumps to
.
Otherwise it jumps to
.
3) If at its ﬁrst point
sees a
, it jumps to
. If it sees
a
, it jumps to
.
Consider the cost function that has as the
values for the
three
values
, respectively.
For
will produce a sample
for this
function, and
will produce
.
The proof is completed if we show that there is no cost
function so that
produces a sample containing
and
and such that
produces a sample containing
and
.
There are four possible pairs of samples to consider:
i)
;
ii)
;
iii)
;
iv)
.
Since if its ﬁrst point is a
,
jumps to
which is where
starts, when
’s ﬁrst point is a
its second point must
equal
’s ﬁrst point. This rules out possibilities i) and ii).
For possibilities iii) and iv), by
’s sample we know that
must be of the form
, for some variable
. For
case iii),
would need to equal
, due to the ﬁrst point in
’s sample. For that case, however, the second point
sees
would be the value at
, which is
, contrary to hypothesis.
For case iv), we know that the
would have to equal
, due
to the ﬁrst point in
’s sample. That would mean, however,
that
jumps to
for its second point and would therefore
see a
, contrary to hypothesis.
Accordingly, none of the four cases is possible. This is
a case both where there is no symmetry under exchange of
’s between
and
, and no symmetry under exchange of
histograms.
APPENDIX G
FIXED COST FUNCTIONS AND CHOOSING PROCEDURES
Since any deterministic search algorithm is a mapping from
to
, any search algorithm is a vector in the
space
. The components of such a vector are indexed by
the possible samples, and the value for each component is the
that the algorithm produces given the associated sample.
Consider now a particular sample
of size
. Given
,
we can say whether or not any other sample of size greater
than
has the (ordered) elements of
as its ﬁrst
(ordered)
elements. The set of those samples that do start with
this
way deﬁnes a set of components of any algorithm vector
.
Those components will be indicated by
.
The remaining components of
are of two types. The ﬁrst is
given by those samples that are equivalent to the ﬁrst
elements in
for some
. The values of those components
for the vector algorithm
will be indicated by
. The
second type consists of those components corresponding to
all remaining samples. Intuitively, these are samples that are
not compatible with
. Some examples of such samples are
those that contain as one of their ﬁrst
elements an element
not found in
, and samples that re-order the elements found
in
. The values of
for components of this second type will
be indicated by
.

----- Page 16 (native) -----
82
IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 1, NO. 1, APRIL 1997
Let
represent either
or
. We are interested in
The summand is independent of the values of
and
for either of our two
’s. In addition, the number of such
values is a constant. (It is given by the product, over all
samples not consistent with
, of the number of possible
each such sample could be mapped to.) Therefore, up to an
overall constant independent of
, and
, the sum
equals
By deﬁnition, we are implicitly restricting the sum to those
and
so that our summand is deﬁned. This means that
we actually only allow one value for each component in
(namely, the value that gives the next
element in
) and
similarly for
. Therefore the sum reduces to
Note that no component of
lies in
. The same is true
of
. So the sum over
is over the same components
of
as the sum over
is of
. Now for ﬁxed
and
,
’s choice of
or
is ﬁxed. Accordingly, without loss of
generality, the sum can be rewritten as
with the implicit assumption that
is set by
. This sum
is independent of
.
APPENDIX H
PROOF OF THEOREM 11
Let
refer to a choosing procedure. We are interested in
The sum over
and
can be moved outside the sum
over
and
. Consider any term in that sum (i.e.,
any particular pair of values of
and
). For that
term,
is
just
one
for
those
and
that result in
and
, respectively, when
run on
, and zero otherwise. (Recall the assumption
that
and
are deterministic.) This means that the
factor simply restricts our sum
over
and
to the
and
considered in our theorem.
Accordingly, our theorem tell us that the summand of the sum
over
and
is the same for choosing procedures
and
.
Therefore the full sum is the same for both procedures.
REFERENCES
[1] L. J. Fogel, A. J. Owens, and M. J. Walsh, Artiﬁcial Intelligence Through
Simulated Evolution.
New York: Wiley, 1966.
[2] J. H. Holland, Adaptation in Natural and Artiﬁcial Systems.
Cam-
bridge, MA: MIT Press, 1993.
[3] H.-P. Schwefel, Evolution and Optimum Seeking.
New York: Wiley,
1995.
[4] S. Kirkpatrick, D. C. Gelatt, and M. P. Vecchi, “Optimization by
simulated annealing,” Science, vol. 220, pp. 671–680, 1983.
[5] W. G. Macready and D. H. Wolpert, “What makes an optimization
problem hard?” Complexity, vol. 5, pp. 40–46, 1996.
[6] D. H. Wolpert and W. G. Macready, “No free lunch theorems for
search,” Santa Fe Institute, Sante Fe, NM, Tech. Rep. SFI-TR-05-010,
1995.
[7] F. Glover, “Tabu search I,” ORSA J. Comput., vol. 1, pp. 190–206, 1989.
[8]
, “Tabu search II,” ORSA J. Comput., vol. 2, pp. 4–32, 1990
[9] E. L. Lawler and D. E. Wood, “Branch and bound methods: A survey,”
Oper. Res., vol 14, pp. 699–719, 1966.
[10] R. Kinderman and J. L. Snell, Markov Random Fields and Their
Applications.
Providence, RI: Amer. Math. Soc., 1980.
[11] D. H. Wolpert, “The lack of a prior distinctions between learning
algorithms,” Neural Computation, vol. 8, pp. 1341–1390, 1996.
[12]
, “On bias plus variance,” Neural Computation, vol. 9, pp.
1271–1248, 1996.
[13] D. Griffeath, “Introduction to random ﬁelds,” in Denumerable Markov
Chains, J. G. Kemeny, J. L. Snell, and A. W. Knapp, Eds.
New York:
Springer-Verlag, 1976.
[14] C. E. M. Strauss, D. H. Wolpert, and D. R. Wolf, “Alpha, evidence,
and the entropic prior,” in Maximum Entropy and Bayesian Methods.
Reading, MA: Addison-Wesley, 1992, pp. 113–120.
[15] T. M. Cover and J. A. Thomas, Elements of Information Theory.
New
York: Wiley, 1991.
ACKNOWLEDGMENT
The authors would like to thank R. Das, D. Fogel, T.
Grossman, P. Helman, B. Levitan, U.-M. O’Reilly, and the
reviewers for helpful comments and suggestions.
David H. Wolpert received degrees in physics from
the University of California, Santa Barbara, and
Princeton University, Princeton, NJ.
He was formerly Director of Research at TXN Inc
and a Postdoctoral Fellow at the Santa Fe Institute.
He now heads up a data mining group at IBM Al-
maden Research Center, San Jose, CA. Most of his
work centers around supervised learning, Bayesian
analysis, and the thermodynamics of computation.
William G. Macready received the Ph.D. degree in
physics at the University of Toronto, Ont., Canada.
His doctoral work was on high-temperature super-
conductivity.
He recently completed a postdoctoral fellowship
at the Santa Fe Institute and is now at IBM’s
Almaden Research Center, San Jose, CA. His recent
work focuses on probabilistic approaches to ma-
chine learning and optimization, critical phenomena
in combinatorial optimization, and the design of
efﬁcient optimization algorithms.