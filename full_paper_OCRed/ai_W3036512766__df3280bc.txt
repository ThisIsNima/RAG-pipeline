

----- Page 1 (native) -----
Springer Series in Information Sciences 
8

----- Page 2 (native) -----
Springer Series in Information Sciences 
Editors: Thomas S. Huang 
Teuvo Kohonen 
Manfred R. Schroeder 
Managing Editor: 
H.K. V. Lotsch 
Content-Addressable Memories 
By T. Kohonen 2nd Edition 
2 Fast Fourier Transform and 
Convolution Algorithms 
By H. J. Nussbaumer 2nd Edition 
3 Pitch Determination of Speech Signals 
Algorithms and Devices By W. Hess 
4 Pattern Analysis By H. Niemann 
5 Image Sequence Analysis 
Editor: T.S. Huang 
6 Picture Engineering 
Editors: King-sun Fu and T.L. Kunii 
7 Number Theory in Science and 
Communication With Applications in 
Cryptography, Physics, Digital 
Information, Computing, and Self-
Similarity By M.R. Schroeder 
2nd Edition 
8 Self-Organization and Associative 
Memory By T. Kohonen 3rd Edition 
9 Digital Picture Processing 
An Introduction By L.P. Yaroslavsky 
10 Probability, Statistical Optics and 
Data Testing 
A Problem Solving 
Approach By B.R. Frieden 
11 Physical and Biological Processing of 
Images Editors: O.J. Braddick and 
A.C. Sleigh 
12 MuItiresolution Image Processing and 
Analysis Editor: A. Rosenfeld 
13 VLSI for Pattern Recognition and 
Image Processing Editor: King-sun Fu 
14 Mathematics of Kalman-Bucy Filtering 
By P.A. Ruymgaart and T.T. Soong 
2nd Edition 
15 Fundamentals of Electronic Imaging 
Systems Some Aspects of Image 
Processing By WF. Schreiber 
16 Radon and Projection Transform-
Based Computer Vision Algorithms, 
A Pipeline Architecture, and Industrial 
Applications By J.L.c. Sanz, 
E.B. Hinkle, and A.K. Jain 
17 Kalman Filtering with Real-Time 
Applications By C.K. Chui and G. Chen 
18 Linear Systems and Optimal Control 
By C.K. Chui and G. Chen 
19 Harmony: A Psychoacoustical 
Approach By R. Parncutt 
20 Group Theoretical Methods in Image 
Understanding By Ken-ichi Kanatani 
21 Linear Prediction Theory 
A Mathematical Basis for Adaptive 
Systems By P. Strobach 
22 Optical Signal Processing 
ByP.K. Das

----- Page 3 (native) -----
Teuvo Kohonen 
Self-Organization and 
Associative Memory 
Third Edition 
With 100 Figures 
Springer-Verlag Berlin Heidelberg New York 
London Paris Tokyo Hong Kong

----- Page 4 (native) -----
Professor Teuvo Kohonen 
Laboratory of Computer and Information Sciences, Helsinki University of Technology 
SF-02150 Espoo 15, Finland 
Series Editors: 
Professor Thomas S. Huang 
Department of Electrical Engineering and Coordinated Science Laboratory, 
University of Illinois, Urbana, IL 61801, USA 
Professor Teuvo Kohonen 
Laboratory of Computer and Information Sciences, Helsinki University of Technology 
SF-02150 Espoo 15, Finland 
Professor Dr. Manfred R. Schroeder 
Drittes Physikalisches Institut, Universitat Gottingen, Biirgerstrasse 42-44, 
D-3400 Gottingen, Fed. Rep. of Germany 
Managing Editor: Helmut K. V. Lotsch 
Springer-Verlag, Tiergartenstrasse 17, 
D-6900 Heidelberg, Fed. Rep. of Germany 
ISBN 978-3-540-51387-2 
ISBN 978-3-642-88163-3 (eBook) 
DOl 10.1007/978-3-642-88163-3 
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, 
specifically the rights of translation, reprinting, re·use of illustrations, recitation, broadcasting, reproduction on 
microfilms or in other ways, and storage in data banks. Duplication of this publication or parts thereof is only 
permitted under the provisions of the German Copyright Law of September 9, 1965. in its version of June 24, 
1985, and a copyright fee must always be paid. Violations fall under the prosecution act ofthe German Copyright 
Law. 
© Springer· Verlag Berlin Heidelberg 1984. 1988 and 1989 
The use of registered names. trademarks, etc. in this publication does not imply even in the absence of a specific 
statement. that such names are exempt from the relevant protective laws and regulations and therefore free for 
general use. 
Typesetting: K & V Fotosatz. 6124 Beerfelden 
2154/3150-543210 - Printed on acid-free paper

----- Page 5 (native) -----
Preface to the Third Edition 
While the present edition is bibliographically the third one of Vol. 8 of the 
Springer Series in Information Sciences (IS 8), the book actually stems from 
Vol. 17 of the series Communication and Cybernetics (CC 17), entitled 
Associative Memory -
A System-Theoretical Approach, which appeared 
in 1977. That book was the first monograph on distributed associative 
memories, or "content-addressable memories" as they are frequently called, 
especially in neural-networks research. This author, however, would like to 
reserve the term "content-addressable memory" for certain more traditional 
constructs, the memory locations of which are selected by parallel search. 
Such devices are discussed in Vol. 1 of the Springer Series in Information 
Sciences, Content-Addressable Memories. 
This third edition of IS 8 is rather similar to the second one. Two new 
discussions have been added: one to the end of Chap. 5, and the other (the 
L VQ 2 algorithm) to the end of Chap. 7. Moreover, the convergence proof in 
Sect. 5.7.2 has been revised. 
Otaniemi, Finland 
May 1989 
T. Kohonen

----- Page 6 (native) -----
Preface to the Second Edition 
Two significant things have happened since the writing of the first edition in 
1983. One of them is recent arousal of strong interest in general aspects of 
"neural computing", or "neural networks", as the previous neural models are 
nowadays called. The incentive, of course, has been to develop new com-
puters. Especially it may have been felt that the so-called fifth-generation 
computers, based on conventional logic programming, do not yet contain in-
formation processing principles of the same type as those encountered in the 
brain. All new ideas for the "neural computers" are, of course, welcome. On 
the other hand, it is not very easy to see what kind of restrictions there exist to 
their implementation. In order to approach this problem systematically, cer-
tain lines of thought, disciplines, and criteria should be followed. It is the pur-
pose of the added Chapter 9 to reflect upon such problems from a general 
point of view. 
Another important thing is a boom of new hardware technologies for dis-
tributed associative memories, especially high-density semiconductor circuits, 
and optical materials and components. The era is very close when the parallel 
processors can be made all-optical. Several working associative memory archi-
tectures, based solely on optical technologies, have been constructed in recent 
years. For this reason it was felt necessary to include a separate chapter 
(Chap. 10) which deals with the optical associative memories. Part of its con-
tents is taken over from the first edition. 
The following new items have been included in this edition, too: more 
accurate measures for symbol strings (Sect. 2.2.3), and the Learning Vector 
Quantization for pattern recognition (Sect. 7.5). In addition, I have made a 
few other revisions to the text. 
I would like to emphasize that this monograph only concentrates on a 
restricted area of neural computing, namely, different aspects of memory, in 
particular associative memory. It may not be justified to expect that the 
models which have been set up to illustrate the memory functions will solve 
all the practical problems connected with neural computing. Nonetheless, 
memory seems to playa rather central role in thinking, as well as in sensory 
perception. 
I am very much obliged to Mrs. Leila Koivisto for her invaluable help in 
making this extensive revision. 
Otaniemi, Finland 
August 1987 
T. Kohonen

----- Page 7 (native) -----
Preface to the First Edition 
A couple of years ago the Publisher and I agreed upon a revision of Associa-
tive Memory -
A System-Theoretical Approach (Springer Series in Com-
munication and Cybernetics, CC 17). It turned out that this field had grown 
rather rapidly. On the other hand, there were some classical publications 
which had motivated newer works and which, accordingly, should have been 
reviewed in the context of present knowledge. It was therefore felt that CC 17 
should completely be reorganized to embed both classical and newer results in 
the same formalism. 
The most significant contribution of this work with respect to the earlier 
book, however, is that while CC 17 concentrated on the principles by which a 
distributed associative memory is implementable, the present book also tries 
to describe how an adaptive physical system is able to automatically form re-
duced representations of input information, or to "encode" it before storing 
it. Both of these aspects are, of course, essential to the complete explanation 
of memory. 
Although the scope is now much wider than earlier, it was felt unnecessary 
to alter some rather independent parts of the old edition. Sections 2.1, 6.1 - 7, 
7.1, 2, and 8.1 can be recognized as similar to the corresponding sections of 
CC 17, except for some editing and reorganization. On the other hand, about 
2/3 of the present contents are completely new. 
The book now concentrates on principles and mechanisms of memory and 
learning by which certain elementary "intelligent" functions are formed adap-
tively, without externally given control information, by the effect of received 
signals solely. A significant restriction to the present discussion is set by the 
stipulated property that the systems underlying these principles must be 
physical; accordingly, the basic components cannot implement arbitrary 
arithmetic algorithms although this would be very easy to define even by the 
simplest computer programs. The signal transformations must be as simple as 
possible, and the changes in the system variables and parameters must be con-
tinuous, smooth functions of time. This clearly distinguishes the present ideas 
from the conventional Artificial Intelligence approaches which are totally 
dependent on the use of digital computers and their high-level programming 
languages. 
It is frequently believed that it is impossible to implement higher informa-
tion processes without components the characteristics of which are very 
nonlinear. It may thereby be thought that all decision processes must be based 
on inherently nonlinear operations. If, however, the system properties are 
time-variable, then this requirement can be alleviated. In fact, even a linear

----- Page 8 (native) -----
VIII 
Preface to the First Edition 
system with time-variable parameter values already behaves in a nonlinear way. 
Another remark is that although nonlinearities are needed in some places for 
decision operations, it is not mandatory that every component and elementary 
processing operation be nonlinear; there are many functions, especially those 
performing statistical averaging of signals, which best can be realized by linear 
circuits. We shall revert to this argumentation in Chapter 4. Finally it may be 
remarked that if nonlinearities are needed, they may best be included in more or 
less fIxed preprocessing circuits, especially on the sensory level. 
The early phases of Artificial Intelligence research around 1960 were char-
acterized by an enthusiastic attempt to implement learning functions, i.e., a 
kind of elementary intelligence, using networks built of simple adaptive com-
ponents. In spite of initial optimistic expectations, progress was never par-
ticularly rapid which led to an almost opposite reaction; until quite recent 
years, very few researchers believed in the future of such principles. My per-
sonal view is that the fIrst modelling approaches to learning machines were 
basically sound; however, at least a couple of important properties were mis-
sing from the models. One of them is a genuine memory function, especially 
associative memory which can reconstruct complete representations such as 
images and signal patterns from their fragments or other cues; and the second 
flaw of the early models was that the importance of the spatial order of the 
processing units was never fully realized. It seems, however, that in the biolog-
ical brains a significant amount of information representation is encoded in 
the spatial location of the processing elements. We shall later see in Chapter 5 
that a meaningful spatial encoding can result in a simple self-organizing 
physical process which uses similar components to those applied in the early 
learning machines; the only new feature to be introduced is a characteristic 
local interaction between neighbouring elements. 
Associative memory is a very delicate and complex concept which often 
has been attributed to the higher cognitive processes, especially those taking 
place in the human brain. A statement of this concept can be traced back to 
the empiricist philosophers of the 16th century who, again, inherited their 
views from Aristotle (384 - 322 B.C.). It is nowadays a big challenge to launch 
a discussion on associative memory since its characteristics and nature have 
been understood in widely different ways by different scientists. Perhaps the 
most high-browed one is the approach made in psycholinguistics where the 
aim is to relate conceptual items structurally to produce bases of knowledge. 
Another specialized view is held in computer engineering where, traditionally, 
associative memory is identified with certain searching methods named con-
tent-addressing. The only task has thereby been to locate a data set on the 
basis of a matching portion in a keyword. Between these extremal concep-
tions, there is a continuum of various associative memory paradigms.

----- Page 9 (native) -----
Preface to the First Edition 
IX 
The contents of this book may be seen as a progression, starting from a 
systematic analysis of the most natural basic units, and ending with the inter-
nal representations and associative memory. Thus the main purpose of this 
representation is to demonstrate the gradual evolution of intelligent functions 
in physical systems, and to reveal those conditions under which a passive 
memory medium switches over into an active system that is able to form 
meaningful compressed representations of its input data, i.e., abstractions 
and generalizations which are often believed to be the basic constituents of 
intelligence. 
Some material presented in this book has resulted from collaboration with 
my colleagues Pekka Lehtio and Erkki Oja, to whom I am very much obliged. 
Several pictures relating to computer simulations have been prepared by Kai 
Makisara. Thanks are also due to Eija Dower for typing the manuscript. 
This work has been done under the auspices of the Academy of Finland. 
Otaniemi, Finland 
August 1983 
T. Kohonen

----- Page 10 (native) -----
Contents 
1. Various Aspects of Memory ................................. . 
1.1 On the Purpose and Nature of Biological Memory. . . . . . . . . . . . 
1 
1.1.1 Some Fundamental Concepts ....................... . 
1.1.2 The Classical Laws of Association .................... 
3 
1.1.3 On Different Levels of Modelling .................... 
4 
1.2 Questions Concerning the Fundamental Mechanisms of Memory 
4 
1.2.1 Where Do the Signals Relating to Memory Act Upon? .,. 
5 
1.2.2 What Kind of Encoding is Used for Neural Signals? ..... 
6 
1.2.3 What are the Variable Memory Elements? ............. 
7 
1.2.4 How are Neural Signals Addressed in Memory? ........ 
8 
1.3 Elementary Operations Implemented by Associative Memory .. 
14 
1.3.1 Associative Recall ................................. 
14 
1.3.2 Production of Sequences from the Associative Memory. . 
16 
1.3.3 On the Meaning of Background and Context ........... 
20 
1.4 More Abstract Aspects of Memory. . . . . . . . . . . . . . . . . . . . . . . . . 
21 
1.4.1 The Problem of Infinite-State Memory. . . . . . . . . . . . . . . . 
21 
1.4.2 Invariant Representations ... . . . . . . . . . . . . . . . . . . . . . . . . 
22 
1.4.3 Symbolic Representations. . . . . . . . . . . . . . . . . . . . . . . . . . . 
24 
1.4.4 Virtual Images .................................... 
25 
1.4.5 The Logic of Stored Knowledge ............... . . . . . . . 
27 
2. Pattern Mathematics ........................................ 
30 
2.1 Mathematical Notations and Methods .......... . . . . . . . . . . . . 
30 
2.1.1 Vector Space Concepts ............................. 
30 
2.1.2 Matrix Notations .................................. 
41 
2.1.3 Further Properties of Matrices ....................... 
44 
2.1.4 Matrix Equations .................................. 
48 
2.1.5 Projection Operators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
54 
2.1.6 On Matrix Differential Calculus . . . . . . . . . . . . . . . . . . . . . . 
57 
2.2 Distance Measures for Patterns. . . . . . . . . . . . . . . . . . . . . . . . . . . . 
59

----- Page 11 (native) -----
XII 
Contents 
2.2.1 Measures of Similarity and Distance in Vector Spaces . . . . 
59 
2.2.2 Measures of Similarity and Distance Between Symbol 
Strings ........................................... 
63 
2.2.3 More Accurate Distance Measures for Text ............ 
66 
3. Classical Learning Systems ................................... 
68 
3.1 The Adaptive Linear Element (Adaline) .................... 
69 
3.1.1 Description of Adaptation by the Stochastic 
Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
71 
3.2 The Percept ron ......................................... 
72 
3.3 The Learning Matrix .................................... 
74 
3.4 Physical Realization of Adaptive Weights. . . . . . . . . . . . . . . . . . . 
77 
3.4.1 Percept ron and Adaline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
77 
3.4.2 Classical Conditioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
78 
3.4.3 Conjunction Learning Switches ...................... 
80 
3.4.4 Digital Representation of Adaptive Circuits. . . . . . . . . . . . 
80 
3.4.5 Biological Components ............................. 
81 
4. A New Approach to Adaptive Filters. . . . . . . . . . . . . . . . . . . . . . . . . . . 
82 
4.1 Survey of Some Necessary Functions ....................... 
82 
4.2 On the "Transfer Function" of the Neuron ................ . . 
84 
4.3 Models for Basic Adaptive Units .......................... 
90 
4.3.1 On the Linearization of the Basic Unit ................ 
90 
4.3.2 Various Cases of Adaptation Laws ................... 
91 
4.3.3 Two Limit Theorems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
98 
4.3.4 The Novelty Detector. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 
101 
4.4 Adaptive Feedback Networks ............................. 104 
4.4.1 The Autocorrelation Matrix Memory ................. 105 
4.4.2 The Novelty Filter ................................. 109 
5. Self-Organizing Feature Maps ................................ 119 
5.1 On the Feature Maps of the Brain. . . . . . . . . . . . . . . . . . . . . . . . . . 
119 
5.2 Formation of Localized Responses by Lateral Feedback. . . . . .. 
122 
5.3 Computational Simplification of the Process ................ 127 
5.3.1 Definition of the Topology-Preserving Mapping ........ 127 
5.3.2 A Simple Two-Dimensional Self-Organizing System. . . .. 
130 
5.4 Demonstrations of Simple Topology-Preserving Mappings .... 
133 
5.4.1 Images of Various Distributions of Input Vectors ....... 
133 
5.4.2 "The Magic TV" .................................. 137 
5.4.3 Mapping by a Feeler Mechanism ..................... 139 
5.5 Tonotopic Map. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
140

----- Page 12 (native) -----
Contents 
XIII 
5.6 Formation of Hierarchical Representations ................. 
141 
5.6.1 Taxonomy Example ................................ 
141 
5.6.2 Phoneme Map .................................... 
142 
5.7 Mathematical Treatment of Self-Organization ............... 
143 
5.7.1 Ordering of Weights ............................... 
144 
5.7.2 Convergence Phase ................................ 
150 
5.8 Automatic Selection of Feature Dimensions ................. 
155 
6. Optimal Associative Mappings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
158 
6.1 Transfer Function of an Associative Network . . . . . . . . . . . . . . . . 
159 
6.2 Autoassociative Recall as an Orthogonal Projection .......... 
160 
6.2.1 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
160 
6.2.2 Error-Correcting Properties of Projections ............ 
161 
6.3 The Novelty Filter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
163 
6.3.1 Two Examples of Novelty Filter ...................... 
163 
6.3.2 Novelty Filter as an Autoassociative Memory .......... 
165 
6.4 Autoassociative Encoding ................................ 
165 
6.4.1 An Example of Autoassociative Encoding ............. 
166 
6.5 Optimal Associative Mappings ............................ 
167 
6.5.1 The Optimal Linear Associative Mapping. . . . . . . . . . . . . . 
168 
6.5.2 Optimal Nonlinear Associative Mappings. . . . . . . . . . . . . . 
172 
6.6 Relationship Between Associative Mapping, Linear Regression, 
and Linear Estimation ................................... 
175 
6.6.1 Relationship of the Associative Mapping to Linear 
Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
175 
6.6.2 Relationship of the Regression Solution to the Linear 
Estimator ........................................ 
176 
6.7 Recursive Computation of the Optimal Associative Mapping. . . 
177 
6.7.1 Linear Corrective Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 
178 
6.7.2 Best Exact Solution (Gradient Projection) ............. 
179 
6.7.3 Best Approximate Solution (Regression) . . . . . . . . . . . . . . . 
180 
6.7.4 Recursive Solution in the General Case .... . . . . . . . . . . . . 
182 
6.8 Special Cases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
183 
6.8.1 The Correlation Matrix Memory ..................... 
183 
6.8.2 Relationship Between Conditional Averages and Optimal 
Estimator ............................ ;........... 
184 
7. Pattern Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
185 
7.1 Discriminant Functions .................................. 
185 
7.2 Statistical Formulation of Pattern Classification ............. 
187 
7.3 Comparison Methods ........ " . . . . . . . . .. . . .. . .... . . . . . . . 
190

----- Page 13 (native) -----
XIV 
Contents 
7.4 The Subspace Methods of Classification .................... 192 
7.4.1 The Basic Subspace Method ......................... 192 
7.4.2 The Learning Subspace Method (LSM) . . . . . . . . . . . . . . . . 
193 
7.5 Learning Vector Quantization ............................. 199 
7.6 Feature Extraction ...................................... 202 
7.7 Clustering ............................................. 203 
7.7.1 Simple Clustering (Optimization Approach) ........... 204 
7.7.2 Hierarchical Clustering (Taxonomy Approach) ......... 205 
7.8 Structural Pattern Recognition Methods .................... 206 
8. More About Biological Memory .............................. 210 
8.1 Physiological Foundations of Memory ..................... 210 
8.1.1 On the Mechanisms of Memory in Biological Systems ... 
210 
8.1.2 Structural Features of Some Neural Networks .......... 213 
8.1.3 Functional Features of Neurons. . . . . . . . . . . . . . . . . . . . . . 
218 
8.1.4 Modelling of the Synaptic Plasticity .................. 222 
8.1.5 Can the Memory Capacity Ensue from Synaptic Changes? 
227 
8.2 The Unified Cortical Memory Model. . . . . . . . . . . . . . . . . . . . . . . 
230 
8.2.1 The Laminar Network Organization .................. 230 
8.2.2 On the Roles ofInterneurons ........................ 232 
8.2.3 Representation of Knowledge Over Memory Fields. . . . . . 
233 
8.2.4 Self-Controlled Operation of Memory ................ 237 
8.3 Collateral Reading ...................................... 239 
8.3.1 Physiological Results Relevant to Modelling ........... 239 
8.3.2 Related Modelling ................................. 240 
9. Notes on Neural Computing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 
241 
9.1 First Theoretical Views of Neural Networks ................. 241 
9.2 Motives for the Neural Computing Research ................ 242 
9.3 What Could the Purpose of the Neural Networks be? ......... 245 
9.4 Definitions of Artificial "Neural Computing" and General Notes 
on Neural Modelling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 
249 
9.5 Are the Biological Neural Functions Localized or Distributed? 
253 
9.6 Is Nonlinearity Essential to Neural Computing? . . . . . . . . . . . . .. 
255 
9.7 Characteristic Differences Between Neural and Digital 
Computers .............. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 
259 
9.7.1 The Degree of Parallelism of the Neural Networks is Still 
Higher than that of any "Massively Parallel" 
Digital Computer .................................. 259 
9.7.2 Why the Neural Signals Cannot be Approximated by 
Boolean Variables ................................. 261

----- Page 14 (native) -----
Contents 
XV 
9.7.3 The Neural Circuits do not Implement Finite Automata 261 
9.7.4 Undue Views of the Logic Equivalence of the Brain and 
Computers on a High Level ....................... 263 
9.8 "Connectionist Models" ............................... 264 
9.9 How can the Neural Computers be Programmed? .......... 267 
10. Optical Associative Memories ............................... 269 
10.1 Nonholographic Methods .............................. 269 
10.2 General Aspects of Holographic Memories ................ 271 
10.3 A Simple Principle of Holographic Associative Memory. . . .. 
273 
10.4 Addressing in Holographic Memories .................... 275 
10.5 Recent Advances of Optical Associative Memories ......... 280 
Bibliography on Pattern Recognition ............................. 285 
References ................................................... 289 
Subject Index ................................................. 301