

----- Page 1 (native) -----
Machine Leaming, 20, 273-297 (1995) 
~) 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands. 
Support-Vector Networks 
CORINNA CORTES 
VLADIMIR VAPNIK 
AT&T Bell Labs., Hohndel, NJ 07733, USA 
corinna@ neurai.att.com 
vlad@neurai.att.com 
Editor: Lorenza Saitta 
Abstract. 
The support-vector network is a new leaming machine for two-group classification problems. The 
machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high- 
dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the 
decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector 
network was previously implemented for the restricted case where the training data can be separated without 
errors. We here extend this result to non-separable training data. 
High generalization ability of support-vector networks utilizing polynomial input transformations is demon- 
strated. We also compare the performance of the support-vector network to various classical learning algorithms 
that all took part in a benchmark study of Optical Character Recognition. 
Keywords: 
pattern recognition, efficient learning algorithms, neural networks, radial basis function classifiers, 
polynomial classifiers. 
1. 
Introduction 
More than 60 years ago R.A. Fisher (Fisher, 1936) suggested the first algorithm for pattern 
recognition. He considered a model of two normal distributed populations, N(mt, ~1) 
and N(m2, ~2) ofn dimensional vectors x with mean vectors ml and m2 and co-variance 
matrices Et and E2, and showed that the optimal (Bayesian) solution is a quadratic decision 
function: 
[~ 
1 
IE2I] 
(1) 
Fsq(X) = sign 
(x - ml)7"E~-~(x - ma) - ~(x - m2):rEfl(x - m2) + In 1-~11_] " 
In the case where E1 = Ez = ~ the quadratic decision function (1) degenerates to a linear 
function: 
Flin(X) = sign[(mt- 
m2)T~i-lx- l(mlr~-lml2 -- mT~-lm2)] . 
(2) 
To estimate the quadratic decision function one has to determine ~ 
free parameters. To 
estimate the linear function only n free parameters have to be determined. In the case where 
the number of observations is small (say less than 10 n 2) estimating o(n z) parameters is not 
reliable. Fisher therefore recommended, even in the case of ~1 ~ ~32, to use the linear 
discriminator function (2) with ~ of the form: 
Y]~ = "gY]l -~- (1 -- "Y)~-]2, 
(3) 
where r is some constant 1. Fisher also recommended a linear decision function for the 
case where the two distributions are not normal. 
Algorithms for pattern recognition

----- Page 2 (native) -----
274 
CORTES AND VAPNIK 
[] 
dot-product 
I 
\ 
I 
dot-products 
perceptron output 
weights of the output unit, 
e~ 1 ..... cc 5 
output from the 5 hidden units: z 1 ..... z 5 
weights of the 5 hidden units 
~il 
" ~
"
 
output from the 4 hidden units 
~ 
~ 
~ 
weights of the 4 hidden units 
dot-products 
ut vector, x 
Figure 1. 
A simple feed-forward perceptron with 8 input units, 2 layers of hidden units, and I output unit. The 
gray-shading of the vector entries reflects their numeric value. 
were therefore from the very beginning associated with the construction of linear deci- 
sion surfaces. 
In 1962 Rosenblatt (Rosenblatt, 1962) explored a different kind of learning machines: 
perceptrons or neural networks. The perceptron consists of connected neurons, where each 
neuron implements a separating hyperplane, so the perceptron as a whole implements a 
piecewise linear separating surface. See Fig. 1. 
No algorithm that minimizes the error on a set of vectors by adjusting all the weights of 
the network was found in Rosenblatt's time, and Rosenblatt suggested a scheme where only 
the weights of the output unit were adaptive. According to the fixed setting of the other 
weights the input vectors are non-linearly transformed into the feature space, Z, of the last 
layer of units. In this space a linear decision function is constructed: 
I(x)= sign(~iotizi(x)) 
(4) 
by adjusting the weights oti from the ith hidden unit to the output unit so as to minimize some 
error measure over the training data. As a result of Rosenblatt's approach, construction of 
decision rules was again associated with the construction of linear hyperplanes in some 
space. 
An algorithm that allows for all weights of the neural network to adapt in order locally to 
minimize the error on a set of vectors belonging to a pattern recognition problem was found 
in 1986 (Rumelhart, Hinton & Williams, 1986, 1987; Parker, 1985; LeCun, 1985) when the 
back-propagation algorithm was discovered. The solution involves a slight modification 
of the mathematical model of neurons. Therefore, neural networks implement "piece-wise 
linear-type" decision functions. 
In this article we construct a new type of learning machine, the so-called support-vector 
network. The support-vector network implements the following idea: it maps the input 
vectors into some high dimensional feature space Z through some non-linear mapping 
chosen a priori. In this space a linear decision surface is constructed with special properties 
that ensure high generalization ability of the network.

----- Page 3 (native) -----
SUPPORT-VECTOR NETWORKS 
275 
× 
X 
~xX~ 
O 
O 
C) "~ 
"',;~optimal margin 
o j o0='o-.,1"'-. \ 
~O00ö 
~ "~'optima, hyperp,an e 
Figure 2. 
An example of a separable problem in a 2 dimensional space. The support vectors, marked with grey 
squares, define the margin of largest separation between the two classes. 
EXAMPLE. 
To obtain a decision surface corresponding to a polynomial of degree two, one 
can create a feature space, Z, which has N = @ 
coordinates of the form: 
Zl ~ 
X1, ...,Zn 
~ 
Xn, 
2 
2 
Zn+ 1 ~-- X{, ...,Z2n 
~ 
X n, 
Z2n+l ~ XlX2, ... 
,ZN 
~ 
XnXn-1, 
n coordinates, 
n coordinates, 
n(n - 1) 
-
-
 
coordinates, 
2 
where x = (xl ..... xn). The hyperplane is then constructed in this space. 
Two problems arise in the above approach: one conceptual and one technical. The con- 
ceptual problem is how to find a separating hyperplane that will generalize well: the dimen- 
sionality of the feature space will be lauge, and not all hyperplanes that separate the training 
data will necessarily generalize well 2. The technical problem is how computationally to 
treat such high-dimensional spaces: to construct polynomial of degree 4 or 5 in a 200 
dimensionai space it may be necessary to construct hyperplanes in a billion dimensional 
feature space. 
The conceptual part of this problem was solved in 1965 (Vapnik, 1982) for the case of 
optimal hyperplanes for separable classes. An optimal hyperplane is here defined as the 
linear decision function with maximal margin between the vectors of the two classes, see 
Fig. 2. It was observed that to construct such optimal hyperplanes one only has to take into 
acconnt a small amount of the training data, the so called support vectors, which determine 
this margin. It was shown that if the training vectors are separated without errors by an 
optimal hyperptane the expectation value of the probability of committing an error on a test 
example is bounded by the ratio between the expectation value of the number of support 
vectors and the number of training vectors: 
E[number of support vectors] 
E[Pr(error)] _< 
(5) 
number of training vectors

----- Page 4 (native) -----
276 
CORTES AND VAPNIK 
Note that this bound does not explicitly contain the dimensionality of the space of separation. 
It follows from this bound, that if the optimal hyperplane can be constructed from a small 
number of support vectors relative to the training set size the generalization ability will be 
high----even in an infinite dimensional space. In Section 5 we will demonstrate that the ratio 
(5) for a real life problems can be as low as 0.03 and the optimal hyperplane generalizes 
well in a billion dimensional feature space. 
Let 
wo • z + bo = 0 
be the optimal hyperplane in feature space. We will show, that the weights w0 for the 
optimal hyperplane in the feature space can be written as some linear combination of 
support vectors 
W0 = 
~ 
OtiZi. 
(6) 
support vectors 
The linear decision function I (z) in the feature space will accordingly be of the form: 
l(z) = sign( 
~ 
otizi.z+bo) , 
\support vectors 
(7) 
where zi • z is the dot-product between support vectors zi and vector z in feature space. The 
decision function can therefore be described as a two layer network (Fig. 3). 
However, even if the optimal hyperplane generalizes well the technical problem of how 
to treat the high dimensional feature space remains. In 1992 it was shown (Boser, Guyon, 
& Vapnik, 1992), that the order of operations for constructing a decision function can 
be interchanged: instead of making a non-linear transformation of the input vectors fol- 
lowed by dot-products with support vectors in feature space, one can first compare two 
vectors in input space (by e.g. taking their dot-product or some distance measure), and 
then make a non-linear transformation of the value of the result (see Fig. 4). This en- 
ables the construction of rich classes of decision surfaces, for example polynomial decision 
surfaces of arbitrary degree. We will call this type of learning machine a support-vector 
network 3. 
The technique of support-vector networks was first developed for the restricted case of 
separating training data without errors. In this article we extend the approach of support- 
vector networks to cover when separation without error on the training vectors is impossible. 
With this extension we consider the support-vector networks as a new class of learning 
machine, as powerful and universal as neural networks. In Section 5 we will demonstrate 
how well it generalizes for high degree polynomial decision surfaces (up to order 7) in a 
high dimensional space (dimension 256). The performance of the algorithm is compared 
to that of classical learning machines e.g. linear classifiers, k-nearest neighbors classifiers, 
and neural networks. Sections 2, 3, and 4 are devoted to the major points of the derivation 
of the algorithm and a discussion of some of its properties. Details of the derivation are 
relegated to an appendix.

----- Page 5 (native) -----
SUPPORT-VECTOR NETWORKS 
277 
B 
cation 
w 
WN 
£ U ~  
~ 
~] 
suppOrtvectOrs .. 
in feature space 
input vector in feature space 
n-linear transformation 
" 
input vector, x 
Figure 3. Classification by a support-vector network of an unknown pattern is conceptually done by first trans- 
forming the pattem into some high-dimensional feature space. An optimal hyperplane constructed in this feature 
space determines the output. The similarity to a two-layer perceptron can be seen by comparison to Fig. 1. 
2. 
Opümal Hyperplanes 
In this section we review the method of optimal hyperplanes (Vapnik, 1982) for separation 
of training data without errors. In the next section we introduce a notion of soff margins, 
that will allow for an analytic treatment of learning with errors on the training set. 
2.1. 
The Optimal Hyperplane Algorithm 
The set of labeled training patterns 
(Yl, Xl) ..... (Ye, x~), 
Yi ~ {-1, 1} 
(8) 
is said to be linearly separable if there exists a vector w and a scalar b such that the inequalities 
w. xi + b > 1 
if 
Yi = 
1, 
w.xi+b 
<-1 
if 
Yi = -1, 
(9)

----- Page 6 (native) -----
278 
CORTES AND VAPNIK 
A 
classification 
~j~Lagrange mu[tipliers 
~ 
~ 
comparison 
I 
~'1 
I 
J l 
I ~1 
u¢K(xk,x 
^ 
I 
support 
vectors, x k 
[ 2 
] 
inputvector, x 
Figure 4. 
Classification of an unknown pattern by a support-vector network. The pattern is in input space 
zompared to support vectors. The resulting values are non-linearly transformed. A linear function of these 
Iransformed values determine the output of the classifier. 
are valid for all elements of the training set (8). Below we write the inequalities (9) in the 
forma: 
yi(W" X i -~- b) > 1, 
i = 1 ..... •. 
(10) 
The optimal hyperplane 
wo-x+ bo = 0 
(11) 
is the unique one which separates the training data with a maximal margin: it determines 
the direction w/Iwl where the distance between the projections of the training vectors of 
two different classes is maximal, recall Fig. 2. This distance p(w, b) is given by 
X'W 
X'W 
p(w,b)= min --- 
max 
(12) 
{x:y=l} IW[ 
{x:y=--l} Iwl 
The optimal hyperplane (Wo, bo) is the arguments that maximize the distance (12). It follows 
from (12) and (10) that 
2 
2 
p(wo, bo) .
.
.
.
 
(13) 
Iwol 
,/~. wo

----- Page 7 (native) -----
SUPPORT-VECTOR NETWORKS 
279 
This means that the optimal hyperplane is the unique one that minimizes w • wunder the 
constraints (10). Constructing an optimal hyperplane is therefore a quadratic programming 
problem. 
Vectors xi for which Yi (W. x i -1- b) = 1 will be termed support vectors. In Appendix A. 1 
we show that the vector wo that determines the optimal hyperplane can be written as a linear 
combination of training vectors: 
£ 
WO : 
Z 
yiot?xi' 
(14) 
i=1 
where «o >_ 0. Since « > 0 only for support vectors (see Appendix), the expression (14) 
represents a compact form of writing Wo. We also show that to find the vector of parameters 
of i: 
Aó = («0 ..... 
~o), 
one has to solve the following quadratic programming problem: 
W(A) = Arl - 1ArDA 
(15) 
2 
with respect to A v = («1 ..... de), subject to the constraints: 
A > 0, 
(16) 
ATy = 0, 
(17) 
where I T = (1 ..... 1) is an g-dimensional unit vector, yr = (Yl ..... y~) is the g-dimen- 
sional vector of labels, and D is a symmetric £ × £-matrix with elements 
Dij = yiYjXi • xj, 
i, j = 1 ..... 
1. 
(18) 
The inequality (16) describes the nonnegative quadrant. We therefore have to maximize the 
quadratic form (15) in the nonnegative quadrant, subject to the constraints (17). 
When the training data (8) can be separated without errors we atso show in Appendix A 
the following relationship between the maximum of the functional (15), the pair (A0, bo), 
and the maximal margin Po from (13): 
2 
W(Ao) = ~o 2. 
(19) 
If for some A. and large constant Wo the inequality 
W(A,) > Wo 
(20) 
is valid, one can accordingly assert that all hyperplanes that separate the training data (8) 
have a margin 
J~ 
p <

----- Page 8 (native) -----
280 
CORTES AND VAPNIK 
If the training set (8) cannot be separated by a hyperplane, the margin between patterns 
of the two classes becomes arbitrary small, resulting in the value of the functional W(A) 
turning arbitrary large. Maximizing the functional (15) under constraints (16) and (17) 
one therefore either reaches a maximum (in this case one has constructed the hyperplane 
with the maximal margin Po), or one finds that the maximum exceeds some given (large) 
constant Wo (in which case a separation of the training data with a margin larger then 
is impossible). 
The problem of maximizing functional (15) under constraints (16) and (17) can be solved 
very efficiently using the following scheme. Divide the training data into a number of 
portions with a reasonable small number of training vectors in each portion. Start out by 
solving the quadratic programming problem determined by the first portion of training data. 
For this problem there are two possible outcomes: either this portion of the data cannot be 
separated by a hyperplane (in which case the full set of data as well cannot be separated), 
or the optimal hyperplane for separating the first portion of the training data is found. 
Let the vector that maximizes functional (15) in the case of separation of the first portion 
be A1. Among the coordinates of vector A1 some are equal to zero. They correspond to 
non-support training vectors of this portion. Make a new set of training data containing 
the support vectors from the first portion of training data and the vectors of the second 
portion that do not satisfy constraint (10), where w is determined by A1. For this set a 
new functional W2(A) is constructed and maximized at A2. Continuing this process of 
incrementally constructing a solution vector A, covering all the portions of the training 
data one either finds that it is impossible to separate the training set without error, or one 
constructs the optimal separating hyperplane for the full data set, A, = A0. Note, that 
during this process the value of the functional W(A) is monotonically increasing, since 
more and more training vectors are considered in the optimization, leading to a smaller and 
smaller separation between the two classes. 
3. 
The Soft Margin Hyperplane 
Consider the case where the training data cannot be separated without error. In this case 
one may want to separate the training set with a minimal number of errors. To express this 
formally let us introduce some non-negative variables ~i > O, i = 1 ..... ~. 
We can now minimize the functional 
i=1 
(21) 
for small ~r > 0, subject to the constraints 
yi(w" xi + b) > 1 - ~i, 
i = 1 ..... ~, 
(22) 
~i > 0, 
i = 1 ..... £. 
(23) 
For sufficiently small ~r the functional (21) describes the number of the training errors 5. 
Minimizing (21) one finds some minimal subset of training errors: 
(Yi~, xit) ..... (yi~,xi~).

----- Page 9 (native) -----
SUPPORT-VECTOR NETWORKS 
281 
If these data are excluded from the training set one can separate the remaining part of the 
training set without errors. To separate the remaining part of the training data orte can 
construct an optimal separating hyperplane. 
This idea can be expressed formally as: minimize the functional 
~w2+CF ( ~~~ 
)i=1 
(24) 
subject to constraints (22) and (23), where F(u) is a monotonic convex function and C is 
a constant. 
For sufficiently large C and sufficiently small «, the vector Wo and constant b0, that 
minimize the functional (24) under constraints (22) and (23), determine the hyperplane 
that minimizes the number of errors on the training set and separate the rest of the elements 
with maximal margin. 
Note, however, that the problem of constructing a hyperplane which minimizes the 
number of errors on the training set is in general NP-complete. To avoid NP-completeness 
of our problem we will consider the case of cr = 1 (the smallest value of a for which 
the optimization problem (15) has a unique solution). 
In this case the functional (24) 
describes (for sufficiently large C) the problem of constructing a separating hyperplane 
which minimizes the sum of deviations, ~, of training errors and maximizes the margin 
for the correctly classified vectors. If the training data can be separated without errors the 
constructed hyperplane coincides with the optimal margin hyperplane. 
In contrast to the case with a < 1 there exists an efficient method for finding the solution 
of (24) in the case of a = 1. Let us call this solution the soff margin hyperplane. 
In Appendix A we consider the problem of minimizing the functional 
subject to the constraints (22) and (23), where F(u) is a monotonic convex function with 
F(0) = 0. To simplify the formulas we only describe the case of F(u) = u 2 in this section. 
For this function the optimization problem remains a quadratic programming problem. 
In Appendix A we show that the vector w, as for the optimal hyperplane algorithm, can 
be written as a linear combination of support vectors xi: 
£ 
w o ~-~ ~_£ «°NIX i . 
1 i=1 
To find the vector A r = (~q ..... oee) one has to solve the dual quadratic programming 
problem of maximizing 
W(A,~) = Arl- 
I [ATDA + ~] 
(26) 
subject to constraints 
ATy = 0, 
(27) 
a > 0, 
(28) 
0 < A < al, 
(29)

----- Page 10 (native) -----
282 
CORTES AND VAPNIK 
where 1, A, Y, and D are the same elements as used in the optimization problem for 
constructing an optimal hyperplane, 3 is a scalar, and (29) describes coordinate-wise in- 
equalities. 
Note that (29) implies that the smallest admissible value 3 in functional (26) is 
~- Œmax 
= max(«l,..., otg). 
Therefore to find a soft margin classifier one has to find a vector A that maximizes 
W(A) = AT1 - 
ArDA + 
(30) 
under the constraints A > 0 and (27). This problem differs from the problem ofconstructing 
an optimal margin classifier only by the additional term with Otmax in the functional (30). 
Due to this term the solution to the problem of constructing the soft margin classifier is 
unique and exists for any data set. 
The functional (30) is not quadratic because of the term with Otm~x. Maximizing (30) 
subject to the constraints A > 0 and (27) belongs to the group of so-called convex pro- 
gramming problems. Therefore, to construct a soft margin classifier one can either solve 
the convex programming problem in the £-dimensional space of the parameters A, or one 
can solve the quadratic programming problem in the dual £ + 1 space of the parameters A 
and 3. In our experiments we construct the soft margin hyperplanes by solving the dual 
quadratic programming problem. 
4. 
The Method of Convolution of the Dot-Product in Feature Space 
The algorithms described in the previous sections construct hyperplanes in the input space. 
To construct a hyperplane in a feature space one first has to transform the n-dimensional 
input vector x into an N-dimensional feature vector through a choice of an N-dimensional 
vector function qS: 
¢: ffP ---> 9t N. 
An N dimensional linear separator w and a bias b is then constructed for the set of 
transformed vectors 
~(Xi) : 
t~l (Xi), ~b2(Xi) ..... 
~N(Xi), 
i : 
1 ..... 
6. 
Classification of an unknown vector x is done by first transforming the vector to the sepa- 
rating space (x w-~ ¢ (x)) and then taking the sign of the function 
f(x) = w-¢(x) + b. 
(31) 
According to the properties of the soff margin classifier method the vector w can be 
written as a linear combination of support vectors (in the feature space). That means 
£ 
W : Z 
yioli~)(Xi)" 
i=1 
(32)

----- Page 11 (native) -----
SUPPORT-VECTOR NETWORKS 
283 
The linearity of the dot-product implies, that the classification function f in (31) for an 
unknown vector x only depends on the dot-products: 
f(x) = q~(x)- w + b = S Yi°ti~(x)" ~b(xi) + b. 
i=1 
(33) 
The idea of constructing support-vector networks comes from considering general forms 
of the dot-product in a Hilbert space (Anderson & Bahadur, 1966): 
~b (u)- <p (v) - K (u, v). 
(34) 
According to the Hilbert-Schmidt Theory (Courant & Hilbert, 1953) any symmetric 
function K(u, v), with K(u, v) ~ Le, can be expanded in the form 
oo 
K(u, v) = Z 
~i~i(u) - ~ßi(V), 
i=l 
(35) 
where ~-i c ~t and qSi are eigenvalues and eigenfunctions 
f K (u, v)q~i (u)du = ),i ~bi (v). 
of the integral operator defined by the kernel K (u, v). A sufficient condition to ensure that 
(34) defines a dot-product in a feature space is that all the eigenvalues in the expansion (35) 
are positive. To guarantee that these coefficients are positive, it is necessary and sufficient 
(Mercer's Theorem) that the condition 
f f K(u, v)g(u)g(v)dudv > 0 
is satisfied for all g such that 
f g2(u)du < oo. 
Functions that satisfy Mercer's theorem can therefore be used as dot-products. Aizerman, 
Braverman and Rozonoer (1964) consider a convolution of the dot-product in the feature 
space given by function of the form 
K(u,v) = exp(- lu~vl), 
(36) 
which they call Potential Functions. 
However, the convolution of the dot-product in feature space can be given by any function 
satisfying Mercer's condition; in particular, to construct a polynomial classifier of degree 
d in n-dimensional input space one can use the following function 
K(u, v) = (u- v + 1) d. 
(37)

----- Page 12 (native) -----
284 
CORTES AND VAPNIK 
Using different dot-products K (u, v) one can construct different learning machines with 
arbitrary types of decision surfaces (Boser, Guyon & Vapnik, 1992). The decision surface 
of these machines has a form 
f(x) = ~ 
yiotiK(x, Xi), 
i=1 
where xi is the image of a support vector in input space and «i is the weight of a support 
vector in the feature space. 
To find the vectors xi and weights «i one follows the same solution scheme as for the 
original optimal margin classifier or soft margin classifier. The only difference is that 
instead of matrix D (determined by (18)) one uses the matrix 
Dij : Yi Yj K (xi, x j), 
i, j = 1 ..... 1. 
5. 
General Features of Support-Vector Networks 
5.1. 
Constructing the Decision Rules by Support-Vector Networks is Efficient 
To construct a support-vector network decision rule one has to solve a quadratic optimization 
problem: 
W(A) = Arl- 
~ ArDA+ 
, 
under the simple constraints: 
0_<A<81, 
ATy ----= 0, 
where matrix 
Dij : YiYj K (xi, x j), 
i, j = 1 ..... I. 
is determined by the elements of the training set, and K (u, v) is the function determining 
the convolution of the dot-products. 
The solution to the optimization problem can be found efficiently by solving intermediate 
optimization problems determined by the training data, that currently constitute the support 
vectors. This technique is described in Section 3. The obtained optimal decision function 
is unique 6. 
Each optimization problem can be solved using any standard techniques. 
5.2. 
The Support-Vector Network is a Universal Machine 
By changing the function K(u, v) for the convolution of the dot~product one can implement 
different networks.

----- Page 13 (native) -----
SUPPORT-VECTOR NETWORKS 
285 
In the next section we will consider support-vector network machines that use polynomial 
decision surfaces. To specify polynomials of different order d one can use the following 
functions for convolution of the dot-product 
K(u, v) ---- (u. v + 1) d. 
Radial Basis Function machines with decision functions of the form 
f(x) = sign 
O~ i exp 
äf 
\ i=1 
can be implemented by using convolutions of the type 
/ ,o«,~j 
K (u, v) = exp 
~-i 
" 
In this case the support-vector network machine will construct both the centers xi of the 
approximating function and the weights «i. 
One can also incorporate a priori knowledge of the problem at hand by constructing 
special convolution functions. Support-vector networks are therefore a rather general class 
of learning machines which changes its set of decision functions simply by changing the 
form of the dot-product. 
5.3. Support-Vector Networks and Control of Generalization Ability 
To control the generalization ability of a learning machine one has to control two different 
factors: the error-rate on the training data and the capacity of the learning machine as 
measured by its VC-dimension (Vapnik, 1982). There exists a bound for the probability of 
errors on the test set of the following form: with probability 1 - r/the inequality 
Pr(test error) < Frequency(training error) + Confidence Interval 
(38) 
is valid. In the bound (38) the confidence interval depends on the VC-dimension of the 
learning machine, the number of elements in the training set, and the value of r/. 
The two factors in (38) form a trade-off: the smaller the VC-dimension of the set of 
functions of the learning machine, the smaller the confidence interval, but the larger the 
value of the error frequency. 
A general way for resolving this trade-off was proposed as the principle of structural risk 
minimization: for the given data set one has to find a solution that minimizes their sum. 
A particular case of structural risk minimization principle is the Occam-Razor principle: 
keep the first term equal to zero and minimize the second one. 
It is known that the VC-dimension of the set of linear indicator functions 
l(x)=sign(w.x+b), 
Ix[<Cx 
with fixed threshold b is equal to the dimensionality of the input space. However, the 
VC-dimension of the subset 
l(x) = sign(w, x + b), 
[xm _< C, 
[wg _< Cw

----- Page 14 (native) -----
286 
CORTES AND VAPNIK 
(the set of functions with bounded norm of the weights) can be less than the dimensionality 
of the input space and will depend on Cw. 
From this point of view the optimal margin classifier method executes an Occam-Razor 
principle. It keeps the first term of (38) equal to zero (by satisfying the inequality (9)) 
and it minimizes the second term (by minimizing the functional w. w). This minimization 
prevents an over-fitting problem. 
However, even in the case where the training data are separable one may obtain better 
generalization by minimizing the confidence term in (38) even further at the expense of 
errors on the training set. In the soft margin classifier method this can be done by choosing 
appropriate values of the parameter C. In the support-vector network algorithm one can 
control the trade-off between complexity of decision rule and frequency of error by changing 
the parameter C, even in the more general case where there exists no solution with zero 
error on the training set. Therefore the support-vector network can control both factors for 
generalization ability of the learning machine. 
6. Experimental Analysis 
To demonstrate the support-vector network method we conduct two types of experiments. 
We construct artificial sets of patterns in the plane and experiment with 2nd degree poly- 
nomial decision surfaces, and we conduct experiments with the real-life problem of digit 
recognition. 
6.1. 
Experiments in the Plane 
Using dot-products of the form 
K(n,v) = (u.v+ 1) a 
(39) 
with d = 2 we construct decision rules for different sets of patterns in the plane. Results 
of these experiments can be visualized and provide nice illustrations of the power of the 
algorithm. Examples are shown in Fig. 5. The 2 classes are represented by black and white 
Figure 5. 
Examples of the dot-product (39) with d = 2, Support patterns are indicated with doable circles, 
errors with a cross.

----- Page 15 (native) -----
SUPPORT-VECTOR NETWORKS 
287 
7 
7 
4 
8 
0 
1 
- 
4 
8 
7 
4 
8 
7 
3 
7 
Figure 6. Examples of patterns with labels from the US Postal Service digit database. 
bullets. In the figure we indicate support patterns with a double circle, and errors with a 
cross. The solutions are optimal in the sense that no 2nd degree polynomials exist that make 
less errors. Notice that the numbers of support patterns relative to the number of training 
patterns are small. 
6.2. 
Experiments with Digit Recognition 
Our experiments for constructing support-vector networks make use of two different data- 
bases for bit-mapped digit recognition, a small and a large database. The small one is a US 
Postal Service database that contains 7,300 training patterns and 2,000 test patterns. The 
resolution of the database is 16 x 16 pixels, and some typical examples are shown in Fig. 6. 
On this database we report experimental research with polynomials of various degree. 
The large database consists of 60,000 training and 10,000 test patterns, and is a 50-50 
mixture of the NIST 7 training and test sets. The resolution of these patterns is 28 x 28 
yielding an input dimensionality of 784. On this database we have only constructed a 4th 
degree polynomial classifier. The performance of this classifier is compared to other types 
of learning machines that took part in a benchmark study (Bottou, 1994). 
In all our experiments ten separators, one for each class, are constructed. Each hyper- 
surface makes use of the same dot product and pre-processing of the data. Classification of 
an unknown patterns is done according to the maximum output of these ten classifiers. 
6.2.1. 
Experiments with US Postal Service Database. 
The US Postal Service Database 
has been recorded from actual mail pieces and results from this database have been reported 
by several researchers. In Table 1 we list the performance of various classifiers collected 
Table L Performance of various classifiers collected from publications and own experiments. For references 
see text. 
Classifier 
Raw error, % 
Human performance 
2.5 
Decision tree, CART 
17 
Decision tree, C4.5 
16 
Best 2 layer neural network 
6.6 
Special architecture 5 layer network 
5.1

----- Page 16 (native) -----
288 
CORTES AND VAPNIK 
Table 2. 
Results obtained for dot products of polynomials of various degree. The number of "support vectors" 
is a mean value per classifier. 
Degree of 
Raw 
Support 
Dimensionality of 
polynomial 
error, % 
vectors 
feature space 
1 
12.0 
200 
256 
2 
4.7 
127 
~33000 
3 
4.4 
148 
~1 x 106 
4 
4.3 
165 
~I x 109 
5 
4.3 
175 
~1 x 1012 
6 
4.2 
185 
~1 x 1014 
7 
4.3 
190 
~1 x 1016 
from publications and own experiments. The result of human performance was reported 
by J. Bromley & E. Sackinger (Bromley & Sackinger, 1991). The result with CART was 
carried out by Daryl Pregibon and Michael D. Riley at Bell Labs., Murray Hill, NJ. The 
results of C4.5 and the best 2-layer neural network (with optimal number of hidden units) 
were obtained specially for this paper by Corinna Cortes and Bernard Schoelkopf respee- 
tively. The result with a special purpose neural network architecture with 5 layers, LeNetl, 
was obtained by Y. LeCun et al. (1990). 
On the experiments with the US Postal Service Database we used pre-processing (cen- 
tering, de-slanting and smoothing) to incorporate knowledge about the invariances of the 
problem at hand. The effect of smoothing of this database as a pre-processing for support- 
vector networks was investigated in (Boser, Guyon & Vapnik, 1992). For our experiments 
we chose the smoothing kernel as a Gaussian with standard deviation ~r = 0.75 in agreement 
with (Boser, Guyon & Vapnik, 1992). 
In the experiments with this database we constructed polynomial indicator functions 
based on dot-products of the form (39). The input dimensionality was 256, and the order 
of the polynomial ranged from 1 to 7. Table 2 describes the results of the experiments. The 
training data are not linearly separable. 
Notice that the number of support vectors increases very slowly. The 7 degree polyno- 
mial has only 30% more support vectors than the 3rd degree polynomial--and even less 
than the first degree polynomial. The dimensionality of the feature space for a 7 degree 
polynomial is however 101° times larger than the dimensionality of the feature space for 
a 3rd degree polynomial classifier. Note that performance almost does not change with 
increasing dimensionality of the space--indicating no over-fitting problems. 
The relatively high number of support vectors for the linear separator is due to non- 
separability: the number 200 includes both support vectors and training vectors with a non- 
zero ~-value. If~ > 1 the training vector is misclassified; the number ofmis-classifications 
on the training set averages to 34 per classifier for the linear case. For a 2nd degree classifier 
the total number of mis-classifications on the training set is down to 4. These 4 patterns are 
shown in Fig. 7. 
It is remarkable that in all our experiments the bound for generalization ability (5) holds 
when we consider the number of obtained support vectors instead of the expectation value 
of this number. In all cases the upper bound on the error probability for the single classifier 
does not exceed 3% (on the test data the actual error does not exceed 1.5% for the single 
classifier).

----- Page 17 (native) -----
SUPPORT-VECTORNETWORKS 
289 
~1141~1~ 
:.~ 
4 
4 
8 
5 
Figure 7. Labeled examples of errors on the training set for the 2nd degree polynomial support-vector classifier. 
The training time for construction of polynomial classifiers does not depend on the de- 
gree of the polynomial---only the number of support vectors. Even in the worst case it is 
fastet than the best performing neural network, constructed specially for the task, LeNetl 
(LeCun, et al., 1990). The performance of this neural network is 5.1% raw error. Polyno- 
mials with degree 2 or higher outperform LeNetl. 
6.2.2. 
Experiments with the NIST Database. 
TheNISTdatabasewasusedforbenchmark 
studies conducted over just 2 weeks. The limited time frame enabled only the construction 
of 1 type of classifier, for which we chose a 4th degree polynomial with no pre-processing. 
Our choice was based on our experience with the US Postal database. 
Table 3 lists the number of support vectors for each of the 10 classifiers and gives the 
performance of the classifier on the training and test sets. Notice that even polynomials 
of degree 4 (that have more than 108 free parameters) commit errors on this training set. 
The average frequency of training errors is 0.02% ~ 12 per class. The 14 misclassified test 
patterns for classifier 1 are shown in Fig. 8. Notice again how the upper bound (5) holds 
for the obtained number of support vectors. 
The combined performance of the ten classifiers on the test set is 1.1% error. This result 
should be compared to that of other participating classifiers in the benchmark study. These 
other classifiers include a linear classifier, a k = 3-nearest neighbor classifier with 60,000 
prototypes, and two neural networks specially constructed for digit recognition (LeNetl 
and LeNet4). The authors only contributed with results for support-vector networks. The 
results of the benchmark are given in Fig. 9. 
We conclude this section by citing the paper (Bottou, et al., 1994) describing results of 
the benchmark: 
For quite a long time LeNetl was considered state of the art... 
Through a series 
of experiments in architecture, combined with an analysis of the characteristics of 
recognition error, LeNet4 was crafted .... 
The support-vector network has excellent accuracy, which is most remarkable, be- 
cause unlike the other high performance classifiers, it does not include knowledge 
Table 3. Results obtained for a 4th degree polynomial classifier on the NIST database. The size of the training 
set is 60,000, and the size of the test set is 10,000 patterns. 
C1. 0 
C1. 1 
CI. 2 
C1. 3 
C1. 4 
CI. 5 
CI. 6 
CI. 7 
C1. 8 
C1, 9 
Supp. patt, 
1379 
989 
1958 
1900 
1224 
2024 
1527 
2064 
2332 
2765 
Error train 
7 
16 
8 
11 
2 
4 
8 
16 
4 
1 
Error test 
19 
14 
35 
35 
36 
49 
32 
43 
48 
63

----- Page 18 (native) -----
290 
CORTES AND VAPNIK 
I ~1«1~1 ~ II ~ltl~ 
1 
6 
1 
9 
6 
6 
1 
1~ I~[n l J ~~ 1[ 1, l z~ I~1 
9 
1 
1 
1 
1 
1 
1 
Figure 8. 
The 14 misclassified test pattems with iabels for classifier 1. Patterns with label "1" are false negative. 
Patterns with other labels are false positive. 
Test 
erroi 
2~ 
Figure 9. 
linear 
classifier 
k=3-nearest 
neighbor 
Results from the benchmark study. 
LeNetl 
LeNet4 
SVN 
about the geometry of the problem. In fact the classifier would do as weil if the 
image pixels were encrypted e.g. by a fixed, random permutation. 
The last remark suggests that further improvement of the performance of the support- 
vector network can be expected from the construction of functions for the dot-product 
K(u, v) that reflect a priori information about the problem at hand. 
7. 
Conclusion 
This paper introduces the support-vector network as a new learning machine for two-group 
classification problems. 
The support-vector network combines 3 ideas: the solution technique from optimal hy- 
perplanes (that allows for an expansion of the solution vector on support vectors), the idea of 
convolufion of the dot-product (that extends the solution surfaces from linear to non-linear), 
and the notion of soft margins (to allow for errors on the training set).

----- Page 19 (native) -----
SUPPORT-VECTOR NETWORKS 
291 
The algorithm has been tested and compared to the performance of other classical al- 
gorithms. Despite the simplicity of the design in its decision surface the new algorithm 
exhibits a very fine performance in the comparison study. 
Other characteristics like capacity control and ease of changing the implemented decision 
surface render the support-vector network an extremely powerful and universal learning 
machine. 
A. Constructing Separating Hyperplanes 
In this appendix we derive both the method for constructing optimal hyperplanes and soft 
margin hyperplanes. 
A.1. 
Optimal Hyperplane Algorithm 
It was shown in Section 2, that to construct the optimal hyperplane 
Wo.x+bo =0, 
(40) 
which separates a set of training data 
(Yl, Xl) ..... 
(Ye, Xe), 
one has to minimize a functional 
qb=W'W, 
subject to the constraints 
Yi (Xi • W -1- b) > 1, 
i = 1 ..... £. 
(41) 
To do this we use a standard optimization technique. We construct a Lagrangian 
1 
L(w, b, A) = ~w. w - Zoli[Yi(Xi 
• w + b) - 1], 
(42) 
i=l 
where A T = («1 ..... ole) is the vector of non-negative Lagrange multipliers corresponding 
to the constraints (41). 
It is known that the solution to the optimization problem is determined by the saddle point 
of this Lagrangian in the 2e + 1-dimensional space of w, A, and b, where the minimum 
should be taken with respect to the parameters w and b, and the maximum should be taken 
with respect to the Lagrange multipliers A. 
At the point of the minimum (with respect to w and b) one obtains: 
w wo( 
~ 
) 
OL(w, b, A) 
- ~_, «iyixi 
0, 
(43) 
0W 
= 
W0 
= 
i=l 
O L ( w , b, A) b=bo 
~--d 
= 
yioti = O. 
(44) 
Ob 
«,

----- Page 20 (native) -----
292 
CORTES AND VAPNIK 
From equality (43) we derive 
Wo = ~ 
oliYiXi, 
(45) 
i=1 
which expresses, that the optimal hyperplane solution can be written as a linear combina- 
tion of training vectors. Note, that only training vectors xi with ~i > 0 have an effective 
contribution to the sum (45). 
Substituting (45) and (44) into (42) we obtain 
1 
W(A) 
: 
~ 
o~ i -- ~w 0 • Wo 
(46) 
i=1 
1 e 
e 
= Z oti- "~ Z 2 
oti«jyiyjxi .xj. 
(47) 
i=1 
i=l j=l 
In vector notation this can be rewritten as 
W(A) = Arl - 1ATDA, 
(48) 
2 
where 1 is an l-dimensional unit vector, and D is a symmetric £ x &matrix with elements 
Dij : yiyjxi • xj. 
To find the desired saddle point it remains to locate the maximum of (48) under the 
constraints (43) 
ATY = 0, 
where yT = (Yl ..... Ye), and 
A>0. 
The Kuhn-Tucker theorem plays an important part in the theory of optimization. Ac- 
cording to this theorem, at our saddle point in wo, bo, Ao, any Lagrange multiplier «o and 
its corresponding constraint are connected by an equality 
oli[Yi(Xi • Wo + bo) - l] = 0, 
i = 1 ..... ~. 
From this equality comes that non-zero values «i are only achieved in the cases where 
Yi (xi • W 0 -~- b0) - 
1 = 0. 
In other words: «i ¢ 0 only for cases were the inequality is met as an equality. We call 
vectors xi for which 
Yi (Xi - WO -k- bo) = 1 
for support-vectors. Note, that in this terminology the Eq. (45) states that the solution vector 
Wo can be expanded on support vectors.

----- Page 21 (native) -----
SUPPOR%VECTOR NETWORKS 
293 
Another observation, based on the Kuhn-Tucker Eqs. (44) and (45) for the optimal 
solution, is the relationship between the maximal value W(A0) and the separation distance 
Po: 
wo.wo = E 
°t°yixi'w° = E 
°e°(1 - yibo)= E 
°t°" 
i=1 
i=1 
i=1 
Substituting this equality into the expression (46) for W(Ao) we obtain 
* 
1 
wo • wo 
W(Ao) = ~o,°- ~wo. wo- ~- 
i=1 
Taking into account the expression (13) from Section 2 we obtain 
2 
W(Ao) 
»° 2, 
where Po is the margin for the optimal hyperplane. 
A.2. 
Soff Margin Hyperplane Algorithm 
Below we first consider the case of F(u) = u k. Then we describe the general result for a 
monotonic convex function F(u). 
To construct a soft margin separating hyperplane we maximize the functional 
1 
(~)~ 
= -w. w+ C 
~i 
, 
2 
k>l, 
under the constraints 
yi(xi 
- W -t- b) > 1 -- ~i, 
i = 1 ..... •, 
(49) 
~i > 0, 
i = 1 ..... e. 
(50) 
The Lagrange functional for this problem is 
L(w, ~, b, A, R) 
1 
g 
g 
= 
--W" W + 
C 
~i 
- 
oli[Yi(Xi • W + 
b) - 
1 + ~i] - 
ri~i, 
(51) 
2 
_ 
i=1 
where the non-negative multipliers A r = (oq, oe2 ..... 
Œl) arise from the constraint (49), 
and the multipliers R r = (rl, re ..... rt) enforce the constraint (50). 
We have to find the saddle point of this functional (the minimum with respect to the 
variables wi, b, and ~i, and the maximum witll respect to the variables oei and ri). 
Let us use the conditions for the minimum of this functional at the extremum point: 
OL 
e 
= WO -- E 
otiYiXi ~- O, 
w=wo 
i=I 
(52)

----- Page 22 (native) -----
294 
CORTES AND VAPNIK 
OL 
e 
: 
y~otiy 
i ~- O, 
"~ 
b=bo 
i=I 
~/~0~ 
(~1 ~-1 
OL 
= kC 
~o 
_ ol i _ ri" 
(53) 
(54) 
If we denote 
, 
(±)~ 
Z~«°= 
i=l 
~k Ck,] 
' 
(55) 
we can rewrite Eq. (54) as 
~ -- Oli -- ri =0. 
(56) 
From the equalities (52)-(55) we find 
W O = ZoliYiXi, 
i=1 
_otiYi 
= O, 
i=1 
8 = ot i "-I- ri. 
(57) 
(58) 
Substituting the expressions for Wo, bo, and 8 into the Lagrange functional (51) we obtain 
W(A'8)= Y~~°ti--2 i~.a j~l 
.= .= 
(kC)l/k_ 1 1-- 
. 
(59) 
To find the soft margin hyperplane solution one has to maximize the form functional 
(59) under the constraints (57)-(58) with respect to the non-negative variables «i, ri with 
i = 1 ..... I. In vector notation (59) can be rewritten as 
W(A, ô) = Arl - 
A~DA + (kC)l/~_ l 
1 - 
, 
(60) 
where A and D are as defined above. To find the desired saddle point one therefore has to 
find the maximum of (60) under the constraints 
and 
ATy = 0, 
(61) 
A + R = 81, 
(62) 
A > 0, 
(63) 
R > 0. 
(64) 
From (62) and (64) one obtains that the vector A should satisfy the conditions 
0 < A < 81. 
(65)

----- Page 23 (native) -----
SUPPORT-VECTOR NETWORKS 
295 
From conditions (62) and (64) one can also conclude that to maximize (60) 
3 = 0tma x = max0xl ..... 
o~e). 
Substituting this value of ~ into (60) we obtain 
W(A) = ATI_ [1ATDA + 
°tkm/kaZ . 
(l-k) 
] 
(kC)l/k- 1 
(66) 
To find the soft margin hyperplane one can therefore either find the maximum of the quadratic 
form (51) under the constraints (61) and (65), or one has to find the maximum of the convex 
function (60) under the constraints (61) and (56). For the experiments reported in this paper 
we used k = 2 and solved the quadratic programming problem (51). 
For the case of F(u) = u the same technique brings us to the problem of solving the 
following quadratic optimization problem: minimize the functional 
1 
r 
W(A) = AT1 -- -A DA, 
2 
under the constraints 
and 
O<A<C1, 
ATy = 0. 
The general solution for the case of a monotone convex function F(u) can also be obtained 
from this technique. The soft margin hyperplane has a form 
e 
w --~ ~_~iYiXi, 
i=1 
where Ao r = (n ° ..... oe °) is the solution of the following dual convex programming prob- 
lem: maximize the functional 
W(A)=Arl-[1ArDA+(otma~f-I(-~))-CF(f-1(~-~))I, 
ATy = 0, 
A>_0, 
where we denote 
f(u) = F'(u). 
For convex monotone functions F(u) with F(O) = 0 the following inequality is valid: 
uF'(u) > F(u). 
Therefore the second term in square brackets is positive and goes to infinity when Otmax goes 
to infinity. 
under the constraints

----- Page 24 (native) -----
296 
CORTES AND VAPNIK 
Finally, we can consider the hyperplane that minimizes the form 
-- 
W 
• W-I- 
~i 
2 
subject to the constraints (49)-(50), where the second term minimizes the least square value 
for the errors. This lead to the following quadratic programming problem: maximize the 
functional 
1 [ArDA + IArA] 
(67) 
W(A) = A~I- ä 
in the non-negative quadrant A _> 0 subject to the constraint ATy = 0. 
Notes 
I. The optimal coefflcient for ~ was found in the sixties (Anderson & Bahadur, 1966). 
2. Recall Fisher's concerns about small amounts of data and the quadratic discriminant function. 
3. With this name we emphasize how crucial the idea of expanding the solution on support vectors is for these 
learning machines. In the support-vectors learning algorithm the complexity of the construction does not 
depend on the dimensionality of the feature space, but on the number of support vectors. 
4. Note that in the inequalities (9) and (10) the right-hand side, but not vector w, is normalized. 
5. A training error is here defined as a pattern where the inequality (22) holds with ~ > 0. 
6. The decision function is unique but not its expansion on support vectors. 
7. National Institute for Standards and Technology, Special Database 3. 
References 
Aizerman, M., Braverman, E., & Rozonoer, L. (1964). Theoretical foundations of the potential function method 
in pattern recognition learning. Automation and Remote Control, 25:821-837. 
Anderson, T.W., & Bahadur, R.R. (1966). Classification into two multivariate normal distributions with different 
covariance matrices. Ann. Math. Stat., 33:420--431. 
Boser, B.E., Guyon, I., & Vapnik, V.N. (1992). A training algorithm for optimal margin classifiers. In Proceedings 
of the F(fth Annual Worlaghop of Computational Learning Theory, 5, 144-152, Pittsburgh, ACM. 
Bottou, L., Cortes, C., Denker, J.S., Drucker, H., Guyon, I., Jackel, L.D., LeCun, Y., Sackinger, E., Simard, P., 
Vapnik, V., & Miller, U.A. (1994). Comparison of classifier methods: A case study in handwritten digit 
recognition. Proceedings of 12th International Conference on Pattern Recognition and Neural Network. 
Bromley, J., & Sackinger, E. (1991). Neural-network and k-nearest-neighbor classifiers. Technical Report 11359- 
910819-16TM, AT&T. 
Courant, R., & Hilbert, D. (1953). Methods ofMathematical Physics, Interscience, New York. 
Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. Ann. Eugenics, 7:111-132. 
LeCun, Y. (1985). Une procedure d'apprentissage pour reseau a seuil assymetrique. Cognitiva 85: A la Frontiere 
de l'Intelligence Artificielle des Sciences de la Connaissance des Neurosciences, 599-604, Paris. 
LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., & Jackel, L.D. (1990). Handwritten 
digit recognition with a back-propagation network. Advances in Neural Information Processing Systems, 2, 396- 
404, Morgan Kaufman. 
Parker, D.B. (1985). Learning logic. Technical Report TR-47, Center for Computational Research in Economics 
and Management Science, Massachusetts Institute of Technology, Cambridge, MA. 
Rosenblatt, F. (1962). Principles ofNeurodynamics, Spartan Books, New York.

----- Page 25 (native) -----
SUPPORT-VECTORNETWORKS 
297 
Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986). Learning intemal representations by backpropagating 
errors. Nature, 323:533-536. 
Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1987). Learning intemal representations by error propagation. 
In James L. McClelland & David E. Rumelhart (Eds.), Parallel Distributed Processing, 1, 318-362, MIT Press. 
Vapnik, V.N. (1982). Estimation ofDependences Based on Empirical Data, Addendum 1, New York: Springer- 
Verlag. 
Received May 15, 1993 
Accepted February 20, 1995 
Final Manuscript March 8, 1995