

----- Page 1 (native) -----
Meta-analytic procedures for social
research
Citation
Rosenthal, Robert. Meta-analytic procedures for social research. Applied Social Research
Methods Series 6. Newbury Park, CA: Sage Publications, 1991.
Published version
https://doi.org/10.4135/9781412984997
Link
http://nrs.harvard.edu/urn-3:HUL.InstRepos:38784661
Terms of use
This article was downloaded from Harvard University’s DASH repository, and is made
available under the terms and conditions applicable to Other Posted Material (LAA), as set
forth at
https://harvardwiki.atlassian.net/wiki/external/NGY5NDE4ZjgzNTc5NDQzMGIzZWZhMGFlOWI2M2EwYTg
Accessibility
https://accessibility.huit.harvard.edu/digital-accessibility-policy
Share Your Story
The Harvard community has made this article openly available.
Please share how this access benefits you. Submit a story

----- Page 2 (ocr) -----
_ META-ANALYTIC |
- PROCEDURES FOR ’
‘SOCIAL RESEARCH _

Applied ooo Research Methods Series | |

----- Page 3 (ocr) -----
APPLIED SOCIAL RESEARCH :
METHODS SERIES
; Series Editors: :
LEONARD BICKMAN, Peabody College, Vanderbilt University, Nashville :
DEBRA J. ROG, Vanderbilt University, Washington, DC :
1, SURVEY RESEARCH METHODS 15. PARTICIPANT OBSERVATION: eo W AL y TIC
Revised Edition A Methodology tor Human Studies :
by FLOYD J. FOWLER, ur. by DANNY L. JORGENSEN
2. INTEGRATING RESEARCH: A Guide 16, INTERPRETIVE INTERACTIONISM OCEDURES
for Literature Reviews by NORMAN K, DENZIN
Second Edition :
by HARRIS M. GQOPER 17. ETHNOGRAPHY: Step by Step
3, METHODS FOR POLICY RESEARCH by DAVID M. FETTERMAN
by ANN MAJCHRZAK 18. STANDARDIZED SURVEY
; INTERVIEWING: |
4. Information Sen aor ethode Minimizing interviewer-Related Error : SOCIAL :
by DAVID W. STEWART by FLOYD J. FOWLER, Jr. :
; and THOMAS W. MANGIONE
5. CASE STUDY RESEARCH: 19. PRODUCTIVITY MEASUREMENT: :
Design and Methods A Guide for M dE : 1 :
Revised Edition Guide for Managers and Evaluators : |
by ROBERT K. VIN by ROBERT O. BRINKERHOFF
y and DENNIS E. DRESSLER
6. META-ANALYTIC PROCEDURES FOR ;
SOCIAL RESEARCH 20. FOCUS GROUPS:
Revised Edition Theory and Practice : 0 er osen a '
by ROBERT ROSENTHAL by DAVID W. STEWART '
and PREM N. SHAMDASANI :
7. TELEPHONE SURVEY METHODS: ; ,
Sampling, Selection, and Supervision 21. PRACTICAL SAMPLING :
by PAUL J. LAVRAKAS by GARY T. HENRY 3
8. DIAGNOSING ORGANIZATIONS: 22. DECISION RESEARCH: ,
Methods, Models, and Processes A Field Guide : .
by MICHAEL |. HARRISON by JOHN S. CARROLL :
9. GROUP TECHNIQUES FOR and ERIC J. JOHNSON
IDEA BUILDING 23, RESEARCH WITH :
by CARL M. MOORE HISPANIC POPULATIONS :
10. NEED ANALYSIS: Tools for the TBA MEATLA CAKIOSS MAR
Human Services and Education and BARBARA VANOSS MARIN : Applied Social Research Methods Series |
by JACK McKILLIP 24, INTERNAL EVALUATION: ppited Sock
11. LINKING AUDITING AND Building Organizations from Within : Volume 6 '
METAEVALUATION: by ARNOLD J. LOVE :
Enhancing Quality in Applied Research 25. COMPUTER SIMULATION : : j
by THOMAS A. SCHWANDT APPLICATIONS: An Introduction 1 i
and EDWARD S. HALPERN by MARCIA LYNN WHICKER a a
12. ETHICS AND VALUES and LEE SIGELMAN .
IN APPLIED SOCIAL RESEARCH 26. SCALE DEVELOPMENT: / ; a
by ALLAN J. KIMMEL Theory and Applications 2 SAGE PUBLICATIONS :
13. ON TIME AND METHOD by ROBERT F. DeVELLIS | The International Professional Publishers
by JANICE A. KELLY 27. STUDYING FAMILIES : . |
and JOSEPH E. McGRATH by ANNE P. COPELAND Newbury Park London New Delhi i
14, RESEARCH IN HEALTH CARE and KATHLEEN M. WHITE '
SETTINGS
by KATHLEEN &. GRADY :
and BARBARA STRUDLER WALLSTON |
|

----- Page 4 (ocr) -----
To Jessica, Emily, and Stephen S
for their combined level of significance -
Copyright © 1984, 1991 by Sage Publications, Inc. 2
All rights reserved. No part of this book may be reproduced or utilized CONTENTS
in any form or by any means, electronic or mechanical, including /
photocopying, recording, or by any information storage and retrieval |
system, without permission in writing from the publisher. :
: Preface to the Revised Edition vii
For information address: |
| Preface to the First Edition ix
SAGE Publications, Inc. |
2455 Teller Road . . oye :
Newbury Park, California 91320 A Note on the Revised Edition 1
6 Bonhill Street ;
London EC2A 4PU | I. Two Sources of Pessimism in the Social Sciences 3
United Kingdom II. Early Examples of Meta-Analytic Procedures 5
SAGE Publications India Pvt. Ltd, / Tif. The Current Status of Meta-Analytic Procedures 10
M-32 Market IV. An Empirical Evaluation of Meta-Analytic Procedures 12
Greater Kailash I - . :
| I. Effect Size and Statistical Significance 14
Printed in the United States of America IL. Inferential Errors 20
| III. Adjusting Effect Size Estimates 21
Library of Congress Cataloging-in-Publication Data IV. Some Solutions to the Problem of Multiple
Rosenthal, Robert, 1933 / (Correlated) Results 26
osenthal, Robert, - . : : 34
Meta-analytic procedures for social research / Robert Rosenthal. — V. A Summary of Some Effect Size Indicators
Rev. ed. 3. Retrieving and Assessing Research Results 36
p. cm. -- (Applied social research methods series; v. 6)
Includes bibliographical references and index. : I. Retrieving Research Results 36
ISBN 0-8039-4245-1. — ISBN 0-8039-4246-X (pbk.) | II. Assessing Research Results 46
1. Social sciences—Research. [. Title. if. Series. :
H62.R6565 1991 4. Comparing and Combining Research Results 59
300'.72—de20 -941 :
7 ° nn ° | I. A Framework for Meta-Analytic Procedures 59
II. Meta-Analytic Procedures: Two Independent Studies 61 |
FIRST PRINTING, 1991 : TH. Meta-Analytic Procedures: Any Number of
/ Independent Studies 72 |
Sage Production Editor: Michelle R. Starika

----- Page 5 (ocr) -----
| 5. Combining Probabilities so | !
I, General Procedures 89 7 |
: Il. Special Issues 101 : |
6. Illustrations of Meta-Analytic Procedures 110 :

I. Displaying and Combining Effect Sizes 110 : i
Il. Combining Effect Sizes and Significance Levels 111 :
Ill. Combining Effect Sizes and Blocking 114 PREFACE TO THE 4

IV. Combining Effect Sizes, Blocking, and Confidence Intervals 116 REVISED EDITION

Vv. Comparing Effect Sizes: Early Comparisons, Focused : |

and Diffuse HW7

Vi. Comparing Effect Sizes: More Recent Comparisons 120 :

VI. A One-Sample Effect Size Index: 1 123 : The purpose of this second edition is simple. It is to make readily accessible

7. The Evaluation of Meta-Analytic Procedures and some newer meta-analytic procedures that have become available since the |

Meta-Analytic Results 127 : publication of the first edition. These newer procedures along with the basic |

procedures also described will make it possible for readers to conduct their

L Sampling Bias and the File Drawer Problem 128 | own meta-analyses and to evaluate more wisely the meta-analyses conducted |
Il. Loss of Information 128 / by others. L

| II. Problems of Heterogeneity 129 In the years since publication of the first edition Donald B. Rubin has i
IV. Problems of Independence 130 continued to tutor me in matters meta-analytic and otherwise quantitative. |

V. Exaggeration of Significance Levels 132 : When we collaborate, and we frequently do, he does all the work that is hard

VI. The Practical Importance of the Estimated Effect Size 132 (for me, not for him) and original. Then he insists we publish alphabetically. ;

| | Whatacountry! . i
| References 137 /
—Robert Rosenthal

Index 149 '

About the Author 155 7 : :

|

a

vil

----- Page 6 (ocr) -----
My interest in meta-analysis grew out of a research result I couldn’t quite
: believe. For reasons chronicled elsewhere (Rosenthal, 1985b), I conducted
/ some studies to investigate the effect of psychological experimenters’ expec-
tations on the responses obtained from: their research subjects. These studies
] suggested that experimenters’ expectations might indeed affect the results of
; their research. In the late 1950s that result was not very plausible — not to me
: and not to my colleagues.
7 Along series of replications followed which eventually persuaded me that
there must be something to the phenomenon of interpersonal expectations.
: Since the early 1960s I have been combining and comparing the results of
/ series of research ‘studies dealing with experimenters’ and others’ expecta-
tions (Rosenthal, 1961, 1963). The basic quantitative procedures for com-
bining and comparing research results were available even then (Mosteller
7 & Bush, 1954; Snedecor, 1946). i
7 In the mid-1960s I began teaching a variety of meta-analytic procedures
in courses on research methods though they were not then called meta-
] analytic. Neither this teaching nor my writing employing meta-analytic
procedures seemed to have much effect on the probability of others’ employ-
ing these procedures. What did have an effect on others’ employing meta-
| analytic procedures was an absolutely brilliant paper by Gene V Glass
(1976a, 1976b). In this paper, Glass named the summarizing enterprise as
“meta-analysis” and gave an elegant example of a way of doing a meta- i
| analysis. In the process, he persuaded me, a former psychotherapist, that
/ I probably had helped those patients I’d thought I'd helped.
Since this early work by Glass, and the subsequent work with his col-
leagues frequently cited in this book, there has been an extraordinary rate of
: production of meta-analytic research. Since the late 1970s, there have been
/ hundreds of published and unpublished ‘meta-analyses. |
7 The table of contents and the introductory chapter tell in detail what is in
| this book. Its purpose, very briefly, is to describe meta-analytic procedures
' in sufficient detail so that they can be carried out by readers of this book and

----- Page 7 (ocr) -----
x META-ANALYTIC PROCEDURES 7
so that they can be wisely evaluated when they have been carried out by ]
others. -
The book was designed to be used by advanced undergraduate students,
graduate students, and researchers in the social and behavioral sciences. The
level of mathematical sophistication required is high school algebra. The 2
level of statistical sophistication required is about half-way through a second
course in data analysis (e.g., Rosenthal and Rosnow, 1984a; 1991). A NOTE ON THE
I am doubly grateful to the National Science Foundation: first, for having REVISED EDITION
frequently supported, since 1961, the substantive research on interpersonal /
expectations —the research that gave me something to meta-analyze; and |
second, for having supported in part the development of some of the meth- . .s . .
odological procedures to be described in this book. This revision makes readily accessible some newer meta-analytic procedures
Let me also thank especially these people: Frederick Mosteller for having that have been developed since the 1984 edition. A new effect size indicator,
markedly enlarged my horizons about meta-anal ytic procedures some 20 I, for one-sample data is introduced as is anew coefficient of robustness of .
years ago; Jacob Cohen, a fine colleague I have never met, but whose writings / replication. Procedures for combining-and comparing effect sizes for multi-
about power and effect size estimation have influenced me profoundly; and ple dependent variables are described and new data are reported on the
Donald B, Rubin, a frequent collaborator and my long standing tutor on | magnitude of the problem of incomplete retrieval (the file drawer problem).
matters meta-analytic and otherwise quantitative. I have described our col- / Finally, new results are provided on the social, psychological, economic, and
laboration to students as follows: “I ask him questions and he answers them.” 7 medical importance of small effect sizes.
Clearly, the ideal collaboration! . a
The manuscript was improved greatly by the suggestions of Len Bickman, 1
Debra Rog, Harris Cooper, and an anonymous reviewer, and it was superbly 7
typed by Blair Boudreau, whose legendary accuracy ruins one’s skill at,and .
motivation for, proofreading. | ;
Finally, I thank MaryLu Rosenthal for what she taught me about biblio- ' =,
graphic retrieval in the social sciences (M. Rosenthal, 1985) and for the |
countless ways in which she improved the book and its author. | a
—R. R. 7
.

----- Page 8 (ocr) -----
| Introduction
Two sources of pessimism in the social sciences are discussed. Early examples of meta-
. analytic procedures are given that illustrate (1) summarizing relationships, (2) determin- |
: ing moderator variables, and (3) establishing relationships by aggregate analysis. The
: current status of meta-analytic procedures is described and an empirical evaluation of the :
_ employment of meta-analytic procedures is provided.
There is a chronic pessimistic feeling in the social and behavioral sciences
that, when compared to the natural sciences, our progress has been exceed-
ingly slow, if indeed there has been any progress at all. From time to time
this chronic state erupts into an acute condition, or crisis, precipitated in /
: part by “local” (i.e., disciplinary) developments. For example, in the disci- '
pline of social psychology, the precipitating factors leading to prolonged
crisis have been brilliantly analyzed by Ralph Rosnow (1981) in his book,
Paradigms in Transition. It seems a good bet, however, that had we been ;
: doing better as a science on a chronic basis, our acute crisis would have been '
| less severe. Two general purposes of this book are to describe quantitative
| procedures that will show (1) how we can “do better” than we have been :
| doing, and (2) how we have, in fact, been “doing better” than we think we
/ have been doing.
J. TWO SOURCES OF PESSIMISM U
| IN THE SOCIAL SCIENCES! |
/ L.A. Poor Cumulation :
- One of the two sources of pessimism in the social sciences, the one which |
is the focus of this book, is the problem of poor cumulation. This problem |
7 refers to the observation that the social sciences do not show the orderly
7 progress and development shown by such older sciences as physics and chem- /

----- Page 9 (ocr) -----
4 META-ANALYTIC PROCEDURES INTRODUCTION 5

istry. The newer work of the physical sciences builds directly upon the older | Thus, the complaint goes, even if some social action program works, or if

work of those sciences. The social sciences, on the other hand, seem almost to : some new teaching method works, or if psychotherapy works, the size of the |

be starting anew with each succeeding volume of our scientific journals. | =ffect is likely to be so small that it is of no practical consequence whatever.

While it appears that the natural and physical sciences have problems of | One specific purpose of this book is to describe a procedure for helping |

their own when it comes to successful cumulation (Collins, 1985; Hedges, _ us to evaluate the social importance of the effects of any independent varia- :

1987; Hively, 1989; Koshland, 1989; Mann, 1990; Pool, 1988, 1989; Taubes, - ble. This is done in detail in the final chapter. :

1990), there is no denying that in the matter of cumulating evidence we have .

much to be modest about. Poor cumulation does not seem to be due primarily /

; to lack of replication or failure to recognize the need for replication. Indeed, . META. eC ROC RD 5 :
the calls for further research with which we so frequently end our articles are 2 URE U

carried wherever our scholarly journals are read. It seems rather that we have a Early applications of meta-analytic procedures were of three types. The :

been better at issuing such calls than at knowing what to do with the answers. : first type was that in which the goal was to summarize for a set of studies what |

There are many areas of the social sciences for which we do have the results . the overall relationship was between two variables that had been investigated

of many studies all addressing essentially the same question. Our summaries . in each study. Often this goal was approached by trying to estimate the aver- ,

of the results of these sets of studies, however, have not been nearly i age relationship between two variables found in a set of studies. Often this :

informative as they might have been, either with respect to summarized goal was approached by significance testing, i.e., by trying to determine the

significance levels or with respect to summarized effect sizes. me the fest . probability that the relationship obtained could have been obtained if, in the /

i isti toldus lation from which the studies had been sampled, the true relationshi '

reviews of research by the most sophisticated workers have rarely told u populati pled, p

more about each study in a set of studies than the direction of the relationship were Zero. :
between the variables investigated and whether or not a given p level was The second type of early application of meta-analytic procedures was not °

attained. This state of affairs is beginning to change. More and more reviews so much concerned with summarizing the relationship between two varia- |

of the literature are moving from the traditional literary format te the / bles, but with determining the factors that were associated with variations '

itati verviews see Cooper, 1984, 1989b; Glass, McGaw, in the magnitude of relationships between the two variables (i.e., the factors :

quantitative format (for ove per, menitt 1900: Hunter, ‘ i ; ups |

& Smith, 1981; Hedges & Olkin, 1985; Hunter & Schmidt, ; Hunter, that served as moderator variables). ;

Schmidt, & Jackson, 1982; Light & Pillemer, 1984; Mullen, 1989; Mullen & : The third type of early application did not examine any relationship

Rosenthal, 1985; Rosenthal, 1980, 1984). within each study. Instead, each study provided only aggregated data for ;

Three more specific purposes of this book relevant to the problem of each variable, for instance, the average attitude held by the participants in a |
poor cumulation include the following: : study or their average level of cognitive performance. These aggregated or 7

*5 “results” Jearly than is our custom in / averaged data were then correlated with each other or with other character-

(1) Defining the concept of a study's “results” more clearly . istics of the study to test hypotheses or to suggest hypotheses to be tested in |

the social sciences. . | ‘ 7 ; i

(2) Providing a general framework for conceptualizing meta-analysis, i.e. the subsequent specifically designed studies. _

antitative summary of research domains / To summarize the differences among these three types of early application .
(3) illustrating the quantitative procedures within this framework so they can be - of meta-analytic procedures we can say that: (1) the first type generally re- ,

applied by the reader and/or understood more clearly when applied by oth- sulted in an estimate of the average correlation (or the combined p level asso- :
ers. / ciated with that correlation) found in all the studies summarized; (2) the sec- |
_ ond generally resulted in a correlation between some characteristic of the a
LB. Small Effects ; . ; f / studies and the correlation (or other index of the size of the effect) found in |
The second source of pessimism in the social sciences on which we focus the studies; and (3) the third simply correlated mean data obtained from each t
in this book is the problem of small effects. Even when we do ihe tect te ____ study with other mean data or with other characteristics obtained from each |
up with a possibly replicable result, the practical magnitude of the ette ___. Study. We turn now to some examples of these three types of early application. (
' almost always small, i.e., accounts for only a trivial proportion of the variance. - '

----- Page 10 (ocr) -----
6 META-ANALYTIC PROCEDURES | INTRODUCTION 7

i.A. Summarizing Relationships . | menters had served as subjects in the same task they were now administer-

One of our early examples is drawn not from social but from agricultural __ ing to others in their role of experimenter. The correlations could therefore

science. Jay Lush (1931) investigated the relationship between the initial __ be obtained between the performance of experimenters at a given task and :

weight of steers and their subsequent gain in weight. Lush had six samples __ the average performance of those experimenters’ subjects on the same task. |

of steers available and he was interested in computing the average of the six __ The application of Snedecor’s test showed the eight correlation coefficients /

correlations he had available (median r = .39). _ to be significantly heterogenous (Rosenthal, 1961, 1963). |

What made these six samples of steers famous was not Lush’s averaging _ Snedecor’s textbook example illustrated both the computation of average :

of correlations, but George Snedecor’s (1946) putting the six correlations _ r’s and a test for heterogeneity of 1’s. What his example did not illustrate was

into his classic textbook of statistics as an example of how to combine corre- _ an overall test of significance to help us judge the probability that the partic-

lation coefficients. Subsequent editions have retained that famous example _ ular set of r’s with their associated tests of significance could have been |

(e.g., Snedecor & Cochran, 1980, 1989). Snedecor’s long-time coauthor obtained if the true value of rin the appropriate population were zero. Had : ' !

William G. Cochran had himself been a pioneer in the field of meta-analysis. Snedecor wanted to he could readily have illustrated the process of combin- 7

Early on he had addressed himself to the statistical issues involved in com- / ing probability levels. At least two major figures in the history of mathemat- 7

paring and combining the results of series of studies (Cochran, 1937, 1943). _ ical statistics, Ronald Fisher (e.g., 1932, 1938) and Karl Pearson (1933a,

In his textbook example, Snedecor (1946) did much more than show how 1933b) had already described procedures for combining probabilities. Even [

to combine estimates of magnitudes of relationships (1’s). He also showed __ earlier, Tippett (1931) had described a related procedure that did not exactly

how to assess the heterogeneity of a set of correlation coefficients. That is, | combine probabilities but “protected” the smallest obtained p by multiplying |

he showed how a ¥ test could be employed to help us judge whether, over- __ it by the number of tests of significance examined.

all, the correlations differed significantly from each other. | Mosteller and Bush (1954) broadened the Fisher and Pearson perspec- 7

Moving from an agricultural to a social science example, my own early . tives and made several methods of combining independent probabilities

meta-analytic research was also concerned with estimating average correla~ available to social scientists in general and to social psychologists in particu- c

tions. In one summary of what was then known about the effects of experi- lar. An early and ingenious application of a method of combining probabili- L

menters’ expectancies on the results of their research, the average correla- a ties was described by Stouffer and his colleagues (1949). For three samples

tions (based on a number of studies each based on a number of of male soldiers, data were available on the favorability of their view of D

experimenters) were reported between experimenters’ expectancies fortheir women soldiers as a function of the presence of women soldiers at their own

subjects performance and how their subjects subsequently did perform (Ro- _ camp. Male soldiers tended to be more unfavorable to women soldiers (as L

: senthal, 1961, 1963). These average correlations were computed separately defined by not wanting their sisters to join the Army) when women soldiers 7
for experimenters who were explicitly encouraged to bias their results (me- were at their camp. 7

dian r = —.21) and those who were not (median r = .43). Atest (contrast) Returning to our examples of studies of experimenter expectancy effects -

was then performed to help judge whether these average correlations dif- we find illustrations of the application of the Stouffer method of combining |

fered significantly from each other. They did differ, suggesting that, while probabilities. After the first three experiments showed the effects of experi- |

under ordinary circumstances experimenters tended to get the results they mentally-created expectations on the results of their research, the three
expected to get, they tended to get significantly opposite results when they _ probability levels obtained were combined to give an overall test of signifi- :
felt unduly influenced (or even bribed) to bias the results of their research __ cance for the set of three studies (Rosenthal, 1966). :
: (Rosenthal, 1961, 1963). Analogous analyses were performed on a series of |
studies investigating the relationship between experimenters’ personality ILB. Determining Moderator Variables
and the extent to which they obtained data affected by their expectancy (Ro- 7 In this section we describe an early application of meta-analytic proce-
senthal, 1961, 1963, 1964). | dures in which the goal was not to establish an overall relationship between :

Snedecor’s textbook example of testing for the heterogeneity of a set of | two variables, but to determine the factors that were associated with varia- L
| correlation coefficients was also applied to the study of experimenter ef- 1 tions in the magnitudes of the relationships between two variables. Such |
fects. In one such analysis, eight studies could be found in which experi- 7 factors are known as moderator variables because they moderate or alter :

: the magnitude of a relationship. |

----- Page 11 (ocr) -----
8 META-ANALYTIC PROCEDURES = jyTRODUCTION 9
Anearly application was by Thorndike (1933). He obtained the results of / TABLE 1.1

36 independent studies of the retest reliability of the Binet test of intelli- / Illustration of Differences in Summarizing Relationships, Determining |

gence. Thorndike was not interested in an overall estimate of the retest relia- Moderator Variables, and Establishing Relationships

bility per se, but in how the magnitude of the retest reliability correlation - EEE |

varied as a function of the time interval between the first and second testing. __ 4 3 of |

As might be expected, the greater the interval, the lower the retest reliabil- / h Correlations Between Mean Ratings Mean Level [

; : . _ eacher Expectations and of Teacher of Pupil a

ity. These intervals ranged from less than one month to 60 months with a / Study ____ Pupil Performance(r) ___— Excellence _Performance __ i

median interval of about two years. / I 25 8 15 |

Thorndike did not report an overall estimate of the retest reliability _ 2 .20 9 110 /

(r = .84) or the correlation between the magnitude of the retest reliability . 3 30 9 105 /

and the retest time interval (r(34) = —.39). He did report the estimated . ; i : too

reliabilities separately for various retest intervals; e.g., less than one month _ 6 10 5 95 ;

(r = .95) to nearly five years (r = .81). (The average reliabilities reported | Mean Py 73 105.0 i

here are not those reported by Thorndike but are corrected estimates. ) . NOTES. The mean of column A ilustrates the summarizing function; the correlation between :

A somewhat later example of the use of moderator variables is drawn columns A and B (r = .59) (and A and C;r = .53) illustrates the examination of moderator variables; :

from the research program on experimenter effects mentioned earlier. Eight / a This ta Be eaae aira hveathotcal example the attempt to establish a relationship (r =

studies had been summarized in each of which the performance of experi- —__ ,

menters at a given task could be correlated with the average performance of | contributing to the aggregate analysis, however, since it is characteristic of |

those experimenters’ subjects on the same task. Rosenthal (1963, 1964) was . ; |

; . ; ; ; _ aggregate analyses to yield larger correlations.) |

interested in learning the degree to which these correlations changedfrom 7 :

the earlier to the later studies. He found a significant and substantial . .. . . '

(r = .81) effect of when the study was done; studies conducted earlier ob- | nD. sionships Mets. Avoivtiealh” and Establishing

tained significantly more positive correlations (relative to later-conducted | As areview of the differences among the early meta-analytic procedures

studies) while later-conducted stud ies obtained more negative correlations . designed for the three different purposes we have illustrated so far, Table |

(relative to earlier-conducted studies). 2 1.1 has been prepared, a hypothetical example illustrating differences in |

summarizing relationships, determining moderator variables, and estab-

ILC. Establishing Relationships by Aggregate Analysis lishing relationships. Column A shows the results of six studies of teacher ex- |

In this section we describe an early application of the meta-analytic . pectancy effects expressed for each study as the correlation between teacher /

procedure wherein each study provides only aggregated (average) datafor expectations and pupil performance. Column B shows the mean rating of the

each variable. / excellence of the teachers employed in each of the six studies. Column C
Underwood (1957) was interested in the relationship between the degree / shows the mean level of pupil performance found for all the children of each
| of retention of various kinds of learned materials (e.g., geometric forms, of the six studies.
: nonsense syllables, nouns) and the number of previous lists of materials that / The summarizing function of meta-analysis is illustrated by the mean of
had been learned. He hypothesized that the more lists that had been learned _ Column A, i.e., the mean magnitude of the relationship between teacher
: before the recall tests, the greater would be the forgetting. Underwood expectations and pupil performance.
/ found 14 studies that each yielded both required facts: percentage recalled The determination of moderator variables is illustrated by the correla-
: after 24 hours and the average number of previously learned lists. His hy- _ tion of the data in Column A and the data in Column B. That correlation
| pothesis was strongly supported by the data. The correlation between these __ (© = .59) shows that larger effects of teacher expectations are associated /
| two variables (r based on ranks) was dramatically large: r(12) = —.91! (We _ with teachers who have on the average been judged to be more excellent. |
would not expect to find correlations that large within the individual studies __ Another illustration of moderating effects is found in the correlation be-
| tween Column A and Column C. That correlation (r = .53) shows that

----- Page 12 (ocr) -----
10 META-ANALYTIC PROCEDURES | INTRODUCTION 11 |
larger effects of teacher expectations are associated with pupils who have on _ growing number of investigators who have been discussing, employing, and q
the average shown higher levels of performance. - developing a variety of meta-analytic procedures. These investigators in-
The attempt to establish a relation by aggregate analysis is illustrated by . clude Bloom, 1964; Cook & Leviton, 1980; Cooper, 1979, 1982, 1984, |
the correlation of Columns B and C. That correlation (r = .78) shows that __ 1989a; Cooper & Hazelrigg, 1988; Cooper & Rosenthal, 1980; DePaulo,
higher levels of mean pupil performance are associated with higher levels of . Zuckerman, & Rosenthal, 1980; Dusek & Joseph, 1983; Eagly & Carli, 1981; |
rated teacher excellence. It might be tempting to interpret this correlation to _ Feldman, 1971; Fiske, 1983; Glass, 1976, 1980; Glass & Kliegl, 1983; Glass '
mean that better teachers produce higher levels of pupil performance but that / etal., 1981; Green & Hall, 1984; Hall, 1980, 1984; Harris & Rosenthal, 1985, .
cannot properly be inferred from the correlation obtained. It would take a | 1988; Hedges, 1981, 1982a, 1982b, 1982c, 1983a, 1983b; Hedges & Olkin, |
freshly designed study to establish properly the causal factors, if any, contrib- | 1980, 1982, 1983a, 1983b, 1985; Hunter & Schmidt, 1990; Hunter et al.,
uting to the obtained correlation. Similarly, the correlations describing the - 1982; Kulik, Kulik, & Cohen, 1979; Light, 1979; Light & Pillemer, 1982; a
operation of moderator variables cannot be interpreted causally in most cases _ 1984; Light & Smith, 1971; Mintz, 1983; Mullen, 1989; Mullen & Rosenthal, i
since we did not randomly assign studies to the various levels of the modera- _ 1985; Pillemer & Light, 1980a, 1980b; Rosenthal, 1963, 1964, 1968, 1969, a
tor variables. Cooper (1984, 1989b) has made this point clearly and forcefully _ 1976, 1978, 1979, 1980, 1982, 1983a, 1983b, 1983c, 1985a, 1985b, 1986, 7
in his book in this series. Causal inferences, however, can be made about the | 1987a, 1987b, 1990; Rosenthal & DePaulo, 1979; Rosenthal & Rosnow, |
results of the studies being summarized if these results are based on experi- | 1975; Rosenthal & Rubin, 1978a, 1978b,:1979a, 1980, 1982b, 1982c, 1983, |
ments involving random assignment of subjects to treatment conditions. 1984, 1986, 1988, 1989, 1991; Shapiro & Shapiro, 1983; Shoham-Salomon
___ & Rosenthal, 1987; Smith, 1980; Smith & Glass, 1977; Smith et al., 1980; ,

if. THE CURRENT STATUS OF Strube, Gardner, & Hartmann, 1985; Strube & Hartmann, 1983; Sudman & :

META-ANALYTIC PROCEDURES Bradburn, 1974; Taveggia, 1974; Viana, 1980; Wachter & Straf, 1990; L

We have now examined several early examples of meta-analytic proce- _ Walberg & Haertel, 1980; Wilson & Rachman, 1983; Wolf, 1986; Zucker- |
dures, some going back over half a century. Although several of the proce- . man, DePaulo, & Rosenthal, 1981, and the many others cited in the references '
dures have been available for many years (the present writer has been em- . of these workers. ~ |
ploying some for about 30 years), there has been no revolution in how we . In the pages that lie ahead we consider in detail how to employ a variety i
conduct reviews of the literature or summarize domains of research. That is, of meta-analytic procedures. Our procedures are not perfect, we can use J
most reviews of the literature still follow a more traditional narrative style. them inappropriately, and we will make mistakes. Nevertheless, the alter-
However, there may be a revolution in the making. As evidence, consider native to the systematic, explicit, quantitative procedures to be described is .
that in their analysis of the number of publications on meta-analysis from __ even less perfect, even more likely to be applied inappropriately, and even |
the years 1976 to 1982, Lamb and Whitla (1983) found a strong linear in- __ more likely to lead us to error. There is nothing in the set of meta-analytic |
crease (r = .85) from the 6 papers of 1976 to the 120 papers of 1982. Since _ procedures that makes us less able to engage in creative thought. All the
that time the rapid increase in the employment of meta-analytic ideas and : thoughtful and intuitive procedures of the traditional review of the litera- i
meta-analytic procedures is continuing (Hunter & Schmidt, 1990). | ture can also be employed in a meta-analytic review. However, meta- :
The work that probably did the most to capture the imagination of the | analytic reviews go beyond the traditional reviews in the degree to which :
social sciences as to the value of meta-analytic procedures was the brilliant __ they are more systematic, more explicit, more exhaustive, and more quanti- i
meta-analytic work of Gene Glass and his collaborators. Specifically, Glass __ tative. Because of these features, meta-analytic reviews are more likely to L
and his colleagues, employing meta-analytic procedures very similar to / lead to summary statements of greater thoroughness, greater precision, and 7
those of the present writer (Rosenthal, 1961, 1963, 1969, 1976; Rosenthal _ greater intersubjectivity or objectivity (Kaplan, 1964). In the final chapter ‘
& Rosnow, 1975) but developed independently, were able to demonstrate of this book we consider systematically the several criticisms that have been |
dramatically the effectiveness of psychotherapy (Glass, 1976a, 1976b, | made of meta-analytic procedures and products.
1977; Smith & Glass, 1977; Smith, Glass, & Miller, 1980). Partly because __ : |
of the work of Glass and his group, the last few years have shown a rapidly :

----- Page 13 (ocr) -----
| 12 META-ANALYTIC PROCEDURES -
J

IV. AN EMPIRICAL EVALUATION | i
| Harris Cooper and | were interested in assessing empirically the effects ]
of employing meta-analytic procedures on the conclusions drawn by inves- | |
tigators in training (i.e., graduate students) and experienced investigators _ |
(i.e., faculty members) (Cooper & Rosenthal, 1980). The basic idea was to 7
ask the participants to conduct a review of the literature to address the ques- / |
tion of sex differences in task persistence. Some of the participants were | 7
randomly assigned to the meta-analytic procedure condition, and some | 2
were randomly assigned to the traditional procedure condition. All of the | u
participants were given the same seven studies that we knew beforehand wo i;
significantly supported overall the hypothesis that females showed greater De fining Research Results |
task persistence. . 7
There was a total of 41 participants initially blocked on sex and faculty | |
(versus graduate student) status. However, since neither of these variables - The concept of “research results” is clarified and the relationship between tests of signifi- , | |
affected the results of the experiment, results were reported for all 41 par- _ cance and estimates of effect sizes is emphasized. Various types of effect size estimates and
ticipants combined. Participants assigned to the meta-analytic procedure ata ipte correlated reeults are described, Finally, methods of dealing with the problem :
condition were asked to record the significance level of each study and were / , a
given detailed instructions on how to combine these significance levels to __ 7
obtain an overall test of significance for the entire set of seven studies. Par- | Much of the rest of this book-will deal with quantitative procedures for i
ticipants assigned to the traditional procedure condition were asked to em- | comparing and combining the results of a series of studie P Before these :
ploy whatever procedures they would normally employ to conduct a review | procedures can be discussed meaningfully, however, we must become ex. |
of the literature. - plicit about what we mean when we refer to the results of an individual i
After participants completed their reviews, they were asked whether the _ study.
evidence supported the conclusion that females were more task persistent / We begin by stating what we do not mean when we refer to the results of a J
than males. They could respond: definitely yes, probably yes, can’t tell, study: We do not mean the conclusion drawn by the investigator, since that |
probably no, or definitely no. Participants were also asked to estimate the - is often only vaguely related to the actual results. The metamor, hosis that |
magnitude of the relationship between gender and persistence. To this ques- / sometimes occurs between the results section and the discussion section is i
tion they could respond: none at all, very small, small, moderate, large, and / itself a topic worthy of detailed consideration. For now it is enough to note :
very large. ; ; that a fairly ambiguous result often becomes quite smooth and rounded in |
Despite the fact that the set of seven studies reviewed showed a clearly the discussion section, so that reviewers who dwell too much on the discus- |
significant relationship between sex and task persistence, 73% of the tradi- __ sion and too little on the results can be quite misled as to what actually w :
tional reviewers found probably or definitely no support for the hypothesis __ found. y was ]
compared to only 32% of the meta-analytic reviewers. That difference (sig- We also do not mean the result of an omnibus F test with df > 1 in the :
nificant at p < .005), suggests that traditional methods of reviewing may _ numerator or an omnibus y” test with df > 1. In both cases we are gettin |
suffer a very considerable loss of power relative to meta-analytic methods. - quantitative answers to questions that are often—perhaps usuall ee he |
Put another way, the incidence of type IL errors (failing to reject null hypoth- lessly imprecise. Only rarely is one interested in knowing for an fixed. |
eses that are false) may be far greater for the traditional than for the meta- factor analysis of variance or covariance that somewhere in the thicket of df |
analytic procedures of summarizing research domains. | there lurk one or more meaningful answers to meaningful questions that J]
| we had not the foresight to ask of our data. Similarly, there are few occa- '

NOTE _ sions when what we really want to know is that somewhere in a contingency |

1. Throughout this book reference to the social sciences or to the behavioral sciences will. __ 7
refer to both the social and behavioral sciences. . 7
aaa |

----- Page 14 (ocr) -----
14 META-ANALYTIC PROCEDURES | DEFINING RESEARCH RESULTS 15
| table there is an obtained frequency or two that has strayed too far fromthe TABLE 2.1 L
frequency expected for that cell under the null hypothesis. . Examples of the Relationship Between Tests of Significance .
What we shall mean by the results is the answer to this question: What is _ and Effect Size: y7(1), Z, and t i
| the relationship between any variable X and any variable Y? The variables | Sp op |
| i i i : : - est 0 _ Size of Size 9, .
| Xx and Y are chosen with only the constraint that their relationship be of Equation __ Significance = Effect x Stu oo |
| interest to us. The answer to this question must come in two parts: (1) the ea aes :
| estimate of the magnitude of the relationship (the effect size) and (2)an 32 z
indication of the accuracy or reliability of the estimated effect size (asina : 7 ? * VN :
confidence interval placed around the estimate). An alternative to the sec- 2.3 t = + x Vt a

ond part of the answer is one not intrinsically more useful but one more _ Vin? i

consistent with the existing practices of social researchers, that is, the test of _ 2.4 t = Cc “My x 1 :

significance of the difference between the obtained effect size and the effect | s a L

size expected under the null hypothesis of no relationship between variables | ny ny 7

X and ¥: 2.5 t = (ace) x [rity ]

L. EFFECT SIZE AND 2.6 t _ (5 "ye | vi aE a

STATISTICAL SIGNIFICANCE - \ ¢° (ny + ny) |

Since the argument has been made that the results of a study with respect | 27 t = d * Vat :

to any given relationship can be expressed an an estimate of an effect size © ce ee

es Ls : : oS a. Also called g (Hedges, 1981, 1982a). a

plus a test of significance, we should make explicit the relationship between —_, b.. Also called d (Cohen, 1969, 1977, 1988), |

these two quantitities. The general relationship is shown below: -

Test of - Size of x Size of - . TABLE 2.2 i

Significance Effect Study . Examples of the Relationship Between Tests of Significance

. and Effect Size: F and t for Correlated Observations |

Tables 2.1 and 2.2 give useful specific examples of this general equation. ; —iestof———=—“‘~=t*é‘s*é‘«SS te OfsCi“‘aC=SC*« CCHS |

Equation 2.1 shows that x’ on df = | is the product of the size of the effect | Equation __ Significance Effect, Study i

. expressed by ¢? (the squared product moment correlation) multiplied by N / 2.8 Fa = a x df error _

(the number of subjects or other sampling units). It should be noted that dis rr |

merely Pearson’s r applied to dichotomous data, i.e., data coded as taking on 7 2.9 po = eta2 x df error_ /

only two values such as 0 and 1, land 2,or+land—1. . 1 — eta? df means |

Equation 2.2 is simply the square root of equation 2.1. It shows that the 2.10 pb _ 52 means x ,

standard normal deviate Z (i.e., the square root of x2on 1 df) isthe product S2

of (the product moment correlation) and VN. Equation 2.3 showsthattis © __ _

the product of the effect size r/V1 — rand V df, an index of the sizeofthe Jinn .

study. The denominator of this effect size (V1 — 1) is also knownasthe | Int 1

coefficient of alienation or k, an index of the degree of noncorrelation = 2.12 te = Bb x Va

(Guilford & Fruchter, 1978). This effect size, therefore, can be rewrittenas Sp /

1/k, the ratio of correlation to noncorrelation, a kind of signal-to-noise ratio. / 2.13 tc = a’ x Var i

Equations 2.4 and 2.5 share the same effect size, the difference between the oC SE SSSSSSSSSSSSSSSSSCW(U i

wo . - a. Numerator df = 1. 7

means of the two compared groups divided by, or standardized by, the / b. numerator af may take on any value. :

unbiased estimate of the population standard deviation. _ c, Correlated observations. |

----- Page 15 (ocr) -----
7 16 META-ANALYTIC PROCEDURES | pEFINING RESEARCH RESULTS 7
This latter effect size (M, — M2)/S is the one typically employed by Glass for just as r? does, but eta? carries no implication that the relationship be-
and his colleagues (1981) with the S computed as[> (X ~ X)?/(n, —1) 1% tween the two variables in question is linear. Equation 2. 10 shows the effect
employing only the subjects or other sampling units from the control group, 7 size for F as the ratio of the variance of the condition means to the pooled
The pooled S—that is, the one computed from both groups—tends to pro- - within group variance, while the size of the study is indexed by n, the num-
vide a better estimate in the long run of the population standard deviation. _ ber of observations in each of the groups. Because we rarely employ fixed
However, when the S’s based on the two different conditions differ greatly . effect F tests with df > 1 in the numerator in meta-analytic work, equations
from each other, choosing the control group S as the standardizing quantity _ 7 9 and 2.10 are used infrequently in summarizing domains of research.
is a very reasonable alternative. That is because it is always possible that the - :
experimental treatment itself has made the S of the experimental group too | LA: Comparing r tod
large or too small relative to the S of the control group. | Equation 2.11 has for its test of significance a t for correlated observa-
Another alternative when the S’s of the two groups differ greatly is to / tions or repeated measures. It is important to note that this equation for the
transform the data to make the S’s more similar. Such transformations (¢.g., : correlated t is identical to equation 2.3 (Table 2.1) for the independent sam-
logs, square roots, etc.) of course require our having access to the original _ ples t. Thus when we employ r as our effect size estimate, we need not make
data, but that is also often required to compute S separately for the control __ any special adjustment in moving from t tests for independent to those for
group. When only a mean square error from an analysis of variance is availa- | correlated observations. That is not the situation for equations 2.12 and
ble we must be content to use its square root (S) as our standardizing denom- __ 2.13, however. When the effect size estimates are the mean differences
inator in any case. Or if only the results of at test are given, we are similarly : divided either by S or by a, the definition of the size of the study changes by
forced to compute the effect size using a pooled estimate of S. (We could use . a factor of 2 in going from t for independent observations to t for correlated
equations 2.4 or 2.5 to solve for (M, — M.)/S.) - observations. This inconsistency in definitions of size of study is one of the
Before leaving the topic of whether to compute S only from the control | reasons I have grown to prefer r as an effect size estimate rather than d, after
group or from both groups we should remind ourselves of the following: | many years of using both r and d.
When S’s differ greatly for the two groups so that we are inclined to com- | Another reason for preferring r over d as an effect size estimate is that we
pute S only from the control group, ordinary t tests may give misleading - are often unable to compute d accurately from the information provided by
results. Such problems can be approached by approximate procedures (Sne- - the author of the original article. Investigators sometimes report only their
decor & Cochran, 1989, pp. 96-98) but are perhaps best dealt with by ap- / t’s and df’s but not their sample sizes. Therefore, we cannot use equations
propriate transformation of the data (Tukey, 1977). - 2.4, 2.5, or 2.6 to compute the effect sizes. We could do so only if we
Equation 2.6 shows an effect size only slightly different from that of . assumed n, = np. If we did so, for example, from rearranging equation 2.7,
equations 2.4 and 2.5. The only difference is that the standardizing quantity - we could get d as follows:
for the difference between the means is o (pooled sums of squares divided 7 a
by N) rather than S (pooled sums of squares divided by N —k for k groups). | d= Vat
This is one of the effect sizes employed by Cohen (1969, 1977, 1988) and by _ [2.14]
Friedman (1968). Basically this index, Cohen’s d, is the difference between /
the means of the groups being compared given in standard score units or _ If the investigator’s sample sizes were equal, d would be accurate, but as |
z-scores. Equation 2.7 shows (M, — M,)/o expressed as d and the size of study - nj, and nj become more and more unequal, d will be more and more underes- |
term simplified considerably for those situations in which it is known or in - timated. Table 2.3 shows for eight studies, all with t = 3.00 and df = n, + n,
which it can be reasonably assumed that the sample sizes (n, and n,) are equal. | = 2 = 98, the increasing underestimation of d when we assume equal n’s
: Equation 2.8 of Table 2.2 shows that F with one df in the numerator is the _ and employ equation 2.14. It should be noted, however, that when the split
product of the squared ingredients of the right hand side of equation 2.3 of | _is no more extreme than 70:30 the underestimation is less than 8%. :
: Table 2.1. That is just as it should be, of course, given that t? = F when df = A third reason for preferring r to d as an effect size estimate has to do
1 in the numerator of F. _ with simplicity of interpretation in practical terms. In the final chapter of
Equation 2.9 is the generalization of equation 2.8 tothe situationofdf> _ this book we describe the BESD (binomial effect size display), a method for
1 in the numerator. Thus eta? refers to the proportion of variance accounted displaying the practical importance of the size of an obtained effect. Using
; 7 this method we can immediately convert r to an improvement in success rate

----- Page 16 (ocr) -----
i 18 META-ANALYTIC PROCEDURES DEFINING RESEARCH RESULTS 19
TABLE 2.3 found by rearranging equations 2.1, 2.3, and 2.8 (Cohen, 1965; Friedman, |

Underestimation of d by “Equal n” Formula 1968): :

/ Raw Underestimated | |
Study ny  m,  Accurated* Estimatedd> _ Difference (in percentages) _ b= /X) |

1 50 50 61 61 00 00 | N [2.15]

2 60 40 62 61 ~.01 02 |

3 70 30 66 61 —.05 .08 |

4 80 20 16 61 ~.15 20 _ |

5 90 0 1.01 61 ~.40 40 7 . |

6 95 5 1.39 61 ~.78 56 . r= f_¥_ i

7 98 2 2.16 61 1.55 72 _ + df [2.16] u

8 91 3.05 61 ~2.44 80 | |

ad= 01+") = General formula from rearranging equation 2.6. | where df = n, + ny — 2, and |

bd=_2 = “equal n” formula from rearranging equation 2.7. / r= / Fd,—-) |

Vat | F(1,~) + df error [2.17] |

associated, for example, with employing a new treatment procedure or _ indi ith i i

, ple, Ploy gal P a where F(1, —) indicates any F with df = 1 in the numerator.

new selection device, or anew predictor variable. Because of the probabil. In case none of these tests of significance have been employed or re- i

ity of seriously misinterpreting Its practical importance (as discussed in / ported, we can usefully estimate an effect size r from a p level alone as long i

Chapter 7), we shall not user as an effect size estimate (Rosenthal & Rubin, | as we know the size of the study (N). We convert the obtained p to its stan- |

1982c). | dard normal deviate equivalent using a table of Z values. We then find r |

A final reason for preferring r over d is its greater flexibility; rcan always _ from: |

be used whenever d can be but d cannot always be used whenever r can be. /

Sometimes, for example, the basic hypothesis is that there will be a particular _ [2 2 '

ordering of means, e.g., 1, 3, 5, 7 with contrast weights of —3, -1, 1, 3 . T= VN \N |

(Rosenthal & Rosnow, 1985). In such a situation d is useless but r suits very . [2.18] |

well indeed. _ , 7

Although I have grown to prefer r over d for the reasons just given, the 7 It should be noted that equations 2.15 to 2.18 all yield product moment
most important point to be made is that some estimate of the size of the 7 correlation coefficients. It makes no difference whether the data are in di- :
effect should always be given whenever results are reported. Whether we __ chotomous or continuous form, or whether they are ranked. Thus correla- i
employ r, g, d, Glass’s A (difference between the means divided by the S / tions known as Pearson’s r, Spearman’s rho, phi, or point biserial r, are all /

' computed from the control group only) or any of the other effect size sti- | defined in exactly the same way—though there are computational simplifi- :
mates that could be employed (e.g., Cohen, 1977, 1988) is less important 7 cations available so that some appear to be different from others——and are :

than that some effect size estimate be employed along with the more tradi- _ interpreted in exactly the same way. :
tional test of significance. / _lfwe should want to have r as our effect size estimate when only Cohen’s :

| - dis available we can readily go to r from d (Cohen, 1977):
1.B. Computing Effect Sizes / i

The emphasis in this book will be onr as the primary effect size estimate. | d I
' Since most investigators do not yet routinely provide effect size estimates _ r= Jere
: along with their tests of significance we must usually compute our own from | a+ Pq [2.19] L
| the tests of significance they have provided. The following formulas can be _ |

----- Page 17 (ocr) -----
20 META-ANALYTIC PROCEDURES —_ yg FINING RESEARCH RESULTS 21

: where p is the proportion of the total population that is in the first of the two : TABLE 2.4

: groups being compared and q is the proportion in the second of the two | Population Effect Sizes and Results of Significance Testing
| groups, or 1 — p. When p and q are equal, or when they can be viewed as 7 as Determinants of Inferential Errors '
: in principle, equation 2.19 is simplified to equation 2.20. . OO
d - Population a |

; _ _ _ Effect Size Not Significant Significant
_ Feto No Error Type | Error i
2 i

. Small Type HI Errora No Error

In most experimental applications we use equation 2.20 because we think of | Lane Type Il Errore No Error

equal population sizes in principle. We might prefer equation 2.19 in situa- | ‘a. Low power may lead to failure to detect the true effect, but if the true effect is quite small the costs |

i intrinsic i i ion sizes as when we {this error may not be too great. '

tions where we have intrinsic inequality of population ‘ ze f l d / A Although not an inferential error, if the effect size is very small and N is very large we may mistake a i

compare the personal adj ustment scores of a random sample of normats an 2 result that is merely very significant for one that is of practical importance. '

arandom sample of hospitalized psychiatric patients. - ¢. Low power may lead to failure to detect the true effect and with a substantial true effect the costs of

. , - i be very great.

In those cases where we want to work with Cohen’s d but have onlyr this error may De very g =

available we can go from rto d: _

2 | Ill. ADJUSTING EFFECT SIZE ESTIMATES

d= vi -?7 [2.21] . ILA. The Fisher and the Hedges Adjustments

. In this book our primary effect size estimator will be the correlation

| coefficient r. However, as the population value of r gets further and further

_ from zero the distribution of r’s sampled from that population becomes
’ ‘ . more and more skewed. This fact complicates the comparison and combina- |
Il. INFERENTIAL ERRORS a : “ . }

_ tion of r’s, acomplication addressed by Fisher (1928). He devised a transfor- /

If the reported results of a study always include both an estimate of effect - mation (z,) that is distributed nearly normally. In virtually all the meta- ,
size and a test of significance (or a related procedure such asaconfidence —=—_ analytic procedures we shall be discussing, whenever we are interested in r /
interval) we can better protect ourselves against the inferential invalidity of | we shall actually carry out most of our computations not on r but on its |
type I and type Ierrors. There is little doubt that in the social and behavioral | transformation z, The relationship between r and z, is given by:
sciences type Il errors (concluding that X and Y are unrelated when they | |
' really are related) are far more likely than type I errors (Cohen, 1962, 1977, ahh E tr ] |
: 1988). The frequency of type II errors can be reduced drastically by our / 1 fel T= +r [2.22] |
attention to the magnitude of the estimated effect size. If that estimate is large
' and we find a nonsignificant result, we would do well to avoid deciding that Fisher (1928, p. 172) noted that there was a small and often negligible /
: variables X and Y are not related. Only if the pooled results of a good many / bias in z,, each being too large by r-population/[2(N — 1)}]. Only when N is i
replications point to both a very small effect size on the average and to a very small while at the same time the r-population (the actual population |
combined test of significance that does not reach our favorite alpha level are | value of r) is very substantial is the bias of any consequence. For practical | '
( we justified in concluding that no nontrivial relationship exists between X | purposes, therefore, it can safely be ignored (Snedecor & Cochran, 1989).
and Y. Table 2.4 summarizes inferential errors and some possible conse- | Before leaving this introduction to z,, it should be noted that it would make a |
quences as a joint function of the results of significance testing and the | very serviceable effect size estimate but one not as easily interpreted as r | :
' population effect size. _ (see the final chapter). ; ;
: _ There are analogous biases in other effect size estimates, such as Glass’s i:
| A, Hedges’s g, and Cohen’s d; Hedges (1981, 1982a) has provided both
_ i
' . :

----- Page 18 (ocr) -----
22 META-ANALYTIC PROCEDURES — : pEFINING RESEARCH RESULTS 23
exact and approximate correction factors. Hedges’s unbiased estimator g" jg 7
given by | estimated bias, = m0 -D py 7 029
gt = c(m)g a This corrected bias differs little from our first approximation and leads to a
where g is the effect size estimate computed as (M, — M,)/S (with S com. . corrected Z, of .
puted from both the experimental and the control groups) and c(m) is given __
approximately by | -789 — 029 = 760
3 | which is associated with a corrected r of .641. Note that the corrected r
| cm) = bo [2.24] | differs little from the uncorrected r (.658 versus .641) even though N was
. quite small (12) and r-population was estimated to be quite substantial. :
| To illustrate Hedges’s method of correction for small sample bias we
where m is the df computed from both the experimental and control groups | need g as our estimate of effect size. Since g is defined as (M, — M2)/S we |
or n, +n, — 2 (see also Hedges & Olkin, 1985). 7 can obtain g from equations 2.4 or 2.5 from Table 2.1 as |
IILA.J. Ilustrating Fisher's and Hedges'’s adjustments. To illustrate Fish- _ gatyo tr = (2.76) a¢+37 1.69 .
er’s and Hedges’s methods of adjustment we assume an experiment in which | 1 2
n, = 4and n, = 8 witht (10) = 2.76. To illustrate Fisher’s method we need r | or |.
as our estimate of effect size. Equation 2.3 of Table 2.1 can be used to obtain ° : }
r by way of equation 2.16. For this example: 7 g = tt®™ _ (2.76) 48 - 1.69 [2.26]
| ving TiCOTED |
r= (76? + 10 = .658;z, = .789 | To employ Hedges’s approximate correction we obtain g" as a function o i
: _ c(m) and g. For thisexample m = 4 + 8 — 2 = 10, so: :
The bias to be corrected in z, is r-population divided by 2(N — 1). Of course, [ ; ;
we don’t know the r-population, but we can begin by employing the ob- | e(m) = 1 — “Amat 1~ 35 = 9231 ,
tained r as a first approximation. We therefore estimate the bias in z, as / and ,
estimated bias, = 242 - 1) = .030 - gu = c(m)g = (.9231)1.69 = 1.56 |
This bias is to be removed from the obtained z, of .789 so our corrected z, | Table 2.5 summarizes Fisher’s and Hedges’s adjustments for the present |
is _ example, The reduction in effect size is greater for Hedges’s method than for a
____ Fisher’s method but since the metric r and the metric g are not directly |
189 — .030 = .759 | comparable we must first find a common metric before we can interpret the |
____ telative magnitude of the corrections made. A suitable common metric is |
which is associated with a corrected r of .640. Since we now have a more the t distribution on 10 df since both r and g can be expressed in terms of this ; :
accurate estimate of the population value of r (i.e., .640) we could repeat the _ distribution. The lower half of Table 2.5 shows that, for the present exarn-
calculations to obtain a still more accurate correction for bias: _ ple, Hedges’s correction is more extreme than is Fisher’s correction, but |

----- Page 19 (ocr) -----
Poo 24 META-ANALYTIC PROCEDURES ———_ay gp FINING RESEARCH RESULTS 28 i
: both corrections are less than 8% in units of the t(10) distribution. | as the proper goal of a meta-analysis. That goal is to teach us better what is,

: If one should want to convert r to g it can be done as follows: . not what might some day be in the best of all possible worlds when all our 7

' a | independent and dependent variables are perfectly measured, perfectly valid, i
- perfectly continuous, and perfectly unrestricted in range. '
e= r = jie + Ty) (2.27) | Even when these adjustments are made with the goal of setting some upper |

vi~r mn _ limits of what better instrumentation and better design procedures might .

| yield in future research in an area, these adjustments must be applied with :

TABLE 2.5 | great caution. It has been known for nearly a century, for example, that
Fisher’s and Hedges’s Adjustments for Bias _ correction for unreliability alone can yield “corrected” effect size correla- l

| tions greater than 1.00 (Guilford, 1954; Johnson, 1944; Spearman, 1910). |

Effect Sizes _ . . |

r g . fILC. The Glass, McGaw, and Smith Adjustments i

Effect Size | Studies entering into a meta-analysis differ in the precision of the statisti- .

Original 658 1.69 : cal procedures employed in their analysis. Thus repeated measures designs
Corrected 641 1.56 | (of which gain score analyses are a special case), analysis of covariance i

Difference —cduction 7 on | designs, and designs employing blocking will tend to produce larger effect I

a . , _ sizes and more significant test statistics than would the analogous un- '

antl t(10) Distribution 316 316 | blocked posttest only designs. Glass, McGaw, and Smith (1981) have shown |
Original ae oe | how we might convert the results of various designs onto acommon scale of |

Difference 12 21 2 effect size (e.g., A or g) based on the unblocked posttest only. These adjust- (
Percentage reduction 4.3 7.6 7 ments cannot always be made-for the results of other people's studies, but |

- can often be quite usefully employed. However, when they are employed, I :

' If one should want to convert g to r it can be done as follows: | recommend that both the adjusted and unadjusted statistics be reported.
1 Just as repeated measures, covariance, and blocking designs tend to in- |
re fm {2.28] | crease power, the use of nonparametric tests of significance may tend to
£?nyng + (ny +ng)df | decrease power, and Glass et al. (1981) provide adjustment procedures. As |
' - in the case of adjustments noted earlier, I recommend reporting the unad- '
IILB. The Hunter and Schmidt Adjustments justed statistics along with those that have been adjusted. When nonpara- |
: ; metric tests have been employed, a useful estimate of effect size (r) can be .
: The most elaborate set of adjustments has been proposed by Hunter and obtained from looking up the standard normal deviate (Z) associated with
| Schmidt (1990; see also Hunter, Schmidt, & Jackson, 1982). They recom- | the accurately determined p level and finding r fi
. eer . . gr from /
mend adjustment for unreliability of the independent and dependent vari- _
; ables, dichotomization of continuous independent and dependent variables, - _—
/ restriction of range of the independent and dependent variables, imperfection _ t= vA 2 — {2.18]
| of construct validity of the independent and dependent variables, and even | N Ww |
the employment of unequal sample sizes for the experimental and control | |
groups. The Hunter and Schmidt work is valuable for reminding us that there | An alternative procedure is to find the t(df) that is equivalent to the obtained |
are many sources of noise that may serve to lower obtained effect sizes. Their / p and employ i
work is also valuable for providing us with procedures for adjusting for these / :
sources of noise. The application of these procedures gives us some estimate | pe fate [2.16] :
of what effect size we might expect to find in the best of all possible worlds. _ +f , :
/ That is a useful thing to know — perhaps as a goal to strive for by developing / |
better measures and better design procedures. However, it does not strike me |

----- Page 20 (ocr) -----
26 META-ANALYTIC PROCEDURES 7 pEFINING RESEARCH RESULTS 27 :

: In the past, the accuracy of these procedures has been limited by the | TABLE 2.6 |

' structure of tables of t and of Z, which rarely gave p’s much below .0001. | Matrix of Hypothetical Dependent Variables Obtained in a i

However, inexpensive hand-held calculators are now available that permit __ Set of Studies of Alcoholism Treatment Programs .

: working with p’s as low as 1/105”, Z’s as large as 47.8, and t’s (¢.g., for df= J eee i

: 1 0) of 10°°. ource of Information

_ Type of Self- Family Institutional |

a Variable Report Report Report Mean Pp

| IV. SOME SOLUTIONS TO THE PROBLEM | pysofsobriety re 7

OF MULTIPLE (CORRELATED) RESULTS | Days of employment :

Many of the studies entering into our meta-analyses will have more than | aad vane |

one test of significance relevant to our hypothesis and, since for every test of - onal adjustment |

significance there is an effect size estimate, these studies will have more | Social adjustment |

than one effect size estimate as well. The various dependent variables em- | Mean |

ployed in a study should all be examined for clues as to the types of depen- | ; . ;

dent variable that seem most affected and least affected by the independent Unfortunately, we do not often encounter such nicely filled-in matrices |

variable of interest. If there are many studies using several of the same | of effect sizes. Indeed, we count ourselves fortunate when even a substantial

dependent variables one could perform a separate meta-analysis for each | subset of studies have employed the same types of variables. Assuming the |

different type of dependent variable involved. For example, if one were __ typical situation, then, how are we to analyze multiple results from a single :

studying the effects of alcoholism treatment programs, separate analyses - study? Shall we count each result from a different dependent variable as |
could be performed for the dependent variables of sobriety, number of days _ though it were a separate study, i.c., as though it were an independent |

of employment, number of arrests, general medical health, personal and . result? Smith et al. (1980) and Glass et al. (1981) have treated multiple |

| social adjustment, and so on. Each of these types of dependent variable results as though they were independent, a practice for which they have been |
could be operationalized in several ways. For example, foreach of them we __ unjustifiably criticized. Where have their critics gone wrong? They have

i could obtain self-reports, family reports, and institutional reports (e.g, confused the effect of nonindependence on significance testing with its effect
from hospitals, clinics, courts, police departments, etc.). | on effect size estimation. Treating nonindependent results as independent |
Table 2.6 shows a matrix of 6 types of dependent variables crossed by 3 | does tend to create errors in significance testing, but Smith et al. and Glass |
sources of information. If there were aset of studies that hademployedallof sss al. did not do significance testing. Treating nonindependent results as |
oo the 6 X 3 = 18 specific dependent variables, we could perform a separate ] independent for purposes of effect size estimation simply weights each study \
meta-analysis on each of the 6 types of variables averaged across all 3 | in proportion to the number of different effect sizes it generates. Although '
sources of information to learn which variable, on the average, was most 0 all meta-analysts may wish to employ such weighting, there is certainly |
affected by the treatment. We could also perform a separate meta-analysis nothing wrong with doing so. iy
on each of the three sources of information averaged across all 6 types of My own recommendation is to have each study contribute only a single |
variables to learn which of the sources was most affected by the treatment. _°ffect size estimate and a single significance level to the overall analysis. |
We could examine these matters simultaneously for a set of K studies by | That recommendation does not preclude computing additional overall effect |
| entering effect sizes (or Z’s associated with significance levels) into each of size estimates in which each study is weighted by the number of research
the 6 X 3 = 18 cells of the matrix and then conductingaK x 6 x 3analysis _—=—=@SUllts it yields, by its sample size, by its quality, or by any other reasonable |
of variance on the effect sizes (or the Z’s). In such an analysis there would be 7 weighting factor. |
K independent sampling units (studies) and repeated measures on the 6 In the following sections some procedures are proposed that can be used |
level factor of variable type and on the 3 level factor of information source. ss obtain a single research result from a set of correlated research results. |
Such an analysis would be of great value for the simultaneous light it might | We begin by describing procedures applicable in the usual meta-analytic :
shed on the effects of variable type, information source, and the interaction | situation in which we are given relatively few details in the report available i
of these variables on the magnitude of the experimental effects obtained. to us. Subsequently we describe procedures applicable when more of the
Original data are available to us. |
_

----- Page 21 (ocr) -----
; 28 META-ANALYTIC PROCEDURES DEFINING RESEARCH RESULTS 29 /
In most of these applications we will find that significance levels ang | effect sizes, each based on an associated p, is a different statistic than is the
' effect sizes are highly correlated. That follows from the fact that most cor. | fect size associated with the mean p level. For example, imagine two p
related results will be based on approximately the same sample size. When | el s from the same study (N = 100) associated with standard normal devi-
that is the case there tends to be a perfect monotonic relationship between - ates of 0.00 and 9.00, Their mean is a Z of 4.5. The effect size r associated

significance level and effect size. | with this mean Z is )
| IV.A. Original Data or Intercorrelations _
Among Dependent Variables Not Available | pa De as
IV.A.1. Method of mean result. Perhaps the most obvious method of ob- |

/ taining a single result for a set of results from a single study is to calculate _

ou rthnce vn tailed p levels 25,10, oan eno To averece th one wee | However, the two effect sizes associated with Z’s of 0.00 and 9.00 are r’s of |

. ae noon _ .90 but z,’s of 0.00 and 1.47, respectively. The mean of these z,’s is

first find the standard normal deviate (Z) corresponding to each, and aver. = spout ona corresponding to anrof .63 Clearly the two methods can yield

age these Z . (If results are simply reported as nonsigniicant and we he | quite different results (.63 versus .45), neither of which is intrinsically more

i ther 7 v0 00)! whi ‘ example oe three Ze ave 67. 1.28. an i 3.09 | correct than the other. A reasonable practice is to decide beforehand on one |

>; ns ; : : _ of these procedures and use it-throughout any given meta-analysis. In no i

All three Z's have positive stens because all results were in : “3. “— . case, however, should both procedures be employed unless both are re-

tion. The mean of our three é sso? iM ae eee 4 ‘tha t when we 7 ported. In other words, it will not do to compute both estimates and then

az corresponding t ° hei manana t Gz. ° see d not the level | report or use in the meta-analysis only the personally preferred estimate. |
thamect P “Thk 7 ‘i thelr tin detail in Che, he 45 P | Sometimes only one or two-p levels are reported when a whole array of

| themselves. This is discusse one pe 1 tak th . ifth 7 effect sizes is available. That might happen, for example, if the report pro- '
To average several oer ne estimates we ond t a ‘i Glass A ee | vides a correlation matrix in which the dummy-coded (0,1) independent
are already in Standard’ Cevia te fiat $ as f ne oh . al t b f | variable is correlated with a whole series of dependent variables. In this

: Hedges s g. In the case of % eh irst a orm each Fr value © ote or case, of course, we would base our effect size estimate on the mean of all the
finding the mean. If etter! “ ave mt cen N the we be oe oem tas | effect sizes, not just those for which p levels are reported. In addition, we '
| own, one for each p level, as long as we know N, the number o ping 7 base our p level estimate on the mean of all the p levels associated with all :
: units, since | the effect sizes reported. By rearrangement from equation 2.18 we can get
- the Z associated with each r from the following:
r= — .
7
: For the preceding three p levels we would find corresponding r’s of .067, [ ; ; ; |
| .128, and .309 if N = 100. The z,’s associated with these r’s are found to be | An ater . ive to comp Yt ng the mean of a the “ ® obtained from this |
i .07, .13, and .32, yielding a mean z, of .17 corresponding to an r of .17. __ Ones ure Is fo ara o. yt ‘ veoocs ot. with the mean effect size.
; When 1’s are all quite low, averaging directly yields results essentially like - 4 € ones given a ee a on . vid be ke choosing one of these proce- |
' those obtained when we first transform the 1's to z,'s. For the present exam- __ ures to present as “the result" should be kept in mind. {
: ple, direct averaging of 1’s also yields a mean r of .17. . IV.A.2. Method of median result.When the p levels and/or the effect |
oo An alternative procedure is to compute the mean p level and then simply | sizes produced by a single study are very skewed, some meta-analysts may |
a compute the effect size corresponding to it. Although the two estimates _prefer to compute the median p level and the median effect size. Although
: | will often yield similar values, it should be noted that the mean of a set of | there are a great many statistical applications where medians are to be pre- :
a ____ ferred to means (Tukey, 1977), the use of medians in meta-analytic work |
| | |

----- Page 22 (ocr) -----
_ 30 META-ANALYTIC PROCEDURES _ __pEFINING RESEARCH RESULTS 31
tends to give results consistently favoring type Il errors, i-e., results leading | 5 weights ;
aa to estimates favoring the null hypothesis. An intuitively clear example | padjusted, weighted = (ie) Pms
i might be the following five p levels, all one-tailed: .25, .18, .16, 001, and __ Bir oF Pms |
/ .00003. The median p of . 16 is notably larger than the mean p of .027 asso. __
ciated with the mean Z of 1.93. Intuition may suggest that the mean isa __ _ ( +24+2+ 7 02 = 04 |
better estimate of the gist of the five p levels than is the median, given two __ 5 . . [2.30] |
| such very significant results in the set of five correlated results. That intuj- |
' tion will be supported by the logic of the Bonferroni-based methods to be | Therefore, the adjusted, weighted p is significant at p < .05, whereas the |
discussed next. | unweighted adjusted p value would not have been, since i
IV.A.3. Method of ensemble-adjustment of p. Suppose we had four p lev- 1 p adjusted = (R) p,,, = (4).02 = .08. :

els for a given study: .50, .50, .50. and .001. The median p is .50 and the | |
| mean p is .22. But somehow we think that, of four results, we should not __ In addition, the mean p would have been .17, and the median p would have

find a p as low as .001 if the null hypothesis were true. The Bonferroni. _ been .21. Further details on assigning weights in Bonferroni-based proce- |

based procedures address such issues. One can examine a set of R correlated | dures are given in Rosenthal and Rubin.(1984). Once we have computed our /
results for a single study, compute the most significant p, and calculate the _ ensemble-adjusted p we compute the associated effect size from equation

conservative corrected p that this most significant found p could have been | 2.18. : |

obtained, after examining R results, if the null hypothesis were true (Rosen- | IVB. Original Data or Intercorrelations i

thal & Rubin, 1983). All that needs to be done is to multiply the most signifi- | Among Dependent Variables Available |
i: cant p (Pm) by R, the number of p levels that were examined to find the most | ; :
significant p. Thus, for our example of R = 4 p levels where the most signifi. _ IV.B.1. Creating a single composite variable. When we have access to
cant p was .001, the ensemble-adjusted p value is | the original data, an examination of the intercorrelations of our dependent i

: - variables may suggest that all our dependent variables are substantially i:
p adjusted = (R) pas = (4).001 = .004 [2.29] : intercorrelated. If that is the case we may want to create a composite variable :
: : | made up of all our dependent variables. One easy way to do so is to |
/ . , 7 standard-score (z score) each of our dependent variables and form the
: This procedure for correlated results, which is related to Tippett’s (1931) | composite variable as the mean of the z scores earned on the contributing /
/ procedure applied to independent samples, is employed when we have no | variables. This procedure weights the variables equally. If we have a priori |
| theoretical reasons to expect certain results to be more significant than oth- | theoretical reasons for weighting some variables more heavily than others |
ers. When we have theoretical reasons to expect some results to be more _ we can do so. Any variable in its z score transformation can be multiplied by |
oa significant than others, we can increase our power by assigning weights to | any weight w; we like. Any subject’s score on the composite variable zy is '
| each of the results to correspond to our view of their importance. (The ac- | then defined by the sum of that subject’s z scores, each multiplied by its
Pa tual assignment of weights must be done by an investigator blind to the eight, and this sum divided by the sum of all the weights employed, or |
: results obtained.) For example, suppose we knew a study to yield four p | , > |
an levels. Before examining the results we decided that the first result was of | _  Xew,z) .
. greatest importance with weight 5, the second and third results were of less__ aw Sw, |
Co importance with weights of 2 each, and the fourth result was of least impor- | (2.31] |
: tance with weight 1. Suppose we obtained one-tailed p’s of .02, .19, .24, and | _. ; ;
| .40, respectively. Then the weighted adjusted p level for the most signifi- __ where Z is the mean weighted z score or composite variable score for any
i cant result would be given by | one subject, and wi is the weight given to the i z score (z,).
: _ As an example, imagine a subject whose z scores on four dependent vari- i
oo | ables are 1.10, .66, 1.28, and .92. The weights (w,) assigned to each of these :

----- Page 23 (ocr) -----
ca 32 META-ANALYTIC PROCEDURES yg yNING RESEARCH RESULTS 33 i
: variables were decided on a priori theoretical grounds to be 4, 2, 1, and 1, 7 ZA, t;/[(n - 1)/2]” /
' respectively. Therefore, employing equation 2.31, our subject’s composite _ d= eG [2.32] L
' ‘ = . [p(2Aj)? + (1-p)ZAz] ,
ea variable score (z,,) would be: |
il _  - Sew;z) — (4)1.10 + (2).66 + (11.28 + (1).92 _ 7.92 | where t, is the t test of the significance of the effect of the treatment on the
: Zy = “Sw, ey oer err a .99 | jn dependent variable, A, is the weight we assign (before seeing the data) to
| the importance of the i* dependent variable, p is the typical intercorrelation |
| among the dependent variables, and n is the number of sampling units (¢.g., J

If we want an estimate of the internal consistency reliability of the com- | subjects) in each group, or if these n’s are unequal, the harmonic mean (n,) |
7 posite variable, we can obtain it by one of three ways: (1) applying the | of the two unequal sample sizes, or n, = 2n,n,/ (n,+n2).
Spearman-Brown formula to the mean of the intercorrelations among the For our illustration assume an experiment with equal n (so n, = n, = 6) /

constituent variables, (2) computing an intraclass correlation following an | and with three dependent variables yielding three t(10)’s of .70, 1.37, and 7
analysis of variance in which constituent variables become a repeated mea- | 4.14, with the average intercorrelation (p) among the dependent variables of ,
sures factor, or (3) computing Armor’s theta from the unrotated first principal | 50, and with the variables given equal weight so that A, = A, = 3 = 1. Then

component. All three of these procedures are described in some detail in the _
following chapter and are summarized elsewhere (Rosenthal, 1982a, 1987a). - _ [(1).70 + (1)1.37 + (1) 4.14] / {((6-1)/2]” 3.9275 1.60 L

a An alternative to combining variables by z scoring is to combine the raw | d, = .50(1 + 141)2+(1 -.50)(12 + 12 4 12)]” = 7.4495
scores. This is a reasonable alternative only when the standard deviations of | -
4 each of the constituent variables are similar. If they are not, the variables J Should we want to express d, in terms of its equivalent effect size estimate '
with larger variances dominate the others in the composite variable, usually | r, we can do so from 7
for no good theoretical reason. For example, one variable, ability test score, __ ° |
aa may have o = 20 while another variable, acceptance into a particular college | = de [2.33]
Po (scored as 1 or 0) may have o = 0.50. Adding raw scores from these variables 7 © Vee 4 :
: would yield a new variable that was very little affected by the second variable | '
(acceptance). For our example, then
oan Situations in which direct adding of variables often is useful include those 7 :
po in which the variables are ratings by others on a specific rating scale, scores T, = 160 62. \
: on subtests of personality tests or of tests of cognitive functioning. In no case, _ v(1.60)? + 4 i
| however, should variables in raw score form be combined without an exam- i
aa ination of the standard deviations of the variables. If the ratio of the largest _ We can test the significance of our composite effect size estimate by means i
Log to the smallest o is 1.5 or less, combining is safe. A larger ratio than 1.5 may | of the following: |
: be tolerated when the number of variables to be combined grows larger. - Shit, :
! | IVB.2. Creating a single estimate. A procedure has recently been | te [p(EA,)? + (1 - p)EA? + (1 - p?)EA?t? / 2df]” [2:34] |
P| described that allows us to combine effect sizes for multiple dependent | :
oe variables knowing only the df and the typical intercorrelation among the | For our example, then .
: dependent variables. The illustration of this procedure employstheeffectsize 7
Cohen’s d (the difference between the means of the experimental and control | t.= N70) + ONES) + OTD '
| groups divided by the pooled ). For the general case, and for technical _ [50(1+1+1) + (1 - 50)(1? +1? +1?) :
_ details, the paper by Rosenthal and Rubin (1986) should be consulted. We | + (1 — .502)[12 x (.70)? + 1? x (1.37)? + 12 x (4.14)?] / 2(10)} # :
Cg obtain d,, the composite effect size, from _
ii | oo OT ___—=, 841. 39 i

----- Page 24 (ocr) -----
34 META-ANALYTIC PROCEDURES | —_. prNING RESEARCH RESULTS 35 |
i with df = 10 and p < .02, one-tailed. (For procedures for comparing and _ TABLE 2.7 :
a 1985). multiple significance levels rather than effect sizes, see Strube, 7 Three Types of Effect Size Indicators |
1985). | |
| If there should be a theoretical interest in computing contrasts among the __ __Effect Size Indicator__Definition ee
effect sizes associated with the correlated dependent variables, procedures 7 product Moment Pearson r Y2,zy)/N
are given in Rosenthal and Rubin (1986) for estimating the effect size and Correlation (1) r/k 1/Vi-? |
significance level of any such contrast. | and Functions of r t, vA log, [ “Ltr ]
Vv. ASUMMARY OF SOME EFFECT SIZE INDICATORS | Cohen's q 2, 2x98
In this section we want to bring together the various effect size indicators | ; Cohen’s d (M, — M)/o pooled | :
that have been referred to as well as a few others that may prove useful, Standardized , |
; Logs ; Differences Glass’s A (M, — M,)/S control group |
Table 2.7 serves as a summary. The first four indicators include the very _ Between Means ; |
general Pearson product moment correlation (r) and three related indices, _ Hedges's 8 (My ~ My)/S pooled rf
The indicator r/k is not typically employed as an effect size estimate though _ OT |
it certainly could be. It is included here because of its role in equations 2.3 __ Differences Cohen's g *P— .50 i
and 2.11; that is, it is an effect size estimate that needs only to be multiplied = BW" qo Pia Po |
: : age : a Proportions Cohen’s h : p> — Pp
by Vdf to yield the associated test of significance, t. The index r/k turns out |< —_________$______________*__ > .
also to be related to Cohen’s d in an interesting way —it equals d/2 for situ- - A peers an rect ze to oes neo ee aurerence between ‘wo effect sizes. 1
ations in which we can think of the two populations being compared as __
oa yvically eomployed mn TE nedman, 288). - wah it too, aan ae | not described here since the product moment correlations (based on contin-
However, it is frequently used as a transformation of r in a variety of meta- : oe scores, ranks, o dichotomized data) can be employed in those situa-
’
a analytic procedures. Cohen’s q indexes the difference between two correla tions. We have specifically not included any indices of proportion of vari-
tion coefficients in units of z,. | ance accounted for such as 1, eta’ 3 omega , epsilon’, and so on. As we shall
The next three indicators of Table 2.7 are all standardized mean differ- | sec in the final chap ter, all these indices tend to be misleading at the lower
ences. They differ from each other only in the standardizing denominator. 7 levels. In ae fom thot that are based on F tests with df = 1 in the numera-
A Cohen’s d employs the a computed from both groups employing N rather | tor are generally of little use in the meta-analytic enterprise.
- than N- 1 as the within group divisor for the sums of squares. Glass’s A and |
Hedges’s g both employ N— 1 divisors for sums of squares. Glass, however, _
7 computes S only for the control group, while Hedges computes S from both |
experimental and control groups. | |
The last three indicators of Table 2.7 include two from Cohen (1977). a :
Cohen’s g is the difference between an obtained proportion and a proportion |
of .50. The index d’ is the difference between two obtained proportions. _
bike Cohen’s h is also the difference between two obtained proportions but only __
after the proportions have been transformed to angles (measured in units _ |
called radians, equal to about 57.3 degrees). | /
a: Many other effect size indicators could have been listed. For example, _ .
a Kraemer and Andrews (1982) and Krauth (1983) have described effect size | , |
: estimates when medians rather than means are to be compared. These are | |

----- Page 25 (ocr) -----
7 RETRIEVING AND ASSESSING RESULTS 37 |
_ truly expert in the methods of information retrieval. That is the domain of i
| the information specialist. To give the details required for the serious re- |
. trieval of the results of a research area there is a useful paper on information
| retrieval especially prepared for meta-analysts by an experienced reference |
_|ibrarian (M. Rosenthal, 1985). /
/ 7 When the resources described in that paper have been properly em- |
7 ployed, including an examination of the references of the retrieved docu- :
3 _ ments and correspondence with the contributors to a research area to obtain i
| their unpublished manuscripts and their suggestions as to the location of

| | other unpublished works, we will find four major classes of documents: (1) ;
| . . | Books, including authored books, edited books, and chapters in edited L
Retrie VINg and Assess ing books; (2) Journals, including professional journals, published newsletters,

R esearc h R esu lt ky | magazines, and newspapers; (3) Theses, including doctoral, master’s, and |

| bachelor’s theses; (4) Unpublished work, including technical reports, grant |

/ : proposals, grant reports, convention papers not published in proceedings,

; Procedures for locating and abstracting research results are described and illustrated, | ERIC reports, films, cassette recordings, and other unpublished materials.
: and the reliability of these procedures is discussed. Various ¢ of errors, their preven- __ iabili i ; ‘ tae |
i tion and sorveetion. awe described. Finally, the evaluation of the quality of eoveret resi | TALL Reliability of information sources. The purpose of this section is :

: is discussed. | to present the Tesu ts of an analysis Showing that, for a sample of meta- ;
_ analyses, there is a high degree of reliability among the four types of docu- L
| ments in the average effect size obtained. The raw data for these analyses
Doll There is in principle no difference between the conscientious review of a __ come from Glass et al. (1981, pp. 66-67). The results of 12 meta-analyses on '
oa research area conducted traditionally or meta-analytically. In both cases one | various top es are P resented, For each meta-analys is, an effect size (Glass’s |

| wants to find all the research results. There may be logistic and financial | Aor Cohen’s d) was estimated from at least two different information sources. |
reasons for restricting a review simply to published works, but there are no 1 Table 3.1 shows the six possible pairs of sources of information, the :
scholarly reasons for doing so if our goal is to summarize the research evi- | number of meta-analyses providing effect size estimates for each pair of |
| dence bearing on a given relationship. After we have retrieved all the re- __ sources, the reliability obtained between the two sources of each pair, (com- :
trievable research results, we will want to evaluate whether the sources of | puted over all available areas meta-analyzed) and the p level of the reliabil- |
' our results are significantly and substantially related to the quality of the _ |
oe research conducted and the magnitude of the effects obtained. If they are, _ TABLE 3.1 /
_. we can present our meta-analytic results separately for the various sources | Reliability of Information Sources
of information and the various levels of quality of the research conducted. | for a Sample of Meta-Analyses
| J. RETRIEVING RESEARCH RESULTS | PuirsofSources ___—_—_—~Metardnalyues—_—ofSourcen’ ‘Reliability |
i: LA. Locating Research Results ; Tournat unpublishe 4 " 88 00085 /
' Locating research results has become a more sophisticated enterprise _ Thesis; unpublished q 85 ‘008 |
Dog than spending a few hours with the Psychological Abstracts, Sociological _ Feet: Journal 6 82 025 U
a Abstracts, Child Development Abstracts, Language and Language Behavior | Book, unpublished 3 \ Oo Oo 5 :
; a Abstracts, or the International Bibliography of Social and Cultural Anthro- | Median 6.5 , 87 014 |
pology. Computer-based retrieval systems are not only available but are be- . Weighted Median 7 .85 .008 }
Po ing enlarged and improved at a rate so rapid that few social scientists canbe _ A

----- Page 26 (ocr) -----
| 38 META-ANALYTIC PROCEDURES —_agp'TRIEVING AND ASSESSING RESULTS 39 :
oe ity obtained. The median (unweighted and weighted) reliabilities of .87 and __ TABLE 3.2
oe .85 and the weighted mean r of .83 show that there is a high degree of _ Median Effect Sizes Obtained from Journal Information ;
a reliability, on the average, between the various pairs of information sources, for Meta-Analyses in Which Other Sources _
There is little support here for the position that holds that some sources of 7 Were or Were Not Available
2 information may be misleading relative to the others. On the basis of the 12 | ——— Source Source””~*~*~*weam=SSSSS~S~S~S :
oe meta-analyses available, we can conclude that meta-analyses finding larger | ae Available Unavailable _ Difference Mean |
effect sizes from one source of information are also likely to find larger __ | ny ny rn ne |
effect sizes from other sources of information. , , , /

_ Thesis 5100 49) J 46 /

1.A.2. Differences among information sources. High reliability of sources __ Unpublished 507) 490) Ol 50 |

of information does not necessarily mean that sources will agree in their - Mean 48 51 ~.03 506 :
estimates of effect sizes for a given meta-analytic review. Two sources could | $$ |

\ have perfect reliability (r = 1.00) yet differ greatly in the effect sizes esti- | # Tena a t2motranaos based onjouralrinmation ane was 060. |
mated, so long as the difference was constant for every meta-analysis. The __

purpose of this section is to present the results of an analysis investigating | TABLE 3.3 :
systematic differences among information sources in the average effect size 7 Pairwise Comparisons of Effect Sizes Obtained |
found. Our raw data again come from Glass et al. (1981, pp. 66-67). | from Four Information Sources :

For each of the 12 meta-analyses, the mean effect sizes are reported for __ i

all those sources that provided relevant information. If there had been four __ Number (n) |
sources of information available for each of the 12 meta-analyses, we would __ oe. of Meta- First’ Second Mean Median :

have been able to provide a simpler answer to our question by merely exam- a PairsofSources __Analyses_Mean 0’ _Mean \'_Difference __ Difference :
| ining the means or medians obtained from all four sources. Unfortunately, an vnpublishe 1 0 e ” _ oe -

only 3 of the 12 meta-analyses provided data from all four sources; 13 of the | Thesis; unpublished 7 31 64 33 _07
ui possible 48 (12 x 4) estimates were not available. Under these conditions, | Book; journal 6 34 42 —.08 00 |
eet comparing the grand means or medians of each of the sources of informa- __ Book; thesis 4 .40 27 13 4 /
el tion confounds the source of information with the area being summarized. | Book; unpublished 38S BT
a L problem, Table 3.2 was prepared. It shows the’ median effect size obtained | as well as we can the difference between average sizes obtained from various sources of informa- i
al from journal information for those meta-analyses in which the other sources | b, This is the only significant mean difference t(@) = 4.78, p = .001, two-tailed (p = .001 by sign test
Pp of information were or were not available. Thus the median effect size of | as well). :
meta-analyses in which books were not available was .64 but the median __ i
A effect size was only .44 when books were available. It appears, then, that we | suggestive enough that we should try to assess differences among informa- |
i might erroneously conclude that books underestimate effect sizes relative ____ tion sources correcting for sampling bias. This type of correction can be :
to journals when actually it just happened that books were available as an 4 achieved by considering each type of source pairwise with every other |
; ‘information source for those areas of research showing smaller effect sizes | source as shown in Table 3.3.
Le anyway (as defined by journal information). | The first and second columns of Table 3.3 show the two paired sources of |
on Table 3.2 shows that the availability of unpublished material was unre- | information and the number of meta-analyses upon which each pairwise :
a lated to effect sizes estimated from journal sources. This suggests no prob- | comparison is based. The third and fourth columns give the mean effect size :
oo lem of biased availability of studies as there had been for books as sources of | (A) found for the first and second named source, respectively. The fifth |
| information. The result for thesis-based information showed a small ten- | column gives the difference between the means with the second subtracted i
dency for meta-analyses for which theses were available to be associated | from the first. The final column, and perhaps the most important, gives the |
8 with somewhat larger effect sizes (as defined by journal information). The | median of the n difference scores for each set of matched pairs. The only
a data base is not large enough to warrant firm conclusions, but the data are | significant mean difference shows larger effect sizes obtained from journals u

----- Page 27 (ocr) -----
40 META-ANALYTIC PROCEDURES agp TRIEVING AND ASSESSING RESULTS 41 |
than from theses (excess of A = .26 and .22 for mean and median differ. _ TABLE 3.5
ences, respectively). These results support the conclusion drawn by Glasset _ Mean Effect Sizes (d) for Dissertation and Nondissertation Studies 7
al. (1981) though the present pairwise analysis, controlling for the con. _ Employing or Not Employing Special Control Procedures I
founding of topic and source, shows a difference larger by between 37% and __ |
63% than that reported by Glass et al. 7 _—-Unweighted Weighted
There is certainly no clear difference between mean effect sizes obtained | ECO issertations  Nondissertations Mean Mean _ |
7 from journals compared to unpublished materials. The mean difference fa. | Special controls 78 Epa 66 64 | |
vors one by .08 A units; the median difference favors the other by .05 A __ No special controls ~ 09 : : ‘ | |
co units. The results of this analysis very strongly suggest that the burden of | elem oO a 495 40
proof now rests on those who claim that uripublished (not unretrieved but _ Tn, number ofsucies on which the moanlabasedie shown hiparentnesse, |
retrievable unpublished) studies are biased in their results relative to pub- | : |
a: lished studies. |
On the average, theses obtain smaller effect sizes than do unpublished __pectancy effects, they computed separate analyses for dissertation and non- |
studies but the difference shrinks dramatically when the median difference _ dissertation studies and found that dissertations did indeed yield substantially L
is employed rather than the mean difference. 7 smaller effects on the average. Each of the 345 studies in their sample was also |
Books and journals tend to obtain very similar effect sizes but books do __ classified by whether the investigator(s) had taken special pains to control for
tend to obtain somewhat higher effect sizes than do theses. Finally, books —_—“&FTOTS of recording or cheating by the experimenters or teachers being
: tend to obtain smaller effect sizes than do unpublished papers but the me- _ Studied. a |
2 dian difference is not large and, given the small n (3), even the large mean __ Table 3.5 shows the mean effect sizes obtained in dissertations and non- |
a difference of .37 could be a sampling fluctuation (p = .45). / dissertations that either had or had not instituted special controls for inten-
i: There is no simple way to summarize the data of Table 3.3. One provi- | tional or unintentional errors. Most of the variation (93%) among the four |
sional method that preserves the pairwise nature of the comparisons is to estimated mean effect sizes was due to the difference between dissertations
consider in turn all pairwise comparisons of each of the four sources with all __ employing no special controls and the remaining three groups of studies
others and report the median of all these comparisons. Table 3.4 shows that__ which differed relatively little among themselves. These results suggest the
journal articles, unpublished manuscripts, and books are essentially indis- possibility that the tendency for theses to yield smaller effect sizes than
tinguishable from each other. However, theses tend to yield noticeably | other sources of information may be due primarily to the less carefully exe-
smaller effect sizes than do the other three sources of information— | cuted of the theses. _
| smaller, that is, by about 1/5 of a standard deviation. | Before leaving the comparison of theses with other information sources,
How might we explain this bias for theses to yield smallereffect sizes? One __—~We should note that theses were conspicuously over-represented among the
le analysis that may be instructive was carried out as part of a meta-analysis by [ studies instituting special controls for intentional or unintentional errors. :
Rosenthal and Rubin (1978). In their study of 345 studies of interpersonalex- __—=“The correlation between being a thesis and employing special controls was
7 7 .42 O2(1) = 62.0, N = 345, p < .0001). Of the nondissertations, only 8%
a: TABLE 3.4 | employed special controls; of the dissertations, however, 56% did so. The |
O Pairwise Comparisons of Effect Sizes Obtained | typical dissertation, then, may be more carefully done than the typical non-
From Each Source Against All Others | dissertation. Perhaps this is due to the healthy monitoring that is often car-
—_—$—<$< ried out by a conscientious dissertation committee.
Number of - |
a Pairwise Median _ LB. Abstracting Research Results
a Source Comparisons Difference — Once we have located the studies to include in our meta-analysis, we |
Journal 23 13 _ must decide what information is to be abstracted from each document. We | |
a Unpublished 17 07 _____ know from the last chapter that we will always want to record both the 1
a peek , % ____ Significance level and the size of the effect and that, if one of these is not | :
a ; . provided, we can estimate it if we know the size of the study. But what else |
A Absolutemedian 00 ___are we to record for each study? The answer, of course, depends on the |

----- Page 28 (ocr) -----
: 42 META-ANALYTIC PROCEDURES 7 ___ RETRIEVING AND ASSESSING RESULTS 43 |
2 specific goals of our meta-analysis. It is easiest to begin with examples of 7 (15) Moderating variables, variables associated with differences in ob- '
Cl useful formats for abstracting information from studies. Lt tained results, the direction of their effect, effect size, and signifi- |
; | cance level. |
al I-B.1. Interpersonal exp ectancy effects. Since the early 1960s the 1 (16) Mediation data, any results bearing on the processes by which ex-
a present writer has been conducting meta-analyses of studies of the effects of | perimenters, teachers, or clinicians may have communicated their |
ae experimenters’ (or teachers’ or clinicians’) expectations on the response ob- | expectations to their subjects, pupils, or patients. '
' tained from their subjects (or pupils, or patients). For each of the studies - (17) Expectancy effect, effect size (including direction) and significance ;
retrieved, the following information was typically recorded: | level associated with the effects of the experimenters’, teachers’, |
(1) Complete reference, as for a bibliography. | or clinicians’ expectancies. |

(2) Authors’ full names and addresses, so that they could be contacted | 1.B.2. Psychotherapy outcome. A more detailed type of abstracting was

for further information about the study in question, about their | employed by Glass and his colleagues (e.g,, et al., 1981, pp. 80-91, 233- :

. work in progress, and about the work of others in the same general | 237) in their seminal meta-analysis of psychotherapy outcome experiments. |

area. | They divided their coding into the methodological and substantive features |
(3) Sex of data collector, since the sex of data collector may be related that are briefly summarized here: '

to the results obtained. | co |

(4) Status of data collector, e.g., faculty member, doctoral candidate, 7 LB.2.a. Methodological features. These included (1) date of publication, |

i: graduate student, undergraduate, and so on, since it has been found _ (2) type of publication, (3) degree to which experimenter was blind, (4) how |

that the status of the data collector may affect the results obtained. - clients were obtained, (5) how clients were assigned to conditions, (6) client |
Doe (5) Relationship of data collector to meta-analyst, so that correlations _ loss for each condition, (7) internal validity, (8) experimenter’s probable :
aa could be computed between the results obtained and the degree of _ preference for outcomes, and (9) reactivity of outcome measure. :
L acquaintanceship with the meta-analyst (Rosenthal, 1969). od 1.B.2.b. Substantive features. These included: (10) professional field of :
(6) Sex of subjects, number of each sex who served as subjects, pupils, | experimenter, (11) similarity of client to therapist, (12) diagnosis of client,
an or patients. ; 7 (13) duration of previous hospitalization, (14) intelligence of typical client, /
oa. (7) Nature of subject sample, i.c., where and how obtained. | (15) mode of therapy (e.g., individual, group), (16) site of therapy, (17) du- |
_ (8) Sex of experimenters, number of each sex who served as experi ration of therapy, (18) therapist experience or status level, (19) outcome |
3 menters, teachers, or clinicians. | measures, (20) type of psychotherapy, (21) degree of confidence in deciding '
7 (9) Nature of experimenter sample, i.e., where and how obtained. - type of therapy, and (22) effect size. : '
Po (10) Relative status of experimenters, since smaller expectancy effects [ |
Poe are obtained when there is little status differential favoring the ex- 1.B.3. Ethnic group and social class differences in need for achievement. i
Poe perimenter. | A very focused type of abstracting was employed by Harris Cooper (1984)
| a (11) Task, test, or other behavior of subjects constituting the dependent 1 in his comparison of ethnic groups and social class levels on need for |
oo variable. | achievement. A summary of his coding sheet follows: (1) complete citation; |
a (12) Unusual design features, e.g., not an experiment but causal infer- _ (2) source of reference; (3) sex of subjects, with n of each, for the two groups |
ence strengthened by use of such procedures as cross-lagged panel | being compared; (4) average age of subjects in each group; (5) geographic | :
7 analysis, analysis of covariance, partial correlations, path analysis, _ location of each group; (6) other restrictions pertaining to each group; (7) |
and so forth. _ ethnicity of each group; (8) mean and S of each ethnic group on need for |
: (13) Additional control groups, as when high and low induced-expect- | achievement; (9) type of significance test and df error employed; (10) value |
_ ancy conditions can be compared to a randomly assigned group of _ of test statistic obtained, and df effect; (11) p level and effect size obtained; Lo
7 no-induced-expectancy subjects. . (12) direction of results; (13) social class of each group; (14) standardized vs. : |
a (14) Procedural controls for cheating and/or observer error, as when all _ informal measure of social class for each group; (15) basis of classification | i"
interactions are filmed, videotaped, or otherwise monitored, and/ _ for each group including occupation, salary, social status, or other; (16) i
. or when experimenters’ recordings can be otherwise checked. | mean and S of each social class group on need for achievement; (17-20) |

----- Page 29 (ocr) -----
ci 44 META-ANALYTIC PROCEDURES agg TRIEVING AND ASSESSING RESULTS 45
. items 9-12 (above) repeated for social class comparison; (21) dependent __ TABLE 3.6
; measures including TAT (n-Ach), French’s Test of Insight, California Psy- | Examples of Items Obtaining Various |
Po chological Inventory, or other; and (22) variables interacting with ethnicity | Proportions of Agreement
Po or social class. | EE |
' - a Proportion of
| 1.B.4. Constructing a format for abstracting research results. Examina- __ Agreement Study I Study 2 __ a
oe tion of the preceding three examples of abstracting formats will be useful in _ 1.00 Median age Name of periodical |
a the construction of a new format. Using these examples and a process of __ 96-.99 Mean age Research index used? L
oe free association, the beginning meta-analyst can construct a preliminary __ .92-.95 Age range Own studies cited? |
7 form. This form should then be discussed with colleagues and advisors who | 88-91 Total N Secondary analyses done? l

ral , : . : . . 84-.87 Was median age reported? Specific recommendations? i

mu can suggest other variables to be included. Finally, arevised form mightbe __ “30-.83 Type of bivariate relationship _ Relationship exists? |
oan sent to workers in the area of the meta-analysis, with an invitation to have | 70-,79 Type of sampling procedure Critique of prior reviews? |
ce them suggest other variables that should be coded. | “60-69 _ Percentage of studies cited |

7 _. that are not directly .

1.C. The Reliability of Retrieval _ relevant

LC.1. Reliability of locating research results. It would be useful to know t 50:.59 Total number of subsamples —_ Percentage of smmaies examining |

a the reliability of locating research results, If two meta-analysts set out to | 40-.49 ae Are surveye among the major
retrieve the relevant research results for the same research question, how __ : approaches employed? |

closely would their acquisitions agree? No empirical answer to that question | Number of items 95 65 |
is available. We do know that if each meta-analyst employed only one (or L Range 57-1.00 44-1,00

two) research indices, each would miss an appreciable proportion ofretriev- _—_25thto 75th percentile (86,965 71.87 |
oa able studies (Glass et al., 1981, pp. 63-65). A thorough retrievaleffort, how- _—_—“Median 92 79 :
Dal ever, would involve going well beyond one or two research indices (M.Ro- |
' senthal, 1985). 7 Median proportions of agreement are very substantial but we could have -
It is not even obvious how one would determine the correlation defining __—_ihad more confidence in the interpretation of these data had the correlations |
| reliability in the situation of our two meta-analysts. Would we set up a2 Xx 2 | been provided rather than the proportion of agreements. It is possible to :
Po table with columns representing the first meta-analyst’s choices (i.e, in- ___have near perfect agreement and still have reliability coefficients of only |
oa cluded vs. not included in the analysis), and with rows representing the — —_about .50. More detailed discussion of this problem is available in Lewin
i) second meta-analyst’s choices (i.e., included vs. not included in the analy- | and Wakefield (1979), Rosenthal (1982a, 1987a), Wakefield (1980), and in u
a sis)? What would be the entry for the cell included in neither analysis? —_gur subsequent discussion of effective reliability, especially the final para-
7 | Would it be the hundreds of thousands of studies not relevant to the analy- | graph on product moment correlations (section II.C.1.a of this chapter). i
oat sis? Whatever the problem of computing (or even defining) the reliability of __ oa. oo . ; |
on locating research results, the problems are identical whether the research __ LC.3. Reliability of significance level and effect size estimates. Perhaps :
a summarizing process is to be traditional or meta-analytic. | the two things we want to code most reliably are the results themselves: /
_ results defined as significance levels and effect size estimates. Unfortu- |
a LC.2. Reliability of coding study features. When we try to estimate the | nately there appear to be no reliability data on the estimation of significance |
7 reliability of the coding of studies after they have been retrieved, we can do | levels as such: We come close in the raw data of the study by Cooper and |
oa quite a bit better. Several studies have reported proportions of agreement on _____ Rosenthal (1980) described earlier. As part of that study, 19 meta-analysts |
Loe specific items as coded by two judges. | were asked to decide whether a given set of seven studies supported the |
i: Table 3.6 presents a summary of two of these studies. Study 1 is by / rejection of the null hypothesis. From the analysis of variance of the 19 x 7 |
Stock, Okun, Haring, Miller, Kinney, and Ceurvorst (1982), and Study 2 is | data matrix, we were able to compute the intraclass correlation which is |
a by Jackson (1978). For each study sample items are given to illustrate items | analogous to the average interjudge reliability. For the 19 meta-analysts, this
a associated with varying proportions of agreement from below .50 to 1.00. 7 correlation was .969, a very high degree of reliability. In a situation where |

----- Page 30 (ocr) -----
i 46 META-ANALYTIC PROCEDURES _gpTRIEVING AND ASSESSING RESULTS 47 I
| 7 accurate p levels were being estimated (rather than accept-reject decisions) | _ tion, learning (human and animal), task ability, psychophysical judgments, |
and meta-analysts might not even have retrieved the same studies, and dif. _ questionnaire responses, classroom behavior, and mental telepathy. In addi- | 7
Do ferent procedures for combining p levels might have been used, the reliabjj. | tion to behavioral research, legal research and health research were repre- '
Po ity would surely be lower. 7 sented. Although there were not enough studies in these various categories to : |
| When we turn to the estimation of effect sizes, relevant data are provided 7 permit sensitive comparisons, there appeared to be no clear relationship be- |
: by Glass et al. (1981). They had six studies for each of which two judges _ tween area of research and either the rate of recording errors or the likelihood :
Ce computed Glass’s A as the effect size estimate. The mean absolute differ. __ of errors being biased when they did occur.
ae ence between the pairs of judges was only .07 standard deviation units (A’s),__ In most of the studies, errors were defined only in terms of misrecording L
ao The mean algebraic difference was even smaller—.01 standard deviation _a response that was either seen or heard by the data recorder. In a few cases, :
units! Reliability, however, is indexed by a correlation coefficient rather _ however, simple arithmetic was also required by the recorder so that ob- |
ee than a mean difference and it is possible in principle to have a very small __ server errors could not be distinguished from arithmetic errors. In these |
oe mean difference (i.e., excellent agreement in mean judgments) yet have a | cases, however, the results were so close to the results of studies of simple :
o low reliability. That did not occur in this case. For the set of six studies and 7 recording errors that they could safely be grouped together, at least for our -
two judgments per study, the intraclass correlation was .993. 7 present purpose. L

Should future studies yield lower reliabilities, Glass et al. would not be _ It is also important that for almost all the 27 studies located, the observers
surprised nor would I. As the former authors point out, although the defini. 7 had finished their task to their satisfaction and did not know that their obser- |
tion of A (and of other effect size estimates) is simple, in actual practice | vations would be checked for errors. Thus whatever checking could be done
a judgments must be made, assumptions must be made, and series of calcula. | or was going to be done by the observers had been done at the time of the .

a tions must be made and all of these may be made somewhat differently by | analysis of errors. It is unlikely, therefore, that the estimates of error were
i equally well-trained and experienced meta-analysts. _ inflated due to the observers’ not having finished their checking operations. |
' | Not all of the studies provided the data in directly usable form and it was :
. 7 necessary to make some estimates from data provided. For example, an :
_ IL. ASSESSING RESEARCH RESULTS . | investigator might mention in passing that 10 Tesponses were misrecorded :
a ____ by the observers but not how many observations were recorded altogether. i
ae IA. Correcting Research Results | Areasonable estimate of this total was often available, however, as when the :
- The purpose of this section is to emphasize that error-making is normal, 7 investigator reported that 5 observers each collected data from 10 partici- '
The meta-analyst will make mistakes and the authors of the studies summa- | pants each of whom made 20 responses (e.g., 5 X 10 x 20 = 1000). :
rized will have made mistakes. Careful reading of the original papers by the . Table 3.7 shows, for each of the 27 studies, the number of observers |
_. meta-analyst will often reveal errors. Fortunately, these errors can often be | involved, the number of recordings made, the number of errors committed, |
co corrected before the meta-analytic procedures are applied (Rosenthal & the percentage of all recordings that were wrong, and the percentage of the '
Rubin, 1978). | errors committed that favored the hypothesis of the observer. Tables 3.8
a One type of error that is difficult for the meta-analyst to correctorevento _—_and 3.9 present stem-and-leaf plots and robust summary statistics of the L
| a diagnose is an error of recording the data as the data were being obtained by | percentage of observations that were in error and the percentage of errors i:
ae the original investigator. How often do recording errors occur? When they do that favored the observers’ hypotheses (Rosenthal & Rosnow, 1975; Tukey, pot

a occur, are they likely to favor the investigator’s research hypothesis? Building — 1977). Tukey (1977) developed the stem-and-leaf plot as a special form of | |

7 on some earlier work on this topic (Rosenthal, 1978b), Iwas able tocollect27___ frequency distribution to facilitate the inspection of a batch of data. Each bot

al studies for this book that yielded some up-to-date information on these two number in the data batch is made up of one stem and one leaf, but each stem
a. questions. Since most of these studies were designed at least in part to permit _ May serve several leaves. Thus, the seventh stem under recording error, a
| the quantitative assessment of error rates, they can not be regarded as repre: _ 1., is followed by two leaves of 59 and 69, representing the numbers 1.59 |
sentative of behavioral research in general. We have no way of knowing, how: _ and 1.69. The first digit is the stem, the next digit(s) the leaf. The eye takes
ee ever, whether these studies are likely to yield overestimates or underestimates _ in a stem-and-leaf plot as it does any other frequency distribution, but the :
: ce of rates of error-making. The 27 studies ranged widely in terms of research _ original data are preserved with greater precision in a stem-and-leaf plot /
a area and locus of data collection, e.g., studies of reaction time, person percep: than would be the case with ordinary frequency distributions.

----- Page 31 (ocr) -----
| 48 META-ANALYTIC PROCEDURES _——_—RETRIEVING AND ASSESSING RESULTS 49 oa
TABLE 3.7 | TABLE 3.8 ,
a] Recording Errors in 27 Studies | Stem-and-Leaf Plots of Recording Error and Bias Rates |
_ (in percentages) 7
Observers Recordings Errors Error Bias | 7
: Study (N = 711)(N = 219,296) (N = 23,605) Percentage Percentage a Recording Error Bias |
Sa AEE aT Sra ae econ ang error Bias
(1) Kennedy & Uphoff, a Le
1939 28 11,125 126 1.13 68 Siem af Stem__Leaf
(2) Rosenthal et al., 1964 30 3,000 20 0.67 75 _ 48. 488 9] 1
(3) Weiss, 1967 34 1,770 30 1.69 85 . 8 5 |
(4) Persinger et al., 1968 el 828 6 0.72 67 | |
(5) Jacob, 1969 36 1,260 40 3.17 60 | 4. 178 7 45 7
(6) Todd, 1971 6 864 2 0.23 50 - 3. 6 025 7 8 |
| (7) Glass, 1971 4 96 4 4.17 33 | 3.> | 17 34 slog /
(8) Hawthorne, 1972 18 1,009 16 1.59 19 > | 50 A | |
(9) McConnell, 1955 393 18,000 0 0.00 ~ | a> | at 313 6 |
: (10) Rosenthal & Hall, _ 1 59 69 >
1968 5 5,012 Al 0.82 _ a ; |
(11) Doctor, 1968 15 9,600 39 0.41 _ | LP | 13 1] 9
a (12) Compton, 1970 9 3,794 36 0.95 — . 0. 62 67 69 71 72 82 94 95
: (13) Howland, 1970 9 360 9 2.50 _ _ 0.6 00 00 23 41
(14) Mayo, 1972 15 688 0 0.00 - | a. Stems between 4 and 48 are omitted to save space.
az (15) Eisner et al., 1974 12 9,600 66 0.69 _ . b. Stems were divided into upper and lower halves'to spread out the distribution.
a (16) Rusch et al., 1978 2 46,079 22,339 48.48 _— _
a (17) Marvell, 1979 2 2,156 52 2.41 — |
(18) Fleming & Anttonen, 9 8 0.62 L TABLE 3.9
a 1971 — 89,980 55 ‘ - a Summary Statistics for R i i
(19) Goldberg, 1978 ~ 5,600 0 071 ~ | y s for Recording Error and Bias
a (20) Tobias, 1979 = 4,221 141 3.34 _ OO
(21) Tobias, 1979 _ 4,254 40 0.94 _ | ‘Statist Recording Error __ Bias
(22) Johnson & Adair, 1970 12 = - = 620 Statistic | impercentages) _—(inpercentages)
(23) Johnson & Adair, 1972 12 - _ _ 58 | Maximum 48.488 9]
: : (24) Ennis, 1974 42 ~ _ _— 74 - Quartile 3 (Q3) 2.46 74
(25) Ruschet al., 1974 2 —_ — _ 36 o Median (Q)) 0.94 64
Gh iomeong Ryan, 1976 = BE Gumtile 10 0.64 6
| > a Minimum 0.00 : 19
Median 12 3,794 39 948 64D Q3- Q, 1.82 28
a. Median weighted by number of recordings = .62. 7 &.75(Q3 ~ Q))] 1.36 2)
b. Median weighted by number of recordings = .68. | / S 10.35 20
| Mean 3.58 60
From Tables 3.7, 3.8, and 3.9 we note that the typical rate of making _ No |
; recording errors is about 1% but, that in an occasional study, the error rate a. This value is a marked outlier; ie, it deviates from the rest of the distribution of recording error
L ; : . 2 fates at p much less than .001. The mean of the distribution dropping the highest and lowest scores is
can climb to an extraordinary level of over 48%. Normally we might expect __ 1.41. |
| an error rate that high when data are recorded from an analogue rather than |
oo a digital mechanism. Thus reading an analogue thermometer as 98.6 could 1 No one should be surprised to learn that data are sometimes wrongly i
a be interpreted as wrong when a digital read-out tells us that the “true” tem- __ recorded. Now, however, we have some idea of how often these errors oc- -
a perature is 98.63. Extraordinary error rates may have more to say about an __ cur. The typical rate of 1% errors is low enough that, even if the errors were |
a overly precise criterion than about any practical problem of measurement. __ undetected, the conclusions of our studies would not be greatly affected. In | |
a These same three tables also suggest that, of the observational errors _ several studies, analyzing the data with anid without the errors corrected '
7 that are made, about two-thirds support the observer's hypothesis when _ made no difference although biased errors would occasionally push a result | |
a only half should do so if the observers were unbiased. (When each study over the magic .05 cliff (Nelson, Rosenthal, & Rosnow, 1986; Rosenthal & |
was weighted by the number of errors made, the overall test that bias was__ Gaito, 1963, 1964). Investigators emphasizing confidence intervals, effect | |
a. nonzero yielded Z = 4.88, p < .000001.) | sizes, and obtained levels of p will be less misled by the presence of some | |

----- Page 32 (ocr) -----
| 50 META-ANALYTIC PROCEDURES | ageTRIEVING AND ASSESSING RESULTS 51 '
. typical degree of error in their data than will investigators following a strict 7 “simply assigning a weight of zero. If there is a dimension of quality of study /
null hypothesis decision procedure (Snedecor & Cochran, 1967, page 28), = ——_@¢g,, internal validity, external validity, and so on) then there can be a corres-

_ Several implications flow from the results of Tables 3.7, 3.8, and 3.9, ponding system of weighting. If we think a study is twice as good as another, i

We should continue to keep track of error rates and the size of observer bias 7 __wecan weight it twice as heavily or four times more heavily, and so forth. '
and do what we can to reduce our errors. Getting all the errors out is proba- 7 There is a danger, however, in assigning quality weights to studies, i.e., |

bly not possible or even desirable from a cost/benefit perspective. Itcosts __—_that we will assign high weights to studies whose results we favor and low |

something to reduce errors, and it probably costs more and more to get rid a weights to those we do not favor (Glass, 1976). The ideal solution is to have :
of each error as there are fewer of them left. We may not feel it to be wise to | each study coded by several excellent methodologists who have no special . i
give up half our research to be able to pay for bringing our accuracy rate | ___ jnvestment in the area being investigated. Their quality assessments would i
from 99.0% to 99.9%, if that should be the price. | be made twice; once based only on their reading of the methods section and
Finally, there is something we can do to keep our errors random with | once based on their reading of the methods plus results section. The reason |
respect to our hypotheses, so that they will not increase Type I errors. We | for the first rating is to ensure that at least one judgment of quality is made |

can keep the processes of data collection and analysis as blind as possible for a before the judge has learned the results of the study. One should be able to |

oe as long as possible. | assess at least the design features relevant to both internal and external
Of course it is no basis for rejoicing to learn that errors may be nearly | validity before reading the results. :

a universal even if they are not typically very damaging in their magnitude, __ The specific judgments to ask of our methodologists can range from the
Yet one desirable consequence of widespread awareness of error might be | most general question of overall quality rated on a 9-point rating scale, to |
oe to generate a more task-oriented attitude toward error than is currently | intermediate level questions of quality of design, quality of statistical analy- ;
_ widely shared. Too often the current attitude is that poor scientists (they) - sis, quality of ecological validity, and the like, all rated on a 9-point scale, to |
: make errors; good scientists (we) don’t make errors. Given this attitude, | a series of very specific questions such as: Was random assignment of sub- :
A when we reanalyze others’ data, we may wax indignant or even triumphant | jects employed? Was the assumption of independence of errors in the analy- '
: when we find errors. Our goal, it should be remembered, is not to show 7 sis of variance met?:-Whether highly specific variables are judged or not, in :
Co someone’s answer wrong; our goal is to get the answer right. Perhaps if we | the end one overall variable (or a smallish number of fairly general varia- (
j held this more task-oriented attitude, investigators would be more willing to | bles) relevant to quality will be constructed and will be correlated with size ’
Poe let others examine their data. Then, perhaps, there would be adrop inthe _ of the effect obtained (Glass, et al., 1981; Rosenthal & Rubin, 1978). (
_. [ frequency of raw-data-consuming fires, a frequency that exceeds the limits __ Glass et al. (1981) have presented convincing evidence that, in the typi- |
fo of credibility (Wolins, 1962). _ cal meta-analysis, there is no strong relation between the quality of the study i
| . . - and the average size of the effect obtained. Nevertheless, whether such a i
ILB. Evaluating the Quality of Research Results ; . ; - relation exists should be assessed specifically for each question being ad- i:
_ A In our earlier discussion of the reliability of retrieval we examined evie [ dressed meta-analytically. Once our methodologists have assessed each
dence relevant to the accuracy of coding of various features of the studies | study for quality, we must assess the assessors. The assessment is made
/ retrieved. High rates of coder agreement were found for such variables as | empirically by determining their reliability. We do not expect reliability co- |
; 4 subjects’ average age and the periodical 0 which the study appeared. Lower ___ efficients to be extremely high for complex judgments of research quality :
_ agreement was found for features requiring a greater degree of personal _____ (Fiske, 1983). Nevertheless we need to know the reliability for several rea-
judgment. In this section we continue the discussion of reliability but with sons. Perhaps the main reason is that knowing the reliability suggests
oa an emphasis now not on what the study did, but how well the study investi- _____ whether we will need to increase our sample of judges of research quality, i
Co gated its topic. | i
a One of the major criticisms of meta-analyses is that poor studies are sum- | ILC. The Reliability of Judgments of Quality i
Po marized as well as good studies. Wise meta-analysts make it their businessto. ; a ; : :
a locate all the studies, poor as well as good. Wise traditional reviewers do the WC.I. Eff ective reli ability. Suppose we had available two judges of the :
_ same. Once all the retrievable studies have been found, decisions can be made __. ae y of the studies nour meta-analy SIs. The correlation coefficient re
Co about the use of any study. Precisely the same decision must be made about | ecting the reliability of th © two judges ratings would be computed ‘0 give |
_ every study retrieved: How shall this study be weighted? Dropping a study is | us our best (and only) estimate of the correlation likely ‘0 be obtained be- :
ro _____ tween any two judges drawn from the same population of judges. This cor-

----- Page 33 (ocr) -----
52 -
im. co a
ad of cie
: fo eb th nt
re y asi er » th
: x Ss eli en, i
Ww am in iabi
er pl gle j ili 18
vere single Iu om Bee ME
vain r “la the rel hes butt curva us TA-ANA
: aini iabi relj c r vari ef L -
a judge’ reliabil wo ility elatio r the le, wh 1s IC PR
Intuition w ility i uldt of th n between ich i t, h OCE _ a
Ps itio do yin he em we nof is ow DU _ _
ve ee ee fan one ne en sO, Avot vudees ating 9 Q |
| result ability C ey that a nd to sof ase Ho Jdge® ratings Sup ality :
€ . am di an 0) s ti s |
ose soporte Our nt forthi degree m ce cond gsc or th : quali, |
w en n m i us au e e 7 a
ell-k Lev, meen would ple tine . Idee’ se th should | 8 |
nown 953 y by be s houl y am see's rand € sec id | KER
Sp ). Wi Ch up d fu ean i ho om ‘ond | ° Re
ear ith arle port rth in m 2 err | Rig *
whe man nota ’S Sp edb erin ter-j gree Ors, | ene +
re -Br tio ear y an cre udg wi _ n QE w a * —_
R= ow. na ma ol ase ec th | & |B 30 ~t
: a nN It n d oO or a aLEN RR 33 ——
4 re er an an ur ‘ T 2 a
ae =n ecti ~ —— 0 sui ili ell fe | ed £5 * * * Cn
r= umb: ive” 1 or . uit jam -kn ce . 135 RRB * * aS a
o : me er Oo relia + ( our Bro own a wn agg 89 20 90 a xe E li
a / U: cor relia udge ity Lr rent in - aa 9 2 b Bx ait ee 7 '
a rou oF this fe ns). amon | gh IRB RRS RAS ** paras | |
a eee g alln j | & Q & BRE S85 RRs ee bt ra |
: . a * * i Ai
| paso e mean relia udges ou | a 828 RAR ERB s tit |
3 S jud avai nr es Gi ] * ° 28 2g Rw * ** i:
7 110 has nid te i, ilable wlabilty a -€., me | Slee BRAK BREE BE « wad , /
a T as b to i show tou ility aco an | z eg RS Bm * at |
a h ee. im e 8 am m of 5 Val 38 aa Re a 4% ** 7
th et n vesti SS . Thi Ct) pa n a ~ n BS aS Ra |
€ ab P st enti 1 n by (n - ter S rR BR ges LB * i:
i n 1 re 1g. t Sa £ abl _ we Ss ee Rw * Li
call am. um e gi par ato iall Ss the e 1 | 3 12 re so o> R at 1
/ fey be ive ed rs y th um: m gro )/2 - S g 3 o 28 8 RH * ef 7
n r st e em e pti sel UW | & 3 A ve gg SS * |
ea g th of j h m s io Vv p : : 3 IS Sx x2 Sz * |
. cho e ju jud e eff ployi ployi ame nis sand wi | as 3 ese Bax BER RR ss :
i f th dg ges ecti ing ing de virt d wi dg . ‘s 3 in “eg PEN Se a i |
| j (1) G e fol es. Iti maki iver the S thes gree ual with cS Ww | (2 a = +19 228 RRS :- SS ** . |
: ud iv lowi iS i in eli Pp e an of y th th oul a ra) § “sg SRB ta ae *
oe : ge en Wi in gt abili ea d reli e e id tet $ Qe Sa a8 Rw *
a8 jud Ss, W a ng ten he iit rm. r iabi sam ac a f= eo = oO i 6 Ea} Fan) : fo Ne) an [ia] *
: ges’ ven an que ded ob y,R an-B elat i tual | > Fle RBS Raa 225 RR , |
the s’ rati {1s tai stio to ser , fo ro ed y- as th . = 1S 2% ed SS * i
a ap tin the ned ns: rv rea wn fi met at . = a Se OB mt > ew «+
( pro gs? ap or . ilita ions ch Or hod: ] 8 a ee a 22 Rw *
: 2) pri ? Th pro esti te , an of m s C & a4 rg 225 2 © 90 96
th Gi iat e xi im ge d se ula , 7 - = on By sf 2S Eg
a e ivi er va im. at tti r ve . abl _ g ga a3 Sg SS *
re num en th: ow lue ate edm ing . the ral v: € . ee 2 gen $85 ass >> @ e+
: | qui be e (n) of R effe ean app m alu . ® 12 m © 53 xe 22
: t ire ro val an : ecti TOxi ean e. . 2 RE Le Zs 4 22
a h d u d iS iv reli xi so L = Ss as Ss 29 a
: en m , of j eo co. re e liabi m. reli fn a 3 in 8a oa x3 eS * |
a on of j ean r judg f the lum ad f a ate an iabili | = N18 SSR $0 50 &9 SRE RRR .
ne c e de judge eliabi eS av: © obt in (t) rom t ilit yf swe ity | a3) aes RE BR RRS BRE |
olum sired os avai ility, ailab ained ; he tal Ys R , anda rs to _ Es} RES RRS Ra SARS a* |
nh is ilab 0? T le or ble , oft sam | ES} as ae 9 £5
oe JS reach le and ae ne desir at th he m ple | aaes SERA BB SD RRS RR |
8 ed. Th is table is will et effe e inte ean oe | aie ARS eEB SRR SESE *
: a . aS
read wipe me ctiv en ore | Lee SEs S2z BSR 5S |
alue of r red in the ereli cont 2 ays SOF $85 Ras RE |
aeross vats me Prox liabili of | a\22 es ERR Bae SRB
ri til e im. ilit a ow ne eS Ss es i
s the nto core y, R - 9 ome adz S32 SQN ge
n re alu Orr Hy 18 8g 38 a8 ef |
ad e of espo oft gaz Sis Sex BBR RES |
Hue of elo ndin he 8 =a FRG Zeek DRA RS
_ t
a cet | Sse age Rin SS Fo B8 RAS |
oo esp the | = an) Sa 3s a3 gg
ondi a Sis moe I 9 BSBSe See n |
ing | 38 sas 22K age eee BER g
| ES sae RNAH F258 RBS e g815 |
| Sse)" S558 NASA BESS BEERS FE |
= as Naa nee ze |
en , Na Sz E
= t N é ~ \o So
28a aas 8 ca)
P<] es 3 |
ages 2 £
8g ao
SRS Es
x Oo li
S&s 8 ij
Pa) ak |
“ |
a
53 |
i
HH
ie
|

----- Page 34 (ocr) -----
_ | 54 META-ANALYTIC PROCEDURES RETRIEVING AND ASSESSING RESULTS 55 | |
- (3) Given an obtained or estimated mean reliability, r, andthe obtainedor ——_neir effective reliability (R). Table 3.10 could get us R from knowing r but
desired effective reliability, R, what is the approximate number (n) of judges _—_—_ tg get r we would have to compute (40 x 39)/2 = 780 correlation coeffi- |
required? The table is entered in the column corresponding tothe meanrelia, ——_—_cents. That is not hard work for computers, but averaging the 780 coeffi-
bility, r, and is read down until the value of R closest to the one desired ig | _cients to get r is very hard work for investigators or their programmers.
reached. The value of n is then read as the corresponding row title. | __ There is an easier way and it involves the analysis of variance.
Examples of each of the preceding questions may be useful: | Table 3.11 shows a simple example of three judges rating the quality of
(1) Meta-analysts want to work with a quality variable believed to showa ~ five studies on a scale of 1 to 7, and Table 3.12 shows the analysis of vari- |
a mean reliability of .5 and they can afford only 4 judges at the moment. They _ ance of these data. Our computations require only the use of the last
believe they should go ahead with their study only if the effective reliability == column, the column of mean squares (Guilford, 1954). Examination of com-
will reach or exceed .75. Shall they go ahead? Answer: Yes, because Table | putational formulas 3.2 and 3.3 given below shows that they tell how well
| 3.10 shows R to be .80 for an n of 4 and anr of .5. 7 the judges can discriminate among the sampling units (e.g., studies) minus
(2) Meta-analysts who will settle for an effective reliability no less than _ the judges’ disagreements controlling for judges’ rating bias or main effects
. .9 have a sample of 20 judges available. In their selection of quality varia- 7 (e.g., MS encoders — MS residuals), divided by a standardizing quantity. |
bles to be judged by these observers, what should be their minimally ac. Our estimate of R, the effective reliability of the ratings of all the judges
ceptable mean reliability? Answer: 30. | is given by .
(3) Meta-analysts who know their choice of variables to have a mean _.
reliability of .4 want to achieve an effective reliability of .85 or higher. How 1 MS studies — MS residual |
a many judges must be allowed for in their preparation of a research budget? _ Rest.) = "yas studies SS [3.2] . | |
a Answer: 9. 7
ILC.1.a. Product moment correlations. It should be noted that the mean | . Our estimate ofr, the mean reliability or the reliability of a single average |
reliability (r) of Table 3.10 is to be a product moment correlation coefficient | judge is given by
such as Pearson’s r or its special cases, the Spearman rank correlation (rho), | : MS studies — MS residual | |
oe the point biserial r, or the phi coefficient. It is not appropriate to employ _ t (€st.) = Wag studies + (n = DMS residual [3.3] |
such indices of reliability as percentage or proportion agreement, ¢.g., num: a
ber of agreements (A) divided by the sum of agreements (A) and disagree- |
2 ments (D), A/(A + D) or net agreements, (A — D)/(A + D). These indices _ TABLE 3.11
ae should not only be avoided in any use of Table 3.10, but they should be | Judges’ Ratings of Research Quality bo
ae avoided in general because of the greatly misleading results that they can | OO Gudges |
yield. For example, suppose two judges are to evaluate 100 field studies for __ Studies A B Cc x
oo the presence or absence of external validity. If both the judges see external _ rr a a a as
a validity in 98 of the field studies and disagree only twice, they would show 7 2 3 6 4 13
98% agreement; yet the x” testing the significance of the product moment | i 3 5 $ 3 i
correlation phi would be essentially zero! Thus two judges who shared the | 5 1 4 4 9 | |
same bias (e.g., almost all field studies are externally valid) could consist- | s 14 2 24 60 |
7 ently earn nearly perfect agreement scores while actually correlating essen- _ TT a
oe tially zero with one another (phi = --.01). 7 a
ILC.2. Reliability and analysis of variance, When there are only two Anatvsi ven Ble 3.12 one |
re judges whose reliability is to be evaluated it is hard to beat the convenience _ nalysis of Variance of Judges’ Ratings |
7 of a product moment correlation coefficient as an appropriate index of relia- | Some SSS |
bility. As the number of judges grows larger, however, working with corre: Studies. ~SOSOCSCt gg |
| lation coefficients can become inconvenient. For example, suppose we em Judges 11.2 2 5 60 i]
- ployed 40 judges and wanted to compute both their mean reliability (r) and Residual 68 RS |

----- Page 35 (ocr) -----
56 META-ANALYTIC PROCEDURES | _gg TRIEVING AND ASSESSING RESULTS 37 |
where n is the number of judges as before (equation 3.3 is known as the = ___ The question we want to put to these data is: What is the degree of agree-
| intraclass correlation). For our example of Tables 3.11 and 3.12 we have | t, in the sense of reliability coefficients, among the three levels of qual-
a7 | ity? We address the question via analysis of variance and find:
Rest.) = 6.00 ~ 88 | MS studies — MS residual 1821 — .0571 |
| Rest.) = ““""""MiSstudies. —=—S=*=é<“‘S*<CS*S~SSSC*C*«‘CS C(O
and , 7 |
] Therefore, the effective reliability of the differentiation of the 11 meta-
6.00 — 0.85 7 analyses is seen to be about .69. Our estimate of the mean reliability (r), or
r (est.) = 6.00 + (3 — 190.85 ~ 669 _ e reliability of a single level of quality is:
th quality
In the present example it will be easy to compare the results of the analy- | MS studies — MS residual 1821 — 0571 |
sis of variance approach with the more cumbersome correlational ap- r (est.) = M4 studies + (n — DMSresidual ~ 1821+ (3 — 1.0571 ~ “422
: proach. Thus the correlations (r) between pairs of judges (rap, fac and rac) |
are .645, .582, and .800 respectively, and the mean intercorrelation is .676 ; . ;
which differs by only .007 from the estimate (.669) obtained by means of __ In this example, the correlation of the high quality with the medium
the analysis of variance approach | __ quality studies was .558, while the correlations of high with low and me-
co If we were employing only the correlational approach we would apply | dium with low quality studies were .381, and .376 respectively. The mean of
a the Spearman-Brown formula (equation 3.1) to our mean reliability of .676 _____ these three reliabilities was .438, a value quite close to that obtained from
to find R, the effective reliability. The result is - the analysis of variance (.422). All in all, the low quality studies do not
, ____agree as well with the others (the high and the medium) in differentiating
(3)(.676) | __ the meta-analyses. a |
Re TT A = «862 |
a7 1 + G ~ 19676) 7 ILC.3. Reliability and principal components. In situations where the rat |
| ; | ings made by all judges have been intercorrelated, and a principal compo-
a which differs by only .004 from the estimate (.858) obtained by means of | ___ nents analysis is readily available, another very efficient alternative to esti-
a the analysis of variance approach. In general, the differences obtained be- | mate the reliability of the total set of judges is available. Armor (1974) has ;
ca tween the correlational approach and the analysis of variance approach are. | developed an index, theta (6), that is based on the unrotated first principal |
quite small (Guilford, 1954). | __ component (where a principal component is a factor extracted from a corre- 4
It should be noted that in our present simple example, the correlational __ lation matrix employing unity [1.00] in the diagonal of the correlation ma- |
approach was not an onerous one to employ, with only three correlations to trix), The formula for theta is
compute. As the number of judges increased, however, we would find our- | |
selves more and more grateful for the analysis of variance approach. | n L-1
. | theta() = =| > [3.4]
a ILC.2.a. Quality of research and effect size. As an additional example of __
i iabili i ine some __
a the computation of reliability from analysis of variance, we exam || where nis the number of judges and L is the latent root or eigenvalue of the
data summarized by Glass et al. (1981). For 11 different meta-analyses, sep-__ : —- : i
- : : : . __ first unrotated principal component. The latent root is the sum of the | 1
oe arate estimates of mean effect size were given for studies judged to be of —__ : ; |
a . . : tas : : eer . squared factor loadings for any given factor and can be thought of as the |
7 high, medium, or low internal validity. On the basis of an analysis weighting __ : : ; BIVe
a , _ . a amount of variance in the judges’ ratings accounted for by that factor. Fac-
the 11 studies equally, the mean effect sizes (A) were found to be .42, 34, __ . ° : | (|
, ; : . . __ tor analytic computer programs generally give latent roots or eigenvalues | i]
and .57 respectively, for the high, medium, and low quality studies. The __ for each factor extracted so that 0 is very easy to obtain in practice | I
ce analogous medians were .48, .31, and .59. These results suggest no very | y easy Pp . |
- great linear effect of quality on mean effect size obtained. (Note that though | I.C.4, Reporting reliabilities. Assuming we have done our reliability i
ao “poor” studies tend to show larger effects, “good” studies tend to show | __ analyses well, how shall we report our results? Ideally, reports of reliability |
it larger effects than intermediate studies.) _ analyses should include both the mean reliability (the reliability of a single |

----- Page 36 (ocr) -----
| 58 META-ANALYTIC PROCEDURES , |
- judge) and the effective reliability (reliability of the total set of judges or of T :
_ the mean judgments). The reader needs to know the latter reliability (R) _ _
. because that is, in fact, the reliability of the variable employed in mog _ |
cases. However, if this reliability is reported without explanation, the reade; 7 |
may not be aware that the reliability of any one judge’s ratings is likely ty __

a be lower, often substantially so. A reader may note a reported reliability of |

oo -80 based on 12 judges and decide that the variable is sufficiently reliable _

for his or her purposes. This reader may then employ a single judge only to 7 _ A

find later that this single judge was operating at a reliability of .25, not .80, :
Reporting both reliabilities avoids such misunderstandings. | ; — |

IL.C.4.a, Split-sample reliabilities. A related source of misunderstanding jg | Comp aring an d Combining

the reporting of correlations between a mean judge of one type with a mean 7 Resear ch Results
judge of another type. For example, suppose we had 10 male and 10 female _ |
judges, or 10 student and 10 faculty judges. One sometimes sees in the litera. | |

ture the reliability of the mean male and mean female judge or of the mean 7 A framework for meta-analytic procedures is described in which the comparing function
student and mean faculty judge. Such a correlation of the mean ratings made__ and the combining function of meta-analytic procedures are distinguished. Procedures |
by all judges of one type with the mean ratings made by judges of another type | are provided for comparing and for combining the tests of significance and the effect size | |
can be very useful, but they should not be reported as reliabilities without the | ___ «stimates from two or more studies. |
explanation that these correlations might be substantially higher than the av- -
erage correlation between any one male and any one female judge or between _ °

a any one student and any one faculty judge. The reasons for this are those | I, A FRAMEWORK FOR META-ANALYTIC PROCEDURES
discussed in the earlier section on effective reliability. | In this chapter we consider in detail the application of various meta-
ILC.4.b. Trimming judges. It sometimes happens that when we examine | analytic procedures. Before we wax computational, however, it will be USE~ i
the intercorrelations among our judges we find one that is very much out of | ful to consider a general framework for putting into perspective a variety of '
] line with all the others. Perhaps this judge tends to obtain negative correla. ___—‘Meta-analytic procedures. Table 4.1 provides a summary of four types of
tions with other judges or at least to show clearly lower reliabilities with other ___—~—‘™M¢ta-analytic procedures that are applicable to the special case where just
Lae judges than is typical for the correlation matrix. If this unreliable judge were | two studies are to be evaluated. It is useful to list the two-study case sepa- |
a dropped from the data, the resulting estimates of reliability would be biased, __—““@#ely because there are some especially convenient computational proce- ‘
| i.e., made to appear too reliable. If a judge must be dropped, the resulting bias __—*4UFeS for this situation. The two columns of Table 4.1 show that there are |
can be reduced by equitable trimming. Thus if the lowest agreeing judge is (W0 MAJOT ways to evaluate the results of research studies — in terms of their |

dropped, the highest agreeing judge is also dropped. If the two lowest agree- 7 statistical significance (e.g., p levels) and in terms of their effect sizes e. 2, |

ing judges are dropped, the two highest agreeing judges are also dropped and | the difference between means divided by the common standard deviation 7 |
so on. Experience suggests that when large samples of judges areemployed __—_—%'S: indices employed by Cohen [1969, 1977, 1988] and by Glass [1980] |
Co the effects of trimming judges are small as is the need for trimming. When the i and Hedges [1981], or the Pearson r). The two rows of Table 4.1 show that |
ur sample of judges is small, we may feel a stronger need to drop a judge, but _ there are two major analytic processes applied to the set of studies to be
L doing so is more likely to leave a residual biased estimate of reliability. A safe | _-°Valuated: comparing and combining. The cell labeled A in Table 4.1 repre- |
7 procedure is to do all analyses with and without the trimming of judges and to | sents the procedure that evaluates whether the significance level of one study i
7 report the differences in results from data with and without the trimming. _“iffers significantly from the significance level of the other study. The cell |
cy Although the method of trimming judges seems not yet to have been system- | _ [labeled B represents the procedure that evaluates whether the effect size (e.g., |
a atically applied, the theoretical foundations for the method can be seen in the | dorr) of one study differs significantly from the effect size of the other study. i
a writings of Mosteller and Rourke (1973), Tukey (1977), and Hoaglin, Mostel- 7 I
- ler, and Tukey (1983). | ;

----- Page 37 (ocr) -----
oo 60 META-ANALYTIC PROCEDURES | _ COMPARING AND COMBINING RESULTS 61 i
| | Cells C and D represent the procedures that are used to estimate the overal 7 tests, or contrasts, we learn whether the studies differ significantly among /
a level of significance and the average size of the effect, respectively. Iustra. | themselves in a theoretically predictable or meaningful way. Thus impor- |
tions of these procedures will be given below. | tant tests of hypotheses can be made by the use of focused tests. Cells E and | |
7 Fof Table 4.2 are simply analogues of Cells C and D of Table 4.1 represent- : |
_ TABLE 4.1 ; d 7 ing procedures used to estimate overall level of significance and average /
| Four i Secniairtinaies hamharnahll | __ size of the effect, respectively. |
_ ale Defined in Termsafe Il. META-ANALYTIC PROCEDURES: | [
- Significance Effect size 1 TWO INDEPENDENT STUDIES /
Analytic Process testing estimation | Even when we have been quite rigorous and sophisticated in the interpre- /
Comparing studies A | - tation of the results of a single study, we are often prone to err in the inter- [i
oo | pretation of two or more studies. For example, Smith may report a signifi- |
a Combining studies Cc D | cant effect of some social intervention only to have Jones publish a rebuttal |
7 demonstrating that Smith was wrong in her claim. A closer look at both |
| their results may show the following:  ° |
a TABLE 4.2 | Smith’s Study: (78) = 2.21, p<.05,d = 50, r= .24. :
_ Six Types of Meta-Analytic Procedures | Jones's Study: (18) = 1.06, p> .30,d = .50, r= .24. |
Applicable to a Set of Three or More Studies 1 Smith’s results were more significant than Jones’s, to be sure, but the stud- |
ss Results Defined inTermsof. StS _ ies were in perfect agreement as to their estimated sizes of effect defined by i
- Significance Effect size | either d or r. A further comparison of their respective significance levels re- i
oe Analytic Process testing estimation 7 veals that these p’s are not significantly different (p = .42). Clearly Jones was :
a. Comparing studies: A | quite wrong in claiming that he had failed to replicate Smith’s results. We |
Diffuse tests - shall begin this section by considering some procedures for comparing quan- |
| | Comparing studies: a titatively the results of two independent studies, i.e., studies conducted with ot
Focused tests © | different research participants. The examples in this chapter are in most cases roy
| | hypothetical, constructed specifically to illustrate ‘a wide range of situations
Combining studies E F | that occur when working on meta-analytic problems. :
| | ILA. Comparing Studies |
Oo Table 4.2 provides a more general summary of six types of meta-analytic 7 ILA. Significance testing. Ordinarily when we compare the results of
| procedures that are applicable to the case where three or more studies areto two studies we are more interested in comparing their effect sizes than their

aa be evaluated. The columns are as in Table 4.1 but the row labeled ““Compar- __ p values. However, sometimes we cannot do any better than comparing their | 7
og ing Studies” in Table 4.1 has now been divided into two rows —one for the | p values and here is how we do it (Rosenthal & Rubin, 1979a): For each of |
: toes of diffuse tests and one for the case of focused tests. | the two test statistics we obtain a reasonably exact one-tailed p level. All of | /
ol When studies are compared as to their significance levels (Cell A) of | the proceaures aes in this chapter require that p levels be recorded as i
oe their effect sizes (Cell B) by diffuse tests, we learn whether they differ sig- __ One-tavied. ‘Thus tl ) = 1.98is recorded as p= 025, not p = .05. Then as | 7
. nificantly among themselves with respect to significance levels or effect _ an illustration of being reasonably exact, if we obtain (30) = 3.03 we give p ou
oo sizes, respectively, but we do not learn how they differ or whether they _ ie 0025, not as ~~ 09. Extended tables of the t distribution are helpful \
_ differ according to any systematic basis. When studies are compared asto _ here (.g., Federighi, 1959; Rosenthal & Rosnow, 1984a; 1991); as are i
| their significance levels (Cell C) or their effect sizes (Cell D) by focused _ Inexpe nsive calculators with built-in distributions of Z, t, F, and x’. For each ‘
iG 7 P, we find Z, the standard normal deviate corresponding to the p value. Since |

----- Page 38 (ocr) -----
/ 2 META-ANALYTIC PROCEDURES ———_—cgMPARING AND COMBINING RESULTS 63 =
. both p’s must be one-tailed, the corresponding Z’s will have the same sign if 7 worst case scenario for inferential errors where investigators might con- |
/ both studies show effects in the same direction but different signsifthe results —_clude that the two results are inconsistent because one is significant and the i
are in the opposite direction. The difference between the two Z’s when 7 other is not. Regrettably, this example is not merely theoretical. Just such
a divided by V2 yields a new Z that corresponds to the p value that the __ errors have been made and documented (Rosenthal & Gaito, 1963, 1964).
difference between the Z’s could be so large, or larger, if the two Z’s did not | The Z’s corresponding to these p’s are 1.64 and 1.55. From equation 4.1 we |
really differ. Recapping, have |
| Z,-Zy (1.64) = (1.55) |
ao 21 ~ 22 is distributed as Z [4.1] Tt a = a = .06
Example 1. Studies A and B yield results in opposite directions and nei- | aso obtained Z of the difference between ap value of .05 and .06. The p '
Co ther is “‘significant.” One p is .06, one-tailed, the other is .12, one-tailed but __ value associated with this difference is 476 one-tailed or .952 two-tailed. i
in the opposite tail. The Z’s corresponding to these p’s are found in a table | __ ifhis example shows clearly just how nonsignificant the difference between
of the normal curve to be +1.56 and —1.18. (Note the opposite signs to significant and nonsignificant results can be.
indicate results in opposite directions.) Then, from the preceding equation 1 ILA.2. Effect size estimation. When we ask whether two studies are tell-
7 (4.1) we have ____ ing the same story, what we usually mean is whether the results (in terms of i
| the estimated effect size) are reasonably consistent with each other or
L Z~ 4 = (1.56) — (~ 1.18) = 1.94 - whether they are significantly heterogeneous. The present chapter will em- ‘|
: v2 1.41 , _____ phasize ras the effect size indicator but analogous procedures are available |
| | for comparing such other effect size indicators as Hedge’s (1981) g or differ- |
' as the Z of the difference between the two p values or their corresponding | __ ences between proportions, d’ (Hedges, 1928b; Hsu, 1980; Rosenthal & |
_ Z’s. The p value associated with a Z of 1.94 is .026 one-tailed or .052two- | _ Rubin, 1982a). These will be described and illustrated shortly. |
tailed. The two p values may be seen to differ significantly, suggesting that __ For each of the two studies to be compared we compute the effect size r |
a we may want to draw different inferences from the results of the two studies. ___and find for each of these r’s the associated Fisher z, defined as 1 log, [(1 + i
a ; ; ; oo, a 1/1 - 1]. Tests of the significance of differences between r’s are more i
Example 2. Studies A and B yield results in the same direction and both | accurate when this transformation is employed (Alexander, Scozzaro, & |
a are significant. One p is .04, the other is -000025. The Z’s corresponding to | Borodkin, 1989). In addition, equal differences between any pair of Fisher |
a these p’sare 1.75 and 4.06. (Since both Z’s are in the same tail they have the ____2,’s are equally detectable, a situation that does not hold for untransformed |
a same sign.) From equation 4.1 we have | t’s. Tables to convert our obtained r’s to Fisher z,’s are available in most |
aa ____ introductory textbooks of statistics. Then, when N, and N, represent the |
: Z, ~ Zq (4.06) — (1.75) a . . . . ; |
7 a = 14 _____ number of sampling units (e.g., subjects) in each of our two studies, the |
al V2 1.41 | quantity |
i as our obtained Z of the difference. The p associated with that Z is 05 __ Zn ~ Ley
at one-tailed or .10 two-tailed, so we may want to conclude that the twop values __ ———— {4.2]
a differ significantly or nearly so. It should be emphasized, however, that ' / oo + 5
ee finding one Z greater than another does not tell us whether that Z was | 2 |
. greater because the size of the effect was greater, the size of the study (e.g, | oo, i
: N) was greater, or both. | is distributed as Z (Snedecor & Cochran, 1967, 1980, 1989). |
Example 3. Studies A and B yield results in the same direction, but one is 7 Example 4. Studies A and B yield results in opposite directions with
oo “significant” (p = .05) and the other is not (p = .06). This illustrates the 7 effect sizes of r= .60(N = 15) andr = —.20(N = 100), respectively. The [
- a Fisher z,’s corresponding to these r’s are .69 and —.20, respectively. (Note

----- Page 39 (ocr) -----
. the opposite signs of the z,’s to correspond to the opposite signs of the I's) _ [[A.2.a. Other effect size estimates. Although r is our preferred effect i:
Then from the preceding equation (4.2) we have 7. size estimate in this chapter, analogous procedures are available for such :

| __ other effect size estimates as (M; — M)/S (Hedges’s g) or the difference a
2 ~ 9 - 66) -© 20) 494 7. petween proportions, d’. We begin with the case of Hedges’s g. |
_i + _ i Asef ___ foreach of the two studies to be compared, we compute the effect size
. Nj-3 N,-3 2 97 -. (M, ~ M2)/S (Hedges’s g) and the quantity 1/w which is the estimated i.

: ___yariance of g. We obtain w as follows (Rosenthal & Rubin, 1982a). '

as the Z of the difference between the two effect sizes. The p value associ. _ 5 5 :

ated with a Z of 2.91 is .002 one-tailed or .004 two-tailed. These two effec; 7. w= Amina) + mg = 2) [4.3] |

sizes, then, differ significantly. . (ny + ng)lt? + 2m, + my ~ 2))
Example 5. Studies A and B yield results in the same direction with effect 7 When we have w we can test the significance of the difference between |

: sizes of r= .70(N = 20) andr = .25 (N = 95), respectively. The Fisher z,’g _ any two independent g’s by means of a Z test since :
corresponding to these r’s are .87 and .26, respectively. From equation 4,2 7 :

we have | 8a [4.4]

: : Wa WsB |
7 7% | is distributed as Z, as shown in somewhat different form in Rosenthal and :
| as our obtained Z of the difference. The p associated with that Z is .01 one- 7 Rubin (1982a). Note the similarity in structure between equations 4.4 and /
: . . . || 4.2: In both cases the differences in effect size are divided by the square :
tailed or .02 two-tailed. Here is an example of two studies that agree ona __ ; : ere . ‘
oo Lage ae . . . : root of the sums of the variances of the individual effect sizes. :
_ significant positive relationship between variables X and Y but disagree 7 :
a significantly in their estimates of the size of the relationship. 7 Example 7. Studies A and B yield results in the same direction with effect ;
7 . : ; ; | sizes of g = 1.86 (t = 4.16; N = 20) and g = .51 (t = 2.49; N = 95), |
a Example 6. Studies A and B yield effect size estimates ofr = 00 (N 2 | respectively. Assuming that the two conditions being compared within each :
oo 17) and r = .30 (N = 45), respectively. The Fisher z,’s corresponding to study are comprised of sample sizes of 10 and 10 in Study A and 47 and 48 in !
a these r’s are .00 and .31, respectively. From equation 4.2 we have 7. Study B, we first find w for each study. |
- __(.00) — (631) = ~1.00 | Wax 2(nyny\X(n, + ny — 2) . 2(10)(10)(10 + 10 — 2) = 3.38
Co fiz. _ (ny + ng)ft? + 2m, + ny — 2)) (10 + 10)[(4.16)2 + 2(10 + 10 — 2)} |
Pk 4 42 a
| | wy = 2m Fm 2) = ATABYAT + 48 = 2) = 29.98 '
/ a as our obtained Z of the difference between our two effect size estimates. | (ny + ng)ft? + 2ny + my — 2)} (47 + 48){(2.49)2 + 2(47 + 48 — 2)) !
: i The p associated with that Z is .16 one-tailed or .32 two-tailed. Here we _ Therefore, from equation 4.4: ;
oe have an example of two effect sizes, one zero (r = .00), the other (r = .30)__ |!
: oe significantly different from zero (t(43) = 2.06, p < .025 one-tailed), but | fa ~ Bn 1.86 — 51 :
: a which do not differ significantly from one another. This illustrates how . a Fe i
: | careful we must be in concluding that results of two studies are heteroge- | a / ! +1
ag neous just because one is significant and the other is not or because one hasa __ wa WB 3.38 © 22.98 |
Lo. i i Rosnow, | ,
oo 1984 ne effect size and the other does not (Rosenthal & Rosn | as our obtained Z of the difference. The p associated with that Z is .01 one- ul
: | a, )- | tailed or .02 two-tailed. Here is an example of two studies that agree there is |

----- Page 40 (ocr) -----
| 66 META-ANALYTIC PROCEDURES 7 COMPARING AND COMBINING RESULTS 67 | |
| C a significant effect of the independent variable, but disagree significantly in 1 g= a ae [4.5] : |
I their estimates of the size of the effect. t f Bit Ma
. Suppose that in the present example we had found Studies A and B but 7 m+n~-2
: : omputed — only t tests. If our preference were ; ; ; . ;
that noe enn a imate we could get r from equation 2.16, |. If our effect size estimate were the difference between proportions (d’),
Recall that 's dN’s for these studies were 4.16 (N = 20) and 2.49 (N= 7 ___ our procedure would be analogous to that when our effect size estimate was |
Recall that ‘ . ans h e can et the twor’s: | _Hedges’s g. Again we need. the estimated variance of the effect size esti- |
95), respectively; then w 6 ____ mate, 1/w. In this application we estimate w by equation 4.6 which works |
_ [2 _ (4.162 = 70 7 well unless nj, or ng is very small and p, or pz is very close to zero or one. If
‘ae t2 + df (4.16)2 + 18 n; OF Nz is very small, a conservative procedure is to replace p (1 — p) by
ee _ 7 its maximal possible value of .25 (i.e, when p = (1 — p) = .50 we find p |
tp = Qo J _ 2.497 = .25 __(1 — p)to be ata maximum and equal to .25). a
2 + df (2.49)2 + 93 7 , |
| ao. 7 we [4.6] 7
We could compare these r’s easily; in fact we did so in example 5. The Z o nyp\(1 — p,) + np,l — p,) _
we obtained there was 2.31, very close to the Z we obtained when compar. 7 | | |
ing g’s (Z = 2.32). | __ In meta-analytic work, however, we are sometimes unable to obtain the
tr from t but had forgotten ___ ’ ever, ble 1 |
Oe eee me ‘Its oocalled how to compare two g’s we call ______ values of n, and nj. Accordingly we employ an approximation to w that |
how to compare two rs. IT we re on 2.27: ____ depends only on the total study size N and the effect size estimate d’ (Rosen- |
convert our r’s to g’s by means of equation 2.27: a thal & Rubin, 1982a): ; |
© Wioe fa | ve [4.7] |
a t example: oo, ; |
For the present examp ______ This approximation to equation 4.6 holds exactly when P; and p> are the | |
70  y fi8G0+ 10) — 1.6 | __ same amount above and below .5 and when n, = np. [
8S 1010 . When we have w we can test the significance of the difference between |
V1 — (70) (10)(10) a ; g |
7 _|_any two independent d’’s by means of a Z test since A
gp = 25 x /® = SI |. i
| eV Cas (47)(48) 7 _fa7 4's isdistributed as Z [4.8]
o Of course, we could also have computed g directly from t by means LS Wr, Wp
| equations 2.25 (or 2.26, or 2.5). From equation 2.25 we have: |T l
a ____asshown in somewhat different form in Rosenthal and Rubin (1982a). Just .
= 141 416 ‘ly 1 = 1.86 ___as was the case when effect size estimates were r and g (equations 4.2 and [
Ba=t — a
a ny ny 10 10 4 4), the differences in effect size are divided by the square root of the sums /
ot ___ ofthe variances of the individual effect sizes.
ou ny ny 47 48 | _ Example 8. Studies A and B yield results in the same direction with effect '
| 1 sizes of d’ = .70(N = 20) andd’ = .25 (N = 95), respectively. Assuming |
Finally, if we should have Cohen’s d available [(M; — Mz)/ol] ae that the two conditions being compared within each study are comprised of | i:
wanted to get g we could do so as follows: - ___ sample sizes of 10 and 10 in Study A and 47 and 48 in Study B, we find w

----- Page 41 (ocr) -----
_ 68 META-ANALYTIC PROCEDURES | CoMPARING AND COMBINING RESULTS 69 cl
| first from equation 4.6. Then, as a further illustration, we also employ the | method, like the method of comparing p values, asks us first to obtain accu- | |
7 approximation equation 4.7: | gate p levels for each of our two studies and then to find the Z corresponding po
_ an (10)(10) - 1 to each of these p levels. Both p’s must be given in one-tailed form and the |
Wa, = I | __ corresponding Z’s will have the same sign if both studies show effects in the
ngpi(l ~ pa) + mipatl ~ Pa) (10).85(,15) + (10).15(-85) 7 same direction. They will have different signs if the results are in the oppo- |
1 site direction. The sum of the two Z’s when divided by V2, yields a new Z. |
nn (47)(48) _ | __This new Z corresponds to the p value that the results of the two studies eal
Wp, = 4 =§ — bined (or results even further out in the same tail) could have occurred be |
nop (1 — py) + nypo(l — Pa) (48).375(.625) + (47).625(.375) | __ combine “oe
___ ifthe null hypothesis of no relationship between X and Y were true. Recap- pe |
Way = —_N_.- — = 39.22, agreeing perfectly with the result 7 ping, | |
—_ ™ above (way). 7 2, + 22 ig distributed as Z [4.9} |
a Wa, = af = —~_ = 101.33, disagreeing only in the second | _ vo"
'-¢ 1 (29) decimal place with the result above 7 We could weight each Z by its df, its estimated quality, or any other desired |
, (Wp,) because this approximation (Wp,) | —_ sights (Mosteller & Bush, 1954; Rosenthal, 1978, 1980).
assumed ny = ng = 47.5 rather than | The general procedure for weighting Z’s is t Itip! h Z by : |
n, = 47 and nj = 48 as in the result : gen prot TENDS 1s fo mUNPIy 6ae y any mo |
above (wg, ). - desired weight (assigned before inspection of the data), add the weighted |
} 7 Z's and divide the sum of the weighted Z’s by the square root of the sum of |
Now, we can test the difference between our two effect sizes from equa- | the squared weights as follows: | |
tion 4.8: | ; wiZ, + WZ, b |
a ’ Weighted Z = ———_———_
a da-dp . 1-258 439 | Vwi? + we {4.10] |
Wa, WB, 39.22 101.32 | Example 11 will illustrate the application of this procedure. |
CL ; . . . . | Example 9. Studies A and B yield results in opposite directions and both
oo as our obtained Z of the difference. The p associated with that Z is .0084 | __are significant. One p is .05, one-tailed, the other is .0000001, one-tailed
one-tailed or .017 two-tailed. This example, example 8, was selected to re- | but in the opposite tail. The Z’s corresponding to these p’s are found in a |
7 flect the same underlying effect size as example 7 and example 5. The three | table of normal deviates to be — 1.64 and 5.20, respectively. (Note the oppo-
: Z’s found by our three methods agreed very well with one another with Z's ____ ite signs to indicate results in opposite directions.) Then from equation 4.9 |
of 2.39, 2.32, and 2.31, respectively. | wehave |
a ILB. Combining Studies |
a a Zi + Zz (—1.64) + (5.20)
. I.B.1. Significance testing. After comparing the results of any two inde- a VE -——_ 2.52 |
a pendent studies, it is an easy matter to combine the p levels of the two |
studies, Thus we get an overall estimate of the probability that the twop | —_as the Z of the combined results of Studies A and B. The p value associated !
levels might have been obtained if the null hypothesis of no relation be- | —_—_with a Z of 2.52 is .006 one-tailed or .012 two-tailed. Thus the combined p | |
7 tween X and Y were true. Many methods for combining the resultsoftwoor _ _ supports the result of the more significant of the two results. If these were ; |
| more studies are available; they will be described later and have been sum- | —_actual results we would want to be very cautious in interpreting our com- | |
ao marized elsewhere (Rosenthal, 1978, 1980). Here it is necessary to giv€ | _ bined p both because the two p’s were significant in opposite directions and !
oo only the simplest and most versatile of the procedures, the method of add- | ___ because the two p’s were so significantly different from each other. We |
ou ing Z’s called the Stouffer method by Mosteller and Bush (1954). This | _ would try to discover what differences between Studies A and B might have | |
a | led to results so different. [

----- Page 42 (ocr) -----
7 META-ANALYTIC PROCEDURES | _cgMPARING AND COMBINING RESULTS 71 Li
Example 10. Studies A and B yield results in the same direction but ng. > oe size estimate in the combining of effect sizes. However, many other
i ther is significant. One p is .11, the other is .09 and their associated Z’s are | —_agtimates are possible (e.g., Cohen’s d, Hedges’s g, or Glass’s A, or differ- |
ae 1.23 and 1.34, respectively. From equation 4.9 we have 7 “ences between proportions, d’). |
| | __Foreach of the two studies to be combined, we compute r and the associ- |
| | (1.23) + (1.34) _ 1.82 / _ ated Fisher z, and have
1.41 |. _
| as our combined Z. The p associated with that Z is .034 one-tailed or .063 T 2 [4.11]
_ two-tailed. _
oe Example 11, Studies A and B are those of example 9 but now we have as the Fisher z, corresponding to our mean r. We use an r to Z, OF Zy tO F table
found from a panel of experts that Study A earns a weight (w)) of 3.4 on —___tolook up ther associated with our mean Z,. Tables are handier than compu-
assessed internal validity while Study B earns only a weight (wz) of 0.9: The ting t from 2, from the following: r = (¢%* . 1)/(e%r + 1) where e
Z’s for Studies A and B had been — 1.64 and 5.20 respectively. Therefore _ 9.71828, the base of the system of natural logarithms. Should we want to do |
emploving equation 4.10 we find "so we could weight each z, by its df, i.e., N- 3 (Snedecor & Cochran, 1967; iH
. ploying eq . Tt _ 1980), by its estimated research quality, or by any other weights assigned
. (3.4)(—-1.64) + (0.95.20) —0.896 ____ before inspection of the data. |
To 7 0.25 a. The weighted mean z, is obtained as follows: |
8 as the Z of the combined results of Studies A and B. The p value associated | __ weighted mean 2y = — [4.12] |
| with this Z is .40 one-tailed or .80 two-tailed. Note that weighting hasledto | ° |
: / a nonsignificant result in this example. In example 9 where there was no Example 14 will illustrate the application of this procedure. |
F weighting (or, more accurately, equal weighting with w,; = w.= 1), thep |_| ; ; |
-F value was significant at p = .012 two-tailed. 7 Example 12. Studies A and B yield results in opposite directions, one r=
oo If the weighting had been by df rather than research quality, and if df for 80, the other r = ~30. The Fisher zZ,’8 corresponding to these r’s are 1.10
oo. Studies A and B had been 36 and 144 respectively, the weighted Z would | _ and -0.31, respectively. From equation (4.11) we have |
| have been 7 . 10) 4 (0.1 |
od ry Fry _ (10) + ($0.31) .395
_ (36)(—1.64) + (144)(5.20) 689.76 _ 465 | 2 2 |
a VGGP + 4a 148.43 7 |
a 7 as the mean Fisher z,. From our z, to r table we find a z, of .395 associated |
oo This result shows the combined Z (p < .000002 one-tailed) to have been — with an r of .38. |
a moved strongly in the direction of the Z with the larger df because of the | Example 13. Studies A and B yield results in the same direction, one r =
7 substantial difference in df between the two studies. Note that when weight __ _ . , . . i
oo ; ; . . _____ 95, the other r = .25. The Fisher z,’s corresponding to these r’s are 1.83 |
_ ing Z’s by df we have decided to have the size of the study play a very large __ . f Pt
— ; . . . and .26, respectively. From equation (4.11) we have if
oe role in determining the combined p. The role is very large because the size _
_ of the study has already entered into the determination of each Z and is 1.83 + 26 /
oo therefore entering a second time into the weighting process. | an 1.045 il
_ ILB.2. Effect size estimation. When we want to combine the results of | ' i
i. two studies, we are at least as interested in the combined estimate of the 7 __ asthe mean Fisher z,. From our z, to r table we find a z, of 1.045 to be :
ae effect size as we are in the combined probability. Just as was the case when associated with an r of .78. Note that if we had averaged the two r’s without |
oe we compared two effect size estimates, we shall consider ras our primafy __first transforming them to Fisher z,’s we would have found the mean r to be

----- Page 43 (ocr) -----
_ 2 META-ANALYTIC PROCEDURE; | __coMPARING AND COMBINING RESULTS 73 i
- (.95 + .25)/2 = .60, substantially smaller than .78. This illustrates that the _ Again, the examples are hypothetical, constructed to illustrate a wide range i
7 use of Fisher’s z, gives heavier weight to r’s that are further from zero jp 7. of situations occurring in meta-analytic work in any domain. Often, of
either direction. _ course, the number of studies entering into our analyses will be larger than
Example 14. Studies A and B are those of example 6 but now we have . Co the number required to illustrate the various meta-analytic procedures.
a decided to weight the studies by their df (i.e., N — 3 in this application), 7 jlL.A. Comparing Studies: Diffuse Tests
Therefore, equation 4.12 can be rewritten to indicate that we are using df as i IILAJ. Significance testing. Given three or more p levels to compare we
weights as follows: it first find the standard normal deviate, Z, corresponding to each p level. All
dfiz. + dfyz, t p levels must be one-tailed and the corresponding Z’s will have the same
weighted Z, = Fra 1a |. sign if all studies show effects in the same direction, but different signs if the
! 2 [4.13] _____ results are not all in the same direction. The statistical significance of the
a. ___ heterogeneity of the Z’s can be obtained from a x? computed as follows
In example 6 we had 1’s of .00 and .30 based on N’s of 17 and 45, respec. (Rosenthal & Rubin, 1979a):
tively. The Fisher z,’s corresponding to our two r’s are .00 and .31. There
fore, we find our weighted z, to be 7. DZ, — Z)? is distributed as y2 with K — 1 df [4.14] i
(17 ~ 3).00 + (45 ~ 3).31 13.02 | __Inthis equation Z; is the Z for any one study, Z is the mean of all the Z’s
Q7-3)+ (45-3) 56 | __ obtained, and K is the number of studies being combined.
a | _ Example 15. Studies A, B, C, and D yield one-tailed p values of .15, .05,
which corresponds to an r of .23. | _.01, and .001, respectively. Study C, however, shows results opposite in
Finally, it should be noted that before combining tests of significance _ direction from those of studies A, B, and D. From a normal table we find the
oo and/or effect size estimates, it is very useful first to test the significance of | Z’s corresponding to the four p levels to be 1.04, 1.64, —2.33, and 3.09.
oe the difference between the two p values or, what is preferable if they are | _ (Note the negative sign for the Z associated with the result in the opposite
of available, the two effect sizes. If the results of the studies do differ we | _ direction.) Then, from the preceding equation 4.14 we have |
oo should be most cautious about combining their p values or effect sizes— __ :
oo especially when their results are in opposite directions. 7 DZ, — Z)? = ((1.04) — (0,86)]2 + [(1.64) — (0.86)]? + [(—2.33) — (0.86)]?
ca 2 + [(3.09) — (0.86)]? = 15.79 . |
oe ILB.2.a. Other effect size estimates. All that has been said about the com. |
oo bining of r’s applies in principle also to the combining of other effect size _ 7 as our x? value which for K — 1 = 4 — 1 = 3 df is significant at p = .0013. !
estimates. Thus we can average Hedges’s g, or Cohen’s d, or Glass’s A, orthe | = —_‘ The four p values we compared, then, are clearly significantly heterogeneous. |
ee difference between proportions, d’, or any other effect size estimate, with or _ . oo, .
a without weighting. The difference in practice is that when we combine r’s we | MLA2. Effect size estimation. Here we want to assess the statistical he-
i typically transform them to Fisher’s z,’s before combining, while with most | terogeneity of three or more effect size estimates. We again emphasize ras !
| other effect size estimates we do not transform them before combining them. _ the effect size estimator, but analogous procedures are available for com-
_|__ paring such other effect size estimators as Hedges’s (1981) g or differences
| | IIL META-ANALYTIC PROCEDURES: | between proportions (Hedges, 1982b; Hsu, 1980; Rosenthal & Rubin, | |
oe ANY NUMBER OF INDEPENDENT STUDIES 7 1982a). These will be described and illustrated shortly. | |
oo | For each of the three or more studies to be compared we compute the ||
_. Although we can do quite a lot in the way of comparing and combining _ effect size r, its associated Fisher z,, and N ~ 3, where N is the number of |
: a the results of sets of studies with the procedures given so far, it often hap: sampling units on which each r is based. Then the statistical significance of
oc pens that we have three or more studies of the same relationship that we the heterogeneity of the r’s can be obtained from ay? (Snedecor & Cochran, i
_. want to compare and/or combine. The purpose of this section is to present 1967, 1980, 1989) because i
generalizations of the procedures given in the last section so that we can , I
compare and combine the results of any number of independent studies. | |

----- Page 44 (ocr) -----
7 META-ANALYTIC PROCEDURES | comparING AND COMBINING RESULTS 75 |
SNj ~ 325 — Z,)7 is distributed as x? with K — | df [4.15] 7. _ Lwygi |
7. gS |
_ a , _ yj [4.18] |
In this equation Zy, is the Fisher z, corresponding to any r, and Z, is the _
. weighted mean z,, i.€., 7 Note the similarity in structure between equations 4.17 and 4.15 and be-
- S04) — 20% | tween 4.18 and 4.16. Equation 4.17 will be an adequate approximation in |
| he most circumstances but it will lose some accuracy when sample sizes are
_ DAN] — 3) (4.16) very small and t statistics are large. |
oo Example 16. Studies A, B, C, and D yield effect sizes of r = .70(N = 30), - Example 17. Studies A, B, C, and D yield effect sizes of g = 1.89 (N =

: r= .45(N = 45), r= .10(N = 20) andr = —.15(N = 25), respectively. The 30), g = -99 (N = 45), 8 = 19 (N = 20) and g = —.29 (N = 25), respec- i
a Fisher z,’s corresponding to these r’s are found from tables of Fisher z,tobe __ tively. To employ equations 4.17 and 4.18 we will need to compute w for |

87, .48, .10, and —.15, respectively, The weighted mean z, is found from __eacheffect size. Equation 4.3 showing how to compute w requires knowing

7 the equation just above (4.16) to be | ithe sample sizes of the two groups being compared in each study (n, and np)

| ss well as the results of the t test. If the t tests were not available we could
(27(.87) + 42(.48) + 1710) + 22(~.15)) 42.05 _ 39 | _compute our own from equations 2.4, 2.5,.2.25, or 2.26, for example: |
oe [27 + 42 +17 + 22] 108 yt /
a _ 8
_ Then from the equation for x? above (equation 4.15) we have 7. as + 1
_ YNj — 3)25 — Z)? = 27.87 — .39)2 + 42(.48 — .39)? + 17610 — .39)? a . a
| + 22(— 15 — 39) = 14.41 ___ ifthen, and nz values are not reported but N (i.¢., ny; + nz) is known and if it
7 oo | __isreasonable to assume approximately equal sample sizes, we can replace
| as our X value which for K — 1 = 3 df is significant at p = .0024. The four | nyandn, by N/2. In that case equation 4.19 simplifies to |
_ effect sizes we compared, then, are clearly significantly heterogeneous. |
. HLA.2.a. Other effect size estimates. Although r is our preferred effect 7 VN |
oo size estimate in this chapter, analogous procedures are available for such _ t=g x ——— [4.20]
_ other effect size estimates as (M,; — M.)/S (Hedges’s g) or the difference __ , |
| between proportions (d’). We begin with the case of Hedges’s g. — and equation 4.3 simplifies to |
: . For each of the studies in the set we compute Hedges’s g [(M; — M2)/S] / _
a and the reciprocal (w) of the estimated variance of g (1/w). We sawin w= NW? i
equation 4.3 how to compute w (Rosenthal & Rubin, 1982a): _ (2 + 2N — 4) [4.21] ot
oe wa An jny\(ny + ny — 2) aT Since in the present example we were not given nj, nz, ort for studies A,
(n, + nylt? + 2(ny + my - 2)) [4.3] | B,C, and D, weemploy equation 4.20 to obtain t and equation 4.21 to obtain
| | 7 w for each study. Table 4.3 shows the results of these computations which
oo Once we have w we can test the heterogeneity of the set of g’s because _—_afe shown in detail only for Study A for which N = 30 and g = 1.89. From i
_ Hedges (1982b) and Rosenthal and Rubin (1982a) have shown that | _ equation 4.20 we find
| a i S.wj(g) — 2)? is distributed approximately as x? with K — 1 df [4.17] 7 t=gx x = 189 x - = = 518 |
oo | The quantity £ is the weighted mean g defined as _ ,

----- Page 45 (ocr) -----
a 76 META-ANALYTIC PROCEDURE; —_coMPARING AND COMBINING RESULTS 77
: TABLE 4,3 FF oem tog’s, still assuming approximately equal sample sizes within each con-
Work Table for Comparing Four Effect Sizes (g) _ dition, we can simplify the conversion equation 2.27 to the following: |
: Study N ga rb 12 we we -_ 2r N-2 /
A 30 1.89 5.18 26.79 507 9.58 -— |. a” Vie * N [4.22] |
oo. B 45 99 3.32 11.03 9,97 9.87 _ a
_ c 20 19 42 18 4.98 95 . ' ; aT : a
: D 95 99 7 53 618  —1.79 | should we want to convert g’s tor’s we can analogously simplify the conver |
x 120 2.78 8.20 38.53 26.20 «18.61 | __ sion equation 2.28 to the following: |
a, Obtainable from: g = <1. (from equation 4.20). _
b. Obtainable from: t =~ 9VN (equation 4.20), _ BN + AN ~ 2) [4.23]
c. Obtainable from: w = —N(N=2)___ (equation 4.21), | _ffour effect size estimate were the difference between proportions (d’),
Q(t + 2N~ 4) | sour procedure would be analogous to that when our effect size estimate was
: _ __ Hedges’s g. For each of the studies in the set we compute d’ and the recipro- |
; ; cal (w) of the estimated variance of d’ (1/w). The basic estimate of w is |
From equation 4.21 we find: | provided by equation 4.6 which works well unless n, or nz is very small and
___ por pz is very close to zero or one. If n, or ny is very small, a conservative |
oo wa NAD 3028) 5.07 ___ procedure is to replace p(1 — p) by its maximal possible value of .25. We il
: Before we can employ equation 4.17, our x? test for heterogeneity, we must | - ‘we mm .
find g, the weighted mean g (see equation 4.18), which can be found from nyp,(1 — py) + nypal — py) [4.6]
| the appropriate entries in the row of sums of Table 4.3: |
ee a The approximation to this expression that depends only on the total study
a _ Dwg 18.61 n ___ size (N) and the effect size estimate d’ was given earlier as equation 4.7: /
_ Now we can employ equation 4.17 to compute 7: a i
Swi(g) — ¥)? = 5.07(1.89 — .71)2 + 9.97.99 = 72 + 4.98019 — e+ This approximation to equation 4.6 holds exactly when p, and p, are the
oo 6.18(—.29 — .71)2 = 15.37 | __ same amount above and below .5 and when n, = np. |
_ | a Once we have w we can test the heterogeneity of the set of d’’s by means |
_ a x? value which, for K — 1 = 3 df, is significant at p = .0015. The four __ of equation 4.17 (Rosenthal & Rubin, 1982a) but substituting d’ for g: |
oF effect sizes we compared, then, are clearly significantly heterogeneous. —_| Shy ge . i
: el : : : 7 Swj(d' — d’')2 is distributed approximately ; [
_ The four effect sizes of this example were chosen to be the equivalents in _ as x2 withK — 1d. [4.24] i
. units of g to the effect sizes of example 16 which were in units of r. The x73) __ |
_ based on g was somewhat larger (by 7%) than the x?(3) based on r and the pot _ The quantity d’ is the wei ghted mean d’ defined as: !
oe .0015 is slightly more significant than that for example 16 (.0024). The agree: _ ,
_ ment is close enough for practical purposes but we should not expect perfect —__ _ Swi;
oo agreement. Incidentally, if we have available a set of r’s and want to co | Sw; [4.25]
| - | ___ quantity defined analogously to g (see equation 4.18). j

----- Page 46 (ocr) -----
a 78 META-ANALYTIC PROCEDURES | __coMPARING AND COMBINING RESULTS 79 il
| TABLE 4.4 | then, employing equation 4.24 we find:
_ Work Table for Comparing Four Effect Sizes (d’) 7. _
8 Study N d' d'?  J-d'?__ wa wd’ -— + 20.20(.10 — 40) + 25.58(-.15 — 40)? = 14.99
_. B 45 45.2025 1915 56.43 25.394 | a y’ value which, forK — 1 = 3 dfis significant at p = .0018. The foureffect
_ D 25 ~ 15.0225 STIS 25.58 3.837 - ___ The foureffect sizes of this example were chosen to be the equivalents in
B20 10 7250__ 3.2750 161.03 64.751 omits of d’ to the effect sizes of example 16 (r) and example 17 (g). Table 4.5
: a. Obtainable from: w= —N_ (equation 4,7). | __ gummarizes the data for the three effect size estimates of examples 16,17,
ta" | and 18. While the three y2(3) values are not identical, they are quite similar
_ to one another as are the three significance levels. Table 4.5 also suggests
Example 18. Studies A, B, C, and D yield effect sizes of d’ = .70, 45, hat the metric r is quite similar to the metric d’. Indeed, we shall see in the
7 .10, and —.15, respectively. Table 4.4 shows the results of the computation, = final chapter of this book that when the proportions being compared are the
of w foreach of the studies. To illustrate these computations for Study 4 We same amount above and below .5 and when n, = n2, rcomputed from sucha
employ equation 4.7 as follows: |. 4 x 2 table does indeed equal d’. ‘
: ; w= —S = ae = 58.82 qa. ULB. Comparing Studies: Focused Tests |
a |. HILB.1. Significance testing. Although we know how to answer the dif-
_ _ _ fuse question of the significance, of the differences among a collection of
_ Before we can employ equation 4.24, our x” test for heterogeneity, . _ significance levels, we are often able to ask a more focused and more useful |
. must find d’, the weighted mean d’ (equation 4.25), which can be found | question. For example, given a set of p levels for studies of teacher expect-
a from the appropriate entries in the row of sums of Table 4.4: 7 ancy effects, we might want to know whether results from younger children |
| ____ show greater degrees of statistical significance than do results from older
| | Swjd'j 64.751 — children (Rosenthal & Rubin, 1978). Normally our greater interest would be
_ qd = ———— = —— = 40 ______in the relation between our weights derived from theory and our obtained |
a Lj 161.03 _ effect sizes. Sometimes, however, the effect size estimates, along with their
oo - _ sample sizes, are not available. More rarely, we may be intrinsically inter-
a TABLE 4.5 | _ ested in the relation between our weights and the obtained levels of signifi-
| Tests for the Heterogeneity of Effect Sizes | __ cance.
oF Defined as r, g, and d’ | _ Aswas the case for diffuse tests, we begin by finding the standard normal
oe ————— ____teviate, Z, corresponding to each p level. All p levels must be one-tailed, |
eo Effect Sizes 8 : : so :
a 2 | ___andthe corresponding 7Z’s will have the same sign if all studies show effects
oe og |__ inthe same direction. The statistical significance of the contrast testing any
Co Study A - oe a to specific hypothesis about the set of p levels can be obtained from a Z com- |
oo. study C 10 19 10 - 7 puted as follows (Rosenthal & Rubin, 1979a):
an mye a 2NG is distributed as Z [
oo Unweighted mean 3 70 28 i , [4.26] |
Weighted mean 37 7 40 oa i
x3) 14.41? 15.37 14.99 | ___Inthis equation A, is the theoretically derived prediction or contrast weight I
Po 02400150018 ___forany one study, chosen such that the sum of the A,’s will be zero, and Z; is ! :
. a. Based on Fisher's z, transformation. - 7 the Z for any one study.

----- Page 47 (ocr) -----
80 META-ANALYTIC PROCEDURES | _cgMPARING AND COMBINING RESULTS 81 |
| Example 19. Studies A, B, C, and D yield one-tailed p values of 1/107, | Example 20. Studies A, B, C, and D yield effect sizes ofr = .89, .76, .23,
0001, .21, and .007, respectively, all with results in the same direction —— ad 59, respectively, all with N = 12. The Fisher z,’s corresponding to |
a From a normal table or from a calculator with a built-in Z distribution we FT inese 1's are found from tables of Fisher z, to be 1.42, 1.00, .23, and .68,
| | find the Z’s corresponding to the four p levels to be 5.20, 3.72, .81, and 2.45, | sespectively. Suppose that Studies A, B, C, and D had involved differing
Suppose that Studies A, B, C, and D had involved differing amounts of pee- — —_ amounts of peer tutor contact such that Studies A, B, C, and D had involved
tutor contact such that Studies A, B, C, and D had involved 8, 6, 4, and2_ 46, 4, and 2 hours of contact per month, respectively. We might, therefore, |
hours of contact per month, respectively. We might, therefore, ask whether | ack whether there was a linear relationship between number of hours of i
there was a linear relationship beetween number of hours of contact and | __gontact and size of effect favoring peer tutoring. As in example 19, the 1

| statistical significance of the result favoring peer tutoring. The weights of a | _ appropriate weights, or A’s, are 3, 1, —1, and ~3. Therefore, from the
linear contrast involving four studies are 3, 1,—1, and-~-3. (These are obtained —_preceding equation we have
from a table of orthogonal polynomials; see, for example, Rosenthal @ |

Rosnow, 1984a, 1991). Therefore, from the preceding equation wehave === SNM GLA? + (100 + (71.23 + (368 2.99
DAZ _ 5.20 + (3.72 + (“DBL + (3/245 _ UIE og tT. /s*- / = + — + wt + = v2.22 |

_ o- our Z value which is significant at p = .022 one-tailed. The four effect |

a as our Z value, which is significant at p = .006, one-tailed. The fourp | _giyes, therefore, tend to grow linearly larger as the number of hours of con- |
values, then, tend to grow linearly more significant as the number of hours __ act time increases. Interpretation of this relation must be very cautious. |

| of contact time increases. | After all, studies were not assigned at random to the four conditions of

: IILB.2. Effect size estimation. Here we want to ask a more focused ques @ contact hours. Generally, variables moderating the magnitude of effects i

_ tion of a set of effect sizes. For example, given a set of effect sizes for studies | und shou Id not be interpreted as giving strong evidence for any causal
. : . : | __ . relationships. Moderator relationships can, however, be very valuable in :
: of peer tutoring, we might want to know whether these effects are increas. = ; vey: . . wey: |

. Nar . suggesting the possibility of causal relationships, possibilities that can then i

a ing or decreasing linearly with the number of hours of contact per month | oi studied experimentally or as nearly experimentally as possible |

Oo We again emphasize r as the effect size estimator but analogous procedures . i
oo are available for comparing such other effect size estimators as Hedges’s. —_—if[.B.2.a. Other effect size estimates. Although r is our preferred effect
ow (1981) g or differences between proportions (d’) (Rosenthal & Rubin, size estimate in this chapter, analogous procedures are available for such i
8 1982a). These will be described and illustrated shortly. | other effect size estimates as (M, — M)/S (Hedges’s g) or the difference |
As was the case for diffuse tests, we begin by computing the effect size iL | between proportions (d’). We begin with the case of Hedges’s g.
ou its associated Fisher z,, and N — 3, where N is the number of sampling units ro Once again we compute the reciprocal (w) of the estimated variance of g |
ot on which each r is based. The statistical significance of the contrast, testing (dh) for each study. We employ equation 4.3 when the individual sample |
oo any specific hypothesis about the set of effect sizes, can be obtained froma ___ sizes (n, and n.) are known and unequal and equation 4.21 when they are |
Z computed as follows (Rosenthal & Rubin, 1982a): ___unknown or when they are equal. These equations are as follows:
: » > — is distributed as Z _ oe _- Amman +m — 2) |
_ Jy [42 fb (a1 + nf? + 2m + m2 — 2)] [4.3] |
oo In this equation, ), is the contrast weight determined from some theory for wa N= i
. | any one study, chosen such that the sum of the A;’s will be zero. The 2,, is the | _ 2? + 2N — 4) [4.21] | i

a Fisher z, for any one study and w; is the inverse of the variance of the effet | | ;
ow size for each study. For Fisher z, transformations of the effect size r, the | We employ the computed w’s to test the significance of any contrast we | |
variance is 1/(N; — 3) s0 w; = Nj ~ 3. _ 2 wish to investigate. The quantity:

----- Page 48 (ocr) -----
Lo 82 META-ANALYTIC PROCEDURp, | _gyipaRING AND COMBINING RESULTS 83 a.
_ DAigi oo, | tte four effect sizes of this example were chosen to be the equivalents in | |
Xo is distributed approximately as Z as of g to the effect sizes of example 20 which were in units of r. The Z
J 2 7 Ease on g is somewhat larger (by 14%) than the Z based on r (2.01) and the |
| : Maggot Q11 is somewhat more significant than that for example 20 (p = .022). |
co . Sea eas 8. refore, is hardly perfect but it is close enough for practi-
_ an equation that is identical in structure to equation 4.27 (Rosenthal&p, | __‘1he agreement, the , YP Bh torP I |
| is application w, is defined as in equations 4. | _cal purposes. |
oo bin, 1982a), In this ap plication w; s defined as in equations 4 3 or 4.21 and = a meta-analyst has a favorite effect size estimate, he or she need not mo |
_ A; is the contrast weight we assign to the j"study onthe basisofourtheoy, . I lovi diffi ffect si timat
_ The only restriction is that the sum of the A,’s must be zero (Rosenth ae et Ch Oe eee eee a ptnccr aut sheuid nn | |
y j n alg would reach a dramatically different conclusion. However, what should not |
_ Rosnow, 1984a, 1985, 1991). | it done is to employ a variety of effect size estimates, perform the various |
Example 21. Studies A, B, C, and D yield effect sizes of g = 3.56,2,33 __meta-analytic procedures on all of them and report only those results most |
.43, and 1.33, respectively, all with N = 12. As in example 20, we assume __ pleasing to the meta-analyst. There is nothing wrong with employing multi-
8, 6, 4, and 2 hours of peer tutoring per month were employed in Studies. A ___ pleeffect size estimates, but all analyses conducted should also be reported.
B, C, and D, respectively. We ask whether there was a linear relationship | General and special equations showing the relationships between g and r are
: between number of hours of contact and size of effect favoring peer tuto. | __ given as equations 2.27, 2.28, 4.22, and 4.23.
ing. As in example 20, the appropriate weights, or \’s, are3,1,—1,and—3 _ If our effect size estimate were the difference between proportions (d’), |
_ Table 4.6 lists the ingredients required to compute our test of signif. | of procedure would be analogous to that when our effect size estimate was |
cance (Z) for the contrast and reminds us of the formulas that can be usedto __Hedges’s g. Once again we compute the reciprocal (w) of the estimated |
| obtain the various quantities. Now we can apply equation 4.28to find __ variance of d’ (1/w) for each study. We employ equation 4.6 when the indi-
oo | vidual sample sizes n, and nz are known and unequal and equation 4.7 when
a > Nigi 8.39 _____they are unknown or when they are equal. The equations are as follows: |
_ a = ee = 29  . oo
: a : as our Z value which is significant at p = .011 one-tailed. — N
7. en . 4.7
Work Table for Computing Contrasts Among Effect Sizes (g) ____ Once we have w we can test any contrast by means of equation 4.28
- ~¢ —M_ | (Rosenthal & Rubin, 1982a) but substituting d’ for g:
oo. Study N 8° re P re w wo
oo A 12 3.56 6.17 38.02, 3 910.681.0388 _2Ad'i is distributed approximately as Z.
oo c 1243 74 55-1 1 —43 2,92 3a 2 |
D 12 1.33 -2.30 5.31 -3 9 3.99 2.37 3.80 2
oo. z 48 7.45 12.90 57.49 «0-20 8.39 8.11 13.44 _ _Inthis application w, is defined as in equations 4.6 or 4.7 and ); is as defined
_ 2 above.
. a. Obtainable from: g = WN (from equation 4.20). a S
, | b Obtainable from: t= 2 YN (equation 4.20). 1 _ Example 22. Studies A, B, C, and D yield effect sizes of d’ = .89, .76, |
| a ) _. 23, and .59, respectively, all with N = 12. As in example 21, we assume 8, ot |
a c. Determined by theory but with 2A = 0. _| 6,4, and 2 hours of peer tutoring per month were employed in Studies A, B, |
| | d Obtainable trom: w= aS 2} ar (equation 4.21, | C and D, respectively. Again we want to test the linear contrast with A’s of 3, i 7
| 1, -1, and —3. Table 4.7 lists the ingredients required to compute our test ag
a. of significance (Z) for the contrast. Now we can apply equation 4.29 to find: oe

----- Page 49 (ocr) -----
META-ANALYTIC PROCEDURE § _coMPARING AND COMBINING RESULTS 85 : |
. pe = Vas 7 4 a Tests for Linear Contrasts in Effect Sizes Defined as r, g, and d
_ Wj _ 2 Effect Sizes 7] |
as our Z value which is significant at p = .05 one-tailed. —————————————— |
— 89 3.56 89 |
TABLE 4.7 | __ssudy€ 23 4 2B a
Work Table for Computing Contrasts Among Effect Sizes(d')_ indy D 59 1.33 . 59 |
. A 12 8 79 2 3 9 2.67 57.14 Ass 8 022 011 .050 co
B12 .76 58 42 1 1 16 28.57 0380 = oe anbmation |
c 12.23 .05 95 -1 1 ~.23 12.63 079), 2 Based on Fisher's z, transformation.
D 12 59 35 65 -3 9 $1.77 18.46 45 |
S48 247 (241.77 223-0 201.43 116.80 75g) SEC: Combining Studies |
' a. Determined by theory butwith oa = 0. | _sIILC.1. Significance testing. After comparing the results of any set of
. b. Obtainable from: w = JreF (equation 4.7). __three or more studies it is an easy matter also to combine the p levels of the
oo ____ get of studies to get an overall estimate of the probability that the set of p |
Gg ; | __ levels might have been obtained if the null hypothesis of no relationship |
| The four effect sizes of this example were chosen to be equivalent in | between X and Y were true. Of the various methods available that will be
units of d’ to the effect sizes of example 20 (r) and example 21 (g). Table | _described in the next chapter, we present here only the generalized version
a 4.8 summarizes the data for the three effect size estimates ofexamples20, | _gfthe method presented earlier in our discussion of combining the results of F
FF 21, and 22. The three Z tests of significance of the linear contrast are _two groups.
_ i somewhat variable, with the Z for the effect size estimator g being about | __ This method requires only that we obtain Z for each of our p levels, all of
oF 14% larger than that for r and the Z for the effect size estimator d’ being | which should be given as one-tailed. Z’s disagreeing in direction from the
4 about 18% smaller than that for r. However, the range of significance le. bulk of the findings are given negative signs. Then, the sum of the Z’s di-
_ els is not dramatic with the most significant result at p = .011 and the least = =—_vided by the square root of the number (K) of studies yields a new statistic
a significant at p = .050. | __ distributed as Z. Recapping,
oo Before leaving the topic of focused tests, it should be noted that theiruse | __ a]
oo is more efficient than the more common procedure of counting each effect SZ.
7 size or significance level as a single observation (e.g., Eagly & Carli, 8 VE is distributed as Z [4.30] at
| Hall, 1980; Rosenthal & Rubin, 1978; Smith et al., 1980). Inthat procedure ——__
| _ we might, for example, compute a correlation between the F isher Z, values | Should we want to do so, we could weight each of the Z’s by its df, its a
_ and the \’s of example 20 to test the hypothesis of greater effect size being ___ estimated quality, or any other desired weights (Mosteller & Bush, 1954; i
| associated with greater contact time. Although that r is substantial ( TD Rosenthal, 1978, 1980).
a . does not even approach significance because of the small number of df upot The general procedure for weighting Z’s is to multiply each Z by any kK
oo which the r is based. The procedures employing focused tests or contrass = desired weight (assigned before inspection of the data), add the weighted pod
| employ much more of the information available and, therefore, are les ____@s, and divide the sum of the weighted Z’s by the square root of the sum of a
likely to lead to Type Il errors. ____ the squared weights as follows: cP

----- Page 50 (ocr) -----
a 86 META-ANALYTIC PROCEDURES | cg PARING AND COMBINING RESULTS 87 a |

| . SwiZ, _ mple weighting by quality of research did not lead to a very different / |

Weighted Z = Vener | ___esult than was obtained when weighting was not employed (example 23); in |

| ' (4.31) Toth cases p = .04 one-tailed. Actually, it might be more accurate to say for : |

Example 24 will illustrate the application of this procedure. |__ example 23 that weighting was equal with all w's = 1 than to say that no |

| Example 23. Stadies A, B, C, and D yield one-tailed p values of .15, 95 |. ; ae .

1, and .001, respectively. Study C, however, shows results opposite j, ————_—“MEC-?. Effect size estimation. When we combine the results of three or |
direction from the results of the remaining studies. The four Z's associateg __ more studies we are at least as interested in the combined estimate of the ae |

a. with these four p’s, then, are 1.04, 1.64, —2.33, and 3.09. From equation __ effect size as we are in the combined probability. We follow here our earlier |

4.30 we have __ procedure of considering r as our primary effect size estimator while recog- a |

| | pizing that many other estimates are possible. For each of the three or more |

: DZ (1.04) + (1.64) + (2.33) + 3.09) _ | __ gtudies to be combined we compute rand the associated Fisher z, and have |

_ as our new Z value which has an associated p value of .043 one-tailed or .086 _ _ K ‘ [4.32] !

ao two-tailed. We would normally employ the one-tailed p value if we had .

a correctly predicted the bulk of the findings but would employ the two-tailed Ts the Fisher Z, corresponding to our meant (where K refers to the number bo

: p value if we had not. The combined p that we obtained in this example 7 ___ ofstudies combined), We use a table of Fisher z, to find the r associated with | ,

supports the results of the majority of the individual studies. However, even Our mean z,. Should we want to give greater weight to larger studies we

a if these p values (.043 and .086) were more significant, we would want to be _ could weight each z, by its df, i.e,, N — 3 (Snedecor & Cochran, 1967,

_ very cautious about drawing any simple overall conclusion because of the 1980, 1989), by its estimated research quality, or by any other weights ,

_ very great heterogeneity of the four p values we were combining. Example | __ assigned before inspection of the data.

15, which employed the same p values, showed that this heterogeneity was | __‘The weighted mean z, is obtained as follows:

| : significant at p = .0013. It should be emphasized again, however, that this — foe

a. great heterogeneity of p values could be due to heterogeneity of effect sizes, . Weighted Z; = 26ity

fF heterogeneity of sample sizes, or both. To find out about the sources of | —__ Dwi [4.33] a

oo. heterogeneity, we would have to look carefully at the effect sizes and sample _ Hl

_ sizes of each of the studies involved. T. Example 26 will illustrate the application of this procedure. |

7 Example 24, Studies A, B, C, and D are those of example 23 just above, - i Example 25. Studies A,B, C, and D yield effect sizes ofr = .70, 45, 10,

i but now we have decided to weight each study by the mean rating of internal _ Co and —.15, respectively. The Fisher 2, values corresponding to these I's are fall

: oo validity assigned it by a panel of methodologists. These weights (w) were Co .87, .48, .10, and — .15, respectively. Then, from equation 4.32 we have a

| | | 2.4, 2.2, 3.1, and 3.8 for Studies A, B, C, and D, respectively. Employing ft Sn (8D (48) + A) + ID ml

_ equation 4.31 we find: a a ees = 32

og 7 K 4 ra

V3wP Vad + 02° + Gl? + GBP i as our mean Fisher z, From our table of Fisher z, values we find az,of .32to Aue

a correspond to an r of .31. Just as in our earlier example of combined p levels, a

10.623 ____ however, we would want to be very cautious in our interpretation of this a

oo “ Vaaes 1°? - _ combined effect size. If the r’s we have just averaged were based on substan- ia |

. _ CC tial sample sizes, as was the case in example 16, they would be significantly Ul |

oo as the Z of the weighted combined results of Studies A, B, C, and D. The p heterogeneous. Therefore averaging without special thought and comment ‘a

Fe value associated with this Z is .036 one-tailed or .072 two-tailed. In this | Would be inappropriate. i

----- Page 51 (ocr) -----
_ Example 26. Studies A, B, C, and D are those of example 25 just abo, | |
_ but now we have decided to weight each study by a mean rating of ecologic al | i |
_. validity assigned to it by several experts. These weights were 1.7, 1.6, 3 |  . I |
and 2.5 for Studies A, B, C, and D, respectively. Employing equation 4398 |
 . we find:  . |
Swjej _ (1.7)(.87) + (1.6)(.48) + (3.16.10) + (2.5\X—.15) - |
_ Weighted Z = ——4 = —— |
_ = ——— = .24 i
_ 8.90 . i
| as our mean Fisher z,, which corresponds to an r of .24. In thi |. ny ity
our | or Zr P this example f Combining Probabilities
weighting by quality of research led to asomewhat smallerestimate ofcom.
_ bined effect size than did equal weighting (.24 versus .31). _
_ IILC.2.a. Other effect size estimates. Any other effect size, e.g., Cohen's various methods for combining independent probabilities are described and compared. A
| d, Hedges’s g, Glass’s A, the difference between proportions (d’) and soon | __ warning is offered against the direct combining of the raw data of different studies. Fi-
- can be combined with or without weighting just as we have shown for r. The | _ nally, the problem of the “file drawer” is discussed in which studies with null results may
- only difference is that when we combine r’s we typically transform them o | unpublished and unretrievable by the meta-analyst.
' Fisher’s z,’s before combining, while for most other effect size estimates we
_ combine them directly without prior transformation. |. |
y P 7 I. GENERAL PROCEDURES eye
EXERCISES _ |
_ Six experiments were conducted to investigate the effects of a new treatment _ ro Inthe preceding chapter, some basic procedures that can be used to com- |
| procedure. The following table shows the effect size (r) obtained in each study and | _ PATE and to combine levels of significance and effect size estimates were
a the number of patients employed in each study (a positive r means the new treatmen, ___ presented. In addition to the basic procedures presented, there are various 1
a was better): | __alternative methods available for combining probability levels that are es-
a Study Effect Size (r) N | ___ pecially useful under particular circumstances. .
| 1 .64 43 - In this section on general procedures we summarize the major methods |
2 .33 64 | ___ forcombining the probabilities obtained from two or more studies testing 7
oo. 3 .03 39 | __ essentially the same directional hypothesis. Although it is possible to do so,
4 02 46 _|___noconsideration is given here to questions of combining results from stud- co
° 7 Oe an | __ iesin which the direction of the results cannot be made immediately appar- a
. ______ent, as would be the case for F tests (employed in analysis of variance) with hay
| oo 1. Compute the significance level for each of the above studies and give the Zt a> 1 for the numerator or for chi-square tests (of independence In contin- me
) _ associated with each significance level. 7 gency tables) with df > 1. Although this section is intended to be self- a
2. Give the weighted and the unweighted mean effect size for these six studies. | __ contained, it is not intended to serve as a summary of all the useful ideas on |
oo. 3. Give the significance level associated with each of the two mean effect sizes | __ the topic at hand that are contained in the literature referenced. The seminal ial
. of question 2. __| _ work of Mosteller and Bush (1954) is especially recommended. For areview La
4, Report and interpret the results of a test of the heterogeneity of these sixeffed | __ of the relevant literature see Rosenthal (1978a). ed
| | sizes. . . ; ayia |
i 5. Test the hypothesis that larger studies obtained larger effect sizes inthissetol | __ LA. The Basic Methods ; . ; le
ee studies, Report the Z, p, andr derived from this contrast. | ___ Table 5.1 presents the results of a set of five illustrative studies. The first ree |
. 6. Convert the effect sizes given above to Cohen’s d or Hedges’s g. Then answet_ | Column of information about the studies lists the results of the t test. The cp
_ questions 1 to 5 for this new effect size estimate. _ i i |

----- Page 52 (ocr) -----
90 META-ANALYTIC PROCEDURE, | COMBINING PROBABILITIES 91
_. TABLE 5.1 _ = 2442 _ 304, |
_. Summary of Seven Methods for | 7 V6600 |
_. ini ilities of Independent Experiments 7.
| Combining Probabilities p p 7. p = .0013 one-tail |
_. One-tail Effect |. Testing Mean o: |
_ ; Study t df p Size r zZ -2 log. p _ 6. Method of Testing Mean p: |
1 +1,19 40 12 18 9 +1.17 42 | Z = (50 — p)(Vi2N ) [5.6] |
| 2 42.39 60 Ot .29 +2.33 921 '
3 -0.60 10 T2019 -0.58 066 | = (50 — .22)V12(5) = 2.17,
_ 4 +1.52 30 07 27 +1.48 53200 fF - 018 i |
_ 5 +0.98 2 17 s+$0.95 354 pe evens |
_ z +5.48 160 1.09 +.76 +535 22.97 7, Method of Testing Mean Z: |
; +, +1.07 4. 7
a Median +11 . . . _ t= = = 2.26, dt = 4, [5.7] j
7 NOTES: The seven methods follow. Sz’ 22618 |
; _”. p <.05 one-tail
1. Method of Adding Logs: . : |
X(df = 2N) = %~ 2log,p = 22.97 _ or _ i
| p = .011 one-tail ce pan OZ? cog t= 1,4 |
: 2. Method of Adding Probabilities (Applicable when =p near unity or less): a i
: _ . P <.05 one-tail |
 . (Sp) (1.09)
_ p= Wr ser = .013 one-tail (5.2) oo i
| __ sign preceding t gives the direction of the results; a positive sign means the |
a 3. Method of Addingts: | __ difference is consistent with the bulk of the results, a negative sign means |
St 5.48 (a ___ the difference is inconsistent. The second column records the df upon which |
| Bdf/(af ~ 2)) 40/38 + 60/58 + 10/8+ 30/28 + 20/18 | ___eacht was based. ;
_______ The third column gives the one-tailed p associated with each t. It should
ee =~ 248. - 2.3 ____ benoted that one-tail p’s are always less than .50 when the results are in the j
FF v5.5197 | ___ consistent direction, but they are always greater than .50 when the results |
| p = .01 one-tail | ___are not consistent. For example, study 3 with a t of — .60 is tabulated with a i
_ : - ____ one-tail p of .72. If the t had been in the consistent direction, i.e., + .60, the
_ 4. Method of Adding Zs: ___ one-tail p would have been .28. It is important to note that it is the direction |
_ 3Z 5,35 ____ of difference which is found to occur on the average that is assigned the + :
LC Za = Oe = 2.39, bil : i i
_ VW OVE ____ Sign, and hence the lower one-tail p. The basic computations and results are
Po ; ||| identical whether we were very clever and predicted the net direction of |
a p = .009 one-tail _ : .
_ | ___ éffect or not clever at all and got it quite wrong. At the very end of our |
oo 5, Method of Adding Weighted Z's: __ __ calculations, we can double the final overall level of significance if we want i
a tty Zy + dlp Zy + .. + dtgZp : i. __ to make an allowance for not having predicted the net direction of effect. i
a Zee = ee (5 The fourth column of the table gives the size of the effect defined in i
oo. ____ terms of the Pearsonr. ‘
| _ (40)(+1.17) + (60)(+2.33) +... + (20)(+0.95) 7 The fifth column gives the standard normal deviate, or Z associated with
| V(40) + (60) +... + (20) __|___ each p value. The final column of our table lists the natural logarithms of the

----- Page 53 (ocr) -----
_ 92 META-ANALYTIC PROCEDURES | _coMBINING PROBABILITIES 93 7
| one-tail p’s (of the third column of information) multiplied by —2. Each is g | adding the obtained t values and dividing that sum by the square root of the 7
' _ quantity distributed as x? with 2 df and is an ingredient of the first method of | um of the df’s associated with the t’s after each df has been divided by df — 2.
_ combining p levels to be presented in this section (Fisher, 1932, 1938), | ___ The result of the calculation is itself approximately a standard normal
| deviate that is associated with a particular probability level when each of |
oo LA.1. Adding logs. The last column of our table is really a list of x? va}. | te t's is based on df of at least 10 or so. When applied to the data of our table,
| ues. The sum of independent x's is also distributed as x? with df equal to | the Winer method yields p = .01, one-tail, a result very close to the earlier
_ the sum of the df’s of the x2’s added. Therefore, we need only add the five | two results. The limitation of this method is that it cannot be employed when
_ ”’s of our table and look up this new x? with 5 x 2 = 10 df. The results are | the size of the samples for which t is computed becomes less than three,
_ given just below the row of medians of our table; x? = 22.97, which jg | _because that would involve dividing by zero or by a negative value. In
_. associated with a p of .011, one-tail, when df = 10. | addition, the method may not give such good approximations to the normal
The method of adding logs, sometimes called the Fisher method, though _| with df < 10 for each t.
_. frequently cited, suffers from the disadvantage that it can yield results that _
| are inconsistent with such simple overall tests as the sign test of the nul _ [A4. Adding Z's. Perhaps the simplest of all, the Stouffer method de-
hypothesis of a 50:50 split (Siegel, 1956). Thus for a large number of stud. ___gcribed in the last chapter (Mosteller & Bush, 1954) asks us only to add the
ies, if the vast majority showed results in one direction, we could easily | standard normal deviates (or Z’s) associated with the p’s obtained, and divide
: reject the null hypothesis by the sign test even if the consistent p values were _ the square root of the number of studies being combined (Adcock, 1960; I
not very much below .50. However, under these situations the Fisher | Cochran, 1954; Stouffer, Suchman, De Vinney, Star, & Williams, 1949, p. 45).
method would not yield an overall significant p (Mosteller & Bush, 1954), | _EachZ was a standard normal deviate under the null hypothesis. The variance co
Another problem with the Fisher method is that if two studies with equally of the sum of independent normal deviates is the sum of their variances. Here, | |
and strongly significant results in opposite directions are obtained, the this sum is equal to the number of studies, since each study has unit variance. :
/ Fisher method supports the significance of either outcome! Thus p’s of OO | Our table shows results for the Stouffer method that are very close to those |
_ forA > B and .001 for B > Acombine toap < .01 forA > BorB>A ___ obtained by the method of adding t’s (Z = 2.39 vs. Z = 2.33). !
_ (Adcock, 1960). Despite these limitations, the Fisher method remainsthe | __ |
_ best known and most discussed of all the methods of combining indepen. _ 1A5,. Adding weighted Z's. Mosteller and Bush (1954) have suggested a
dent probabilities (see Rosenthal, 1978 for a review of the literature). Be | __ technique that permits us to weight each standard normal deviate by the size |
a cause of its limitations, however, routine use does not appear indicated. ofthe sample on which it is based (or by its df), or by any other desirable |
oo __ positive weighting such as the elegance, internal validity, or real-life repre- ne
oo 1.4.2. Adding probabilities. A powerful method has been described by __sentativeness (ecological validity) of the individual study. The method, il- i 1
_ Edgington (1972a) in which the combined probability emerges when the | _ustrated in the last chapter, requires us to add the products of our weights
. sum of the observed p levels is raised to the power equivalent to the number _—_and Z's, and to divide this sum by the square root of the sum of the squared
of studies being combined (N) and divided by N!. Essentially, this formula | _ weights. Our table shows the results of the application of the weighted :
gives the area of a right triangle when the results of two studies are being | Stouffer method with df employed as weights. We note that the result is the 4
FT combined, the volume of a pyramid when the results of three studies are _ lowest overall p we have seen. This is because, for the example, the lowest p |
: combined, and the n-dimensional generalization of this volume when more __levels are given the heaviest weighting because they are associated with the L [
_ studies are involved. Our table shows the results to be equivalent to those | __ largest sample sizes and df. Lancaster (1961) has noted that when weighting :
oa obtained by the Fisher method for this set of data. The basic Edgington | — isemployed, the Z method is preferable to weighting applied to the Fisher ae
method is useful and ingenious but is limited to small sets of studies, since it | _ method for reasons of computational convenience and because the final sum re
| requires that the sum of the p levels not exceed unity by very much. When | obtained is again a normal variable. Finally, for the very special case of just i
| the sum of the p levels does exceed unity, the overall p obtained tends to be _ fwo studies, Zelen and Joel (1959) describe the choice of weights to mini- i : ;
7 too conservative unless special corrections are introduced. | 2 type I errors. /
| LA.3. Adding t's. A method that has none of the disadvantages of the SL LA.6. Testing the mean p. Bdgington (1972b) has proposed a normal |
. ceding two methods was described by Winer (1971). Based on the result tht __ctrve method to be used when there are four or more studies to be com- va
the variance of the t distribution for any given df is df/(df — 2), it requires | _ Pined. The mean of the p’s to be combined is subtracted from .50, and this | Hh

----- Page 54 (ocr) -----
oo 94 META-ANALYTIC PROCEDURES | _ cgMBINING PROBABILITIES 95 a
quantity is multiplied by the square root of 12N, where N is the number of TABLE 5.2 .
_ studies to be combined. (The presence of a 12 derives from the fact thatthe | Counting Method for Assessing Overall ,
_ variance of the population of p values is 1/12, when the null hypothesisofp) | __ Significance of a Relationship (x? Method)
__ treatment effects is true.) 7. ee i
_ . - Studies Reaching Studies Not :
L LA.7. Testing the mean Z. In this modification of the Stouffer Method, == =—_—Counts p <.05 Reaching p < .05 x !
Mosteller and Bush (1954) first convert p levels to Z values. They then com. 7 Obtained 12 108 120 :
_. pute a t-test on the mean Z value obtained with the dffortequaltothe number | —_ Expected |
i of Z values available minus one. Mosteller and Bush, however, advise againg | __(if null hypothesistrue) 2 120 |
this procedure when there are fewer than five studies to be combined. Tha 7. NOTE: x)= = Or = {2 8r. + eer = 6.32,p = .012 (5.8) |
_ suggestion grows out of the low power of the t test when based on fewobse, !
vations. Our table illustrates this low power by showing that this method _ or, since Z = V3¢(1), Z = V6.32 = 2.51, p = .006, one-tailed. [5.9]

_ yields the largest combined p of any of the methods reviewed. _
a, Computed from .05(N) = .05(120) = 6.

LB. Additional Methods | _ 6. Computed trom .95{N) = .95(120) = 114
1B.1. Counting. When the number of studies to be combined grows 7 5 b df for th : d cell is th ber of studi
large, a number of counting methods can be employed (Brozek & Tied, P > N° The ooserve Tequemy or the second cel Is me mu err stu es |
1952; Jones & Fiske, 1953; Wilkinson, 1951). The number of p values beloy | __ With P > N- The resulting ° can then be entered into a table of critical x |

| .50 can be called + , the number of p values above .50 can be called —, and a values. Alternatively, the square root of x” can be computed to yield Z, the |
sign test can be performed. If 12 of 15 results are consistent in either direc. _. standard normal deviate. Although clear-cut results on the issue are not avail- |
/ tion, the sign test tells us that results so rare “occur by chance” only 3. 6% E- able, it appears likely that the counting methods are not as powerful as other - |
the time. This procedure, and the closely related one that follows, have been _ methods described here. |
employed by Hall (1979, 1984). oe ,
FF The x? statistic may also be useful in comparing the number of studies _ 1B.2. Bloceing. the vel ethod, adapted rom he procedure elven by
| | expected to reach a given level of significance under the null hypothesis ——__ Snedecor and Cochran (1967; see also Cochran & Cox, 1957) requires that |
| | with the number actually reaching that level (Rosenthal, 1969, 1976; Ro. | We Teconstruct the means, sample sizes, and mean square within conditions |
a senthal & Rosnow, 1975: Rosenthal & Rubin, 1978a). In this applicatieg | for each of our studies. We then combine the data into an overall analysis of
ok there are two cells in our table of counts, one for the number reaching some 7 variance in which treatment condition is the main effect of primary interest
| | critical level of p, the other for the number not reaching that critical level of 7 and m which studies are regarded as a blocking variable. if required because i
oo p. When there are 100 or more studies available, we can set our critical p of differences among the studies in their means and variances, the depen-
| - level at .05. Our expected frequency for that cell is .OSN while our expected sent variables of the studies can be put onto a common scale (¢.g., zero |
| | frequency for the other cell is .95N.For example, suppose that 12 of 120 | 72" and unit variance). ; ;
| - : studies show results at p < .05 in the same direction. Then our expected When studies are assumed to be a fixed factor, as they sometimes are
| frequencies for the two cells are .05(120) and .95(120) respectively, a (Cochran & Cox, 195 ?); or when the MS for treatments x studies is small
ro shown in Table 5.2. 7 relative to the MS within, the treatment effect is tested against the pooled !
| It is not necessary to set our critical value of p at .05. We could as well use | _ MS within (Cochran & Cox, 1957). When the studies are regarded as a |
i 10 or .01. However, it is advisable to keep the expected frequency of ou | random factor and when the MS for treatments X studies is substantial rela- |
oo smaller cell at 5 or above. Therefore, we would not use a critical value of OL | - live to the MS within (say, F > 2), the treatments x studies effect is the
| unless we had at least 500 studies altogether. To keep our smaller expected | — “PPropriate error term for the treatment effect. Regardless of whether stud- i
| | frequency at 5 or more we would use a critical level of .10 if we had 50 studies, _ - les are viewed as fixed or random factors, the main effect of studies and the |
. acritical level of .20 if we had 25 studies, and so on. More generally, when | _ [traction of treatments x studies are tested against the MS within.
_. there are fewer than 100 studies but more than 9, we enter in one cell a Substantial main effects of studies may or may not be of much interest,
_ expected frequency of 5 and in the other an expected frequency of N—5. The | but substantial treatments x studies interaction effects will usually be of i
| observed frequency for the first cell, then, is the number of studies reachinga | _ considerable interest. It will be instructive to study the residuals defining |

----- Page 55 (ocr) -----
96 META-ANALYTIC PROCEDUREg | COMBINING PROBABILITIES 97 ina
TABLE 5.3 | —_ Total SS = S(M — MY? = (0.48~1.24)? + (2.00-1.24% +... + |
_ The Blocking Method of Combining Probabilities 7. (0.36~ 1.24)? + (2.12—1.24? = 9.4432 {5.10} i
_ Applied to the Studies of Table 5.1 _ _ |
_ _ Row (Studies) SS = L[c(Mp ~ MY] = 2(1.24-1.24% +....4
Co Control Experimental . 2(1.24—1.24P = 0 {5.11] |
_ Study Mean (n) Mean (n) Mean error  . Column (Treatments) SS = 2ir(Mc — My} = 5(.592-1.24 +
1 0.48 (21) 2.00 (21) 1241 5(1.888~ 1.24% = 4.1990 [5.12]
_ 2.48 Gl 1.24 16690
4 0.12 (16) 2.36 (16) 1.24 17.39 _ = 9.4432 — 0 — 4.1990 = 5.2442 [5.13]
_ 5 0.36 = (11) 212 (11) 1.24 1174 . ; ;
_ | _When divided by their appropriate df the Studies, Treatments, and Studies a
Mean 592 _ by Treatments SS’s yield MS’s of 0, 4.1990, and 1.3110, respectively.
i 5 Analysis of Variance: Unweigined Means Ms F | ___ The error term is obtained by dividing the MS error from the original
a Ource 7 : : tae : /
| SA | __ one-way analysis of variance of the 10 conditions (17.64) by the harmonic ‘
Treatments 4.1990 l . 1990 2.98% _|___ mean of the sample sizes, nj. In this case nj, = 12.5016 so our error term is
Studies 0. 4 a _ _
Treatments x studies 5.2442 4 1.3110 _ 7. 164
: Error 160 1.4110> _ Trs0ie = 1.4110 [ |
a. In the example constructed here and, more generally, in cases wherein the data from each study . . | | |
_ are standardized with zero mean and unit variance, the mean square for studies is alwayszerqie, ag Shown in Table 5.3. |
: since the means of all studies have been set equal to each other (to zero) the between studies Ss /
_. and MS must equalzero. _____LC. Choosing a Method ° |
_ Brome Op *Odo one: ma thetext Table 5.4 shows the advantages, limitations, and indications for use of
co ) | __ eachofthe nine methods of combining probabilities. Various methods have |
|. . . __| special advantages under special circumstances. Suppose we were con- F
| the interaction closely for clues as to the nature of the possible moderating ——_ d by 200 . : : ° . nan |
. _ studies, each d wit inform,
| | variables affecting the operation of the treatment effect. Analysis of the a not neach a aies al hele vide A mh only veto *) sivec a ueny |
| residuals might show, for example, that it is the better (or more poorly) | k if 6 1 P ; f ha f x iD ‘i y ;
designed studies that show greater predicted effects. The blocking method | {uick test, Ht not a very elegant or powerful estimate of overall probability. -
 . ; eo Wi many studies to babl i i pl
_ | is sometimes not applicable because authors have not reported sufficient |_| Neth So many tudies to process we would p robab y decide against the
 F data in their papers ____ blocking method on the grounds that the work required would not be justi- |
oF ata In Ener . __|___ fied by any special benefits. We would not be able to a he basi d
| Table 5.3 illustrates this last method as applied to the set of five studies = —_ Hie if ySP bilities £ We wou : otb re fo ap ply t c metho |
_ . ichted . | of adding probabilities for reasons given earlier. Most other methods are i
we have been using as our example (see Table 5.1). An unweighte means | applicable however. L |
a analysis of variance was computed and the results fell within the rangeof | ‘ a . . i
_ results obtained by our earlier described methods. The only real disadvan. If we were combining only a very few studies, we might favor the Pout
_ : is that i invol. iderably more work th an | Method of adding probabilities. We would avoid both the method of testing
. tage of this approach is that it may involve considerably ; ____ the mean Z and the counting methods, which do better on larger numbers of
a some of the other methods. This will be especially true when the number chUr dies mol
| i just a few to dozens, scores, or hundreds. The computa, | ___ OO . ; |
| studies grows trom ‘hte 4 heans analysis of variance are shown next. P | ___ There is no best method under all conditions (Birnbaum, 1954), but the co
Co 10 8 a one that seems most serviceable under the largest range of conditions is that j |
| LB.2.a. Computations: unweighted means. The details of the unweighted — _of adding Z’s, with or without weighting. When the number of studies is hae
. means analysis of variance are given elsewhere (e.g., Rosenthal & Rosnow, = small, it can be suggested that at least two other procedures also be em- le i
: . 1984a, chapter 20; 1991, chapter 16). Basically, we perform our compute | ployed and the overall p’s emerging from all three be reported. When the | |
: . tions on the means of the control and experimental conditions of the five | number of studies is large, a useful combination would seem to be the me }
studies. _ wy

----- Page 56 (ocr) -----
_ 98 META-ANALYTIC PROCEDUREg | coMBINING PROBABILITIES 99 |
TABLE 5.4 | method of adding Z’s combined with one or more of the counting methods Ll
_ Advantages and Limitations of Nine Methods | 4sacheck. Practical experience with the various methods suggests that a
of Combining Probabilities | omere is only rarely a serious discrepancy among appropriately chosen
. Wah tn hehe | methods. It goes without saying, of course, that any overall p that has been
| rr computed (or its associated test statistic with df) should be reported and not 7
oo 1. Adding Well-established Cumulates poorly; N of studies small | : : : : : ; i.
| Logs historically can support opposite (< 5) | __ suppressed for bein g higher or lower than the investi gator might like.
_ conclusions. | __ To make possible the computations described in this chapter, authors
_ | should routinely report the exact t, F, Z, or other test statistic along with its
_ 2. Adding p’s Good power celery) ee small(Zp | _gfor N, rather than simply making such vague statements as “t was signifi-
: - large unless complex | cant at p< 05.”
_ corrections are intro- | ___ Reporting the test statistic along with an approximate p level also seems ia
duced. | _ preferable to reporting the “exact” p level for three reasons: (1) the exact p
3. Adding t’s Unaffected by N of Inapplicable when t’s Studies not based on | level may be difficult to determine without a computer ore calculator that |
| studies given mini- based on very few df. too few df ____ stores such distributions as Z, t, F, and x’, (2) ambiguity about one-tail versus /
_ mum df per study | _ two-tail usage is avoided, and (3) the test statistic allows us to compute exact i
_ 4. Addin Routinely applies Assumes unit vat Anvtime | pas well as the effect size. Speaking of effect size, we encourage editors to I
Os ing ble: simple PP ance when under ¥ 1 routinely require the report of an effect size (e.g., 1, g, A, or d) for every test Fe
_ some conditions Type _|__ statistic reported. /
Lor Type I errors | _ Finally, it should be noted that even if we have established a low com- :
may be increased. 7 bined p, we have said absolutely nothing about the typical size of the effect
: 5. Adding Routinely applica- Assumes unit vari- Whenever weighting . - the “existence” of which we have been examining. We owe it to our readers eo
_ Weighted ble, permits ance when under desired | togive for each combined p estimate an estimate of the probable size of the i"
| Z's weighting some conditions Type _| effect in terms of acorrelation coefficient, a o unit, or some other estimate.
Tor Type II errors | This estimated effect size should be accompanied, when possible, by a con- A
| | may be increased. | __ fidence interval. a
: . : 6. Testing Simple N of studies should N of studies = 4 _ _ LD. On Not Combining Raw Data |
| Mean p not be less than four. | ___ Sometimes it happens that the raw data of two or more studies are availa- Ba |
: : 7. Testing No assumption of Low power when N Nofstudies = 5 a - ble. We have seen how these data could be appropriately combined in the | |
Mean Z. unit variance of studies small. 7 method of blocking. There may, however, be a temptation to combine the 7
| | __ raw data without first blocking or subdividing the data on the basis of the an
| 8. Counting Simple and robust large N of wees N of studies large | _ studies producing the data. The purpose of this section is to help avoid that a
: : in power. ero _ ___ temptation by showing the very misleading or even paradoxical results that 1
| ___ can occur when raw data are pooled without blocking. oa
9. Blocking Displays all means Laborious when N N of studies nottoo | Table 5.5 shows the results of four studies in which the correlation be- i |
_ for inspection, thus —_ large; insufficient large _| tween variables X and Y is shown for two subjects. The number of subjects a
oe facilitating search data may be available | . ; i
for moderators to employ this per study makes no difference and the small number (n = 2) is employed here a |
7 (variables altering procedure, - | _ only to keep the example simple. For each of the four studies the correlation aks /
a the relationship be- _| (between X and Y is —1.00. However, no matter how we combine the raw ad
 F tween independent _| _ data of these four studies, the correlation is never negative again. Indeed, the ion
: mabe. _ ___ Tange of 1’s is from zero (as when we pool the data between any two adjacent | ial
. arn E_ Studies) to .80 (as when we pool the data from studies | and 4). The remainder . 1
_| of Table 5.5 shows the six different correlations that are possible (.00, .45, ray
a 60, .67, .72, .80) as a function of which studies are pooled. I! | |

----- Page 57 (ocr) -----
i 100 META-ANALYTIC PROCEDUREy | cgMBINING PROBABILITIES 101 |
_ TABLE 5.5 _ TABLE 5.6 |
_ Effects of Pooling Raw Data: Four Studies 7 Effects of Pooling Tables of Counts Be
| i” I y Pai 0 Pad - ous 2 Study I Study 2 Pooled a
co ea Alive Dead Alive Dead Alive Dead
_. ubyec’ 65 | eo
Subject 2 0 2 2 4 4 6 6 8 | __ Example! b
: : 1.00 ~1.00 1.00 1.00 | __Control 10100 1000 100 1010 200
oe Correlations obtained Three or _ zy 110 1100 1100 =110 1210 1210 ae
when pooling: Two Studies Four Studies. | _ 1=0 -=0 r= 67 :
r= 00 60 80 AS OT |
a Pooled studies: 1+2 143 144 1,2,3 1,2,3,4 152,4 Example i
ara an 234 134 | treatment 50 100 500 100 100
uc —w“e i Kontrol 0 650 100 50 100 100 a
| »: 50 150 150 50 200 200 Aa
How can these anomalous results be explained? Examination: of the 7. 3 ; 43 9 a
_ means of the X and Y variables for the four studies of Table 5.5 helps yy §_—|—§_ _—___=__-=A AA ae
. understand. The means of the X and Y variables differ substantially from _ a
study to study and are substantially positively correlated. Thus, in study] | —__ . . Ll
. the X and Y scores (although perfectly negatively correlated) are all quite | —-" Example I of Table 5.6 we see two studies showing zero correlation i
_ low relative to the X and Y scores of study 4 which are all quite high (al- ____ between the treatment condition and the outcome. When the raw data of these Al
_ though also perfectly negatively correlated). Thus, across these studies inf TO fe te pooled, however ae fine cramatic © orvelation “ . 7 . ue Lo
which the variation is substantial, we have an overall positive correlation | _opestingt at the treatment was harmiul, Note that in Stu YS omy 220 OF Pa |
| between variables X and Y. Within these studies, where the correlationsare | _ tents survived, while 91 % received the treatment, whereas in Study 2, 91 %of a
oa negative (— 1.00) the variation in scores is relatively small, small enough to _ __ patients survived but only 9% received the treatment. It is these inequalities 7)
|. be swamped by the variation between studies , _____ ofrow and column totals that lead to Yule’s (or Simpson’s) Paradoxes. |
. Although there may be times when it is useful to array the data from | Example Il of Table 5.6 shows two studies each obtaining a strong effect |
: multiple studies in order to see an overall pattern of results, or to see what | ome , he ner th nt condition ~ 33) ne these two aes sa
| might happen if we planned a single study with variation equivalent to that | __ pooled, however, t ese strong ettects vanished. ofe that in study only Md
TL shown by a set of pooled studies, Table 5.5 serves as serious warning of how a 25% of patients survived while 75% received the treatment, whereas in |
Gg a “ce ay Study 2, 75 %of patients survived but only 25 % received the treatment. Had re
a pooled raw data can lead to conclusions (though not necessarily ‘“‘wrong)) | _ : a
_. opposite to those obtained from individual, less variable studies. _ | ___ tow and column totals been equal, the paradoxes of pooling would not have cy
. , _|__ occurred. fa
: _ LD.1. Yule’s or Simpson's Paradox. Nearly a century ago G. Udny Yule _ - The moral of the pooling paradoxes is clear. Except for the exploratory ; |
: (1903) described a related problem in dealing with 2 x 2 tables of counts. He | ___Tiposes mentioned earlier, raw data should not be pooled without block- i LT
| showed how two studies in which no relationship (r= .00) was found between | "B- In otk ach ofieet “. and Significance es should be computed A i |
_ the variables defined by the two rows and the two columns, could yield a y for each study and only then combined. [) |
_ positive correlation (r = .19) when the raw data were pooled. Similarly, | _ does
. Simpson (1951) showed how two studies with modest positive correlations 7 : ie
_ (r’s = .03 and .04) could yield a zero correlation when the raw data were | Il. SPECIAL ISSUES | ( |
| pooled. Table 5.6 illustrates the problem described by Yule (1903), yo Earlier in this chapter we saw that the method of adding Z’s was perhaps tay
- Simpson (1951) and by others (¢.g., Birch, 1963; Blyth, 1972; Fienberg, | the most generally serviceable method for combining probabilities. In the
Fo _ 1977; Glass et al., 1981; and Upton, 1978). ____ following section we provide procedures facilitating the use of this method. i | I

----- Page 58 (ocr) -----
T 102 META-ANALYTIC PROCEDUREs | _ COMBINING PROBABILITIES 103 il
: | ILA. Obtaining the Value of Z _ A very useful and conservative approximation to this formula is also availa- |
_. The method of adding Z’s requires that we begin by converting the oh. _ ble (Rosenthal & Rubin, 1979a): a
i tained one-tailed p level of each study to its equivalent Z. The value ofZjg |
_ zero when the one-tailed p is .50, positive as p decreases from p = .50 to p 7 2 i
: close to zero, and negative as p increases from .50 to p close to unity. Thus - Z=«l - ——) [5.15] | |
_ one-tailed p of .01 has an associated Z of 2.33, while a one-tailed pof 99 | __ Adf ip
_ has an associated Z of —2.33. These values can be located in the tableog | . ; ; ie
_ probabilities associated with observed values of Z in the normal distriby. | _This approximation works best when t’ < df, when t° = df, this approxima-
tion found in most textbooks on statistics. . tion tends to be 10 % smaller than the Z obtained from equation 5.14. |
Unfortunately for the meta-analyst, few studies report the Z associateg | If the test statistic employed was F (from analysis of variance) and df for my
with their obtained p. Worse still, the obtained p’s are often given impre. _ the numerator was unity, we take the VF as t and proceed as we did in the Co
Z cisely as < .05 or < .01, so that p might be .001 or .0001 or .00001. If pis aj | __scase of t with df equal to the df of the denominator of the F ratio. We should
: that is given in a study, all we can do is use a table of the normal distribution - _ note that F ratios of df > 1 in the numerator cannot be used in combining p
to find the Z associated with a reported p. Thus, one-tailed p’s of .05, 91, | _ [evelsto address a directional hy pothesis. ; ;
and .001 are found to have associated Z’s of 1.65, 2.33, and 3.09, respec. | ___ Ib the test statistic employed was x* (for independence in contingency Ll
|. tively. (If a result is simply called “nonsignificant,” and if no further infor, | tables) w ith df= 1, wetake Vx’ directly, since X (1) = 22. We should note i
i mation is available, we have little choice but to treat the result asa pof 50, | __ that x"’s of df > 1 cannot be used in combining p levels to address a direc-
Z = 0.00.) | _tionalhypothesis. rT
. Since p’s reported in research papers tend to be imprecisely reported we a When VF or Vv x? is employed we must be sure that Z is given the appro- : i
can do a better job of combining p’s by going back to the original test statistics | _priate sign to indicate the direction of the effect. ui
employed, e.g., t, F, or x”, Fortunately, many journals require that these statis. | ; ; ; ; ie
os tics, along with their df, be reported routinely. The df fort and forthe denomi- | __ mon ; ea sat wa estimates. Sometimes ba yaa find Z for a study in ie
| nator of the F test in analysis of variance tell us about the size of the study. The | — Whichno test statistic is given (e.g. t, F, x"), but an effect size estimator such ||
| df for x? is analogous to the df for the numerator of the F test in analysisof | 8" (including point biserial r and phi), g, A, ord is given along with a rough |
_ variance and so telis us about the number of conditions, not the number of 7 a" “ indicator such as P . a In tho ‘best the e ae often a TN eo that i /
- sampling units. Fortunately, the 1983 edition of the Publication Manual of the 7 : “one eer and v/Nd ° hi y Vein a at (phi)’ = x°(1)/N so that a
| American Psychological Association has added a requirement that when re- | __ N(phi)’ = x*(1) an (p i) = Vx ( y= eo _ bi
| porting x? test statistics the total N be given along with the df. | __Inthe case ofr or point biserial r, multiplying by VN will yield a gener- aa
a _|__ally conservative approximation to Z. A more accurate value can be ob- ce
. | ILA.1. Test statistics. If at test was employed, we can use at table to find 7. tained by solving for t in the equation: | i
| the Z associated with the obtained t. Suppose t(20) = 2.09 so that p= .025, r : He
i one-tailed. We enter the t table at the row fordf = 20 and read acrosstothet | _ (= aap OX VF oon t= Tex VN-2 [2.3] Hh
_ value of 2.09. Then we read down the column to the entry for df = %, which i
- is the Z identical to the value of t with © df (1.96). Suppose, however, that : _ if
_ | our t was 12.00 with 144 ° ve oxen “ ae a ton. rep ee o and then employing t to estimate Z as shown in equations 5.14 0r 5.15. i i
a values of t are that large for substantia ( cece rz f ‘ t ost ‘table _| ___ Wewill not review here how to get t from the other effect size estimators | | |
Rosnow, 1984a, 1991), A very accurate estimation of @ from tis ava _| __ but that information is found in equations 2.3 - 2.13 (Tables 2.1 and 2.2), ae
_ for such circumstances (Wallace, 1959): | 419, and 4.20. i
a a i oo ILB. The File Drawer Problem ; 4 | | ;
cL _ vA df log, (1 + a ») fl- Car [5.14] Co Statisticians and behavioral researchers have long suspected that the a i |
a= | __ Studies published in the behavioral and social sciences are a biased sample i it | |
- _____ of the studies that are actually carried out (Bakan, 1967; McNemar, 1960; il i

----- Page 59 (ocr) -----
oo 104 META-ANALYTIC PROCEDURE: | _coMBINING PROBABILITIES 105 |
| Smart, 1964; Sterling, 1959). The extreme view of this problem, the file lk overall p to 50 (not to .05), s is the number of summarized studies |
_ | drawer problem, is that the journals are filled with the 5% of the studies tha | gipnificant at p < .05 and n is the number of summarized studies not signifi- |
| show type J errors, while the file drawers back at the lab are filled with the at .05, then ee
; _ 95% of the studies that show nonsignificant (e.g., p > .05) results (Rosen. 7 L
thal, 1979a; Rosenthal & Rubin, 1988). _ *= sn 5.191 HT
_ t a the a there hs file ch little ‘h con do to ke dhe the ne t effect of | tere 19 is the ratio of the total number of nonsignificant (at p > .05)
_ stuanes tuexee away tae erawers that did not make the magic .05 level 1 sults to the number of significant (at p < .05) results expected when the |
(Rosenthal & Gaito, 1963, 1964; Nelson, Rosenthal, & Rosnow, 1986). Now. ss iL hypothesis is true Eo
however, although no definitive solution to the problem s available, we can | ____Another conservative alternative when exact p levels are not available is Ll
establish reasonable boundaries on the problem and estimate the degree of set Z = .00 for any nonsignificant result and to set Z = 1.645 for any ho
damage to any research conclusion that could be done by the file drawer 7. ult significant at p < .05 i
‘dea i : i : - | _ a
problem. The fundamental idea in coping with the file drawer problem ig ____ The equations above all assume that each of the K studies is independent lee
_ simply to calculate the number of studies averaging null results (Z = 0.00) | ot all other K — 1 studies, at least in the sense of employing different sam- | |
that must be in the file drawers before the overall probability of a type I etror | pling units. There are other senses of independence, however; for example, te
can be just brought to any desired level of significance, say p = 05. This i iw“ can think of two or more studies conducted in a given laboratory as less i i
_ number of filed studies, or the tolerance for future null results, is then | independent than two or more studies conducted in different laboratories. | |
evaluated for whether such a tolerance level is small enough to threaten the ach nonindependence can be assessed by such procedures as intraclass a
overall conclusion drawn by the reviewer. If the overall level of significance | correlations. Whether nonindependence of this type serves to increase type He
_ of the Tesea rch review will be brought down to the level of Just signifi ag | lor type Il errors appears to depend in part on the relative magnitude of the | i
by the addition of just a few more null results, the finding is not resistant to | Ps obtained from the studies that-are “correlated” or “too similar.” If the i
i the file drawer threat. (For a more technical discussion ofthe underpinnings correlated Z's are on the average as high as or higher than the grand mean Z ! i
of the following computations see Rosenthal & Rubin, 1988). | corrected for nonindependence, the combined Z we compute treating all ae
| IL.B.1. Computation. To find the number (X) of new, filed, or unretrieved 7 E cadies as independent will be too large, leading to an increase mn type I i
_ : : . . , | errors. If the correlated Z’s are on the average clearly low relative to the i
_ studies averaging null results required to bring the new overall pto any d z df ind di h bined Z ne
. desired level, say, just significant at p = .05 (Z = 1.645), one simply writes, | _ and mean Z corrected for nonindependence, the combines 4 we compute LL
. —— , "treating all studies as independent will tend to be too small, leading to an Bh
| 1645 = AEE [5.16] | increase in type IT errors. i i
| . _ | ILB.2. Illustration. In 1969, 94 experiments examining the effects of in- i i
_ where K is the number of studies combined and Z is the mean Z obtained for . - _terpersonal self-fulfilling prophecies were summarized (Rosenthal, 1969). ! i
oo. the K studies. _| The mean Z of these studies was 1.014, K was 94, and Z for the studies lil
|. Rearrangement shows that | combined was: | |
K[KZ? — 2.706] | 5 ET
| An alternative formula that may be more convenient when the sumofthe | ae
: Z’s (ZZ) is given rather than the mean Z, is as follows: a | ___ How many new, filed, or unretrieved studies (X) would be required to a
| <7 _| _ bring this very large Z down to a barely significant level (Z = 1.645)? From i /
, X = eee _-K [5.18] equation 5.17 of the preceding section: ie |
One method based on counting rather than adding Z’s may be easierto | __ Xx = K[KZ? - 2.706] _ 94[94(1.014)? — 2.706] _ 3.263 a |
_ compute and can be employed when exact p levels are not available, but itis | 2.706 2.706 , a
a al probably less powerful. If X is the number of new studies required to bring One finds that 3,263 studies averaging null results (Z = .00) must be i |
_ crammed into file drawers before one would conclude that the overall a |

----- Page 60 (ocr) -----
ae 106 META-ANALYTIC PROCEDURE | _coMBINING PROBABILITIES 107 Uf
So results were due to sampling bias in the studies summarized by the fae not and in which about half are significant and half are not. (The six Ee
viewer. In a more recent summary of the same area of research (Rosentha, | gudies on which this analysis is based are Atkinson, Furlong, & Wampold,
. & Rubin, 1978) the mean Z of 345 studies was 1.22, K was 345, and X was _ 9g: Blackmore, 1980; Chan, Sacks, & Chalmers, 1982; Coursol & Wagner, a
_ 65,123. Ina still more recent summary of the same area of research, the meay = 1086: simes, 1987; Sommer, 1987.) i
_ Z was 1.30, K was 443, and X was 122,778. Thus over 120,000 unteporieg f ‘
_ studies averaging a null result would have to exist somewhere before th, | —_,8.4.a. Estimating retrieval bias. Several studies have been conducted Le
_ overall results could reasonably be ascribed to sampling bias. _ | try to estimate the number of studies that might be languishing in the | i
. a ; | iat drawers. Shadish, Doherty, and Montgomery (1989) took a simple ran- fe
IL.B.3. Guidelines for a tolerance level. At the present time, no firm | gom sample of 519 possible investigators from a population of 14,002 |
| | guidelines can be given as to what constitutes an unlikely number of unre. | _jyarital/family therapy professionals. Responses concerning their research |
trieved and/or unpublished studies. For some areas of research 100 or even | were obtained from 375 (72%) of the sample, and this yielded only 3 studies Ee
500 unpublished and unretrieved studies may be a plausible state of affairs | = 34) that could have been included in a meta-analysis. Shadish, Doherty, val
while for others even 10 or 20 seems unlikely. Probably any rough and ready | and Montgomery concluded tentatively that the file drawers might contain i 1
guide should be based partly on K so that as more studies are known it | pout 112 (14,002 x 3/375) studies, which is not quite as many as they had he
becomes more plausible that other studies in that area may be in those file | retrieved for their ongoing meta-analysis (about 165). ho
drawers. Perhaps we could regard as robust to the file drawer problem any | Ina survey of all members of a population of researchers, Sommer (1987) ca
combined results for which the tolerance level (X) reaches 5 K + 10, That} wrote to all 140 members of the Society for Menstrual Cycle Research. Based | |
seems a conservative but reasonable tolerance level; the 5 K portion sug. | on response rate of 65%, Sommer found little publication bias. Of the 73 We
: gests that it is unlikely that the file drawers have more than five timesa | published studies, 30% were significant in the predicted direction; of the 42 i
_ many studies as the reviewer, and the +10 sets the minimum number of | studies in the publication pipeline, 38% were significant in the predicted il
ss studies that could be filed away at 15 (when K = 1). ; | direction; and of the the 28 studies securely filed away, 29% were significant. a
_ It appears that more and more reviewers of research literatures willbe | when only those studies were considered for which significance testing data a
| estimating average effect sizes and combined p’s of the studies they summa | were available, the corresponding percentages were 61%, 76%, and 40%. An | |
. rize. It would be very helpful to readers if for each combined p they Pre: | interesting sidelight of Sommer’s study was that far and away the best Ay
| sented, reviewers also gave the tolerance for future null results associated | predictor of publication status of the article was the productivity of the author. UL
. | with their overall significance level. _ Employing a different approach, Rosenthal and Rubin (1988) compared | 1
oT | the meta-analytic results for a research domain for the case of complete if i
_ I.B.4. Empirical estimates of the magnitude of the file drawer p roblem | retrieval with the results for that same research domain for the more typical i
_ In chapter 3 section I.A.2 we examined the differences in effect sizes obtained | dace of incomplete retrieval. ah
from such information sources as journal articles, books, theses, and unpub- | i |
/ | lished materials. There we saw that there was no clear difference in typ ia IL.B.4.b. Complete versus incomplete retrieval. For an earlier meta- i |
effect size obtained in studies that were published in journals versus studies | analysis of 103 studies of interpersonal expectancy effects, studies could be ) ll
| that were not yet published. In this section our emphasis will be on signif: | divided into one group in which all could be retrieved because they were all a
 F cance testing rather than on effect size estimation, and we will try to getsomé | conducted in a single laboratory (Rosenthal, 1969) and a second group of / i
reasonable estimates of the magnitude of the file drawer problem. : | tetrieved studies conducted elsewhere. Table 5.7 shows the mean Z obtained | f
. To begin with, there seems to be little doubt that the statistical significance | for each of these two sets of studies subdivided by whether the study (a) had l L
| of a result is positively associated with its being published. In a series of SK | _ been published at the time of the original (1969) meta-analysis, (b) had been a
studies, for example, the range of these correlations was from .20 to .42.will | unpublished at the time of the meta-analysis but was published by the time / \
_ a median r of .33. This result is roughly equivalent to two thirds of significant | of the present analysis (1990), or (c) had been unpublished at the time of the "| i
| results being published while only one third of nonsignificant results ae | meta-analysis and remained unpublished in 1990. a
a published in a population of studies in which about half are published and | if |

----- Page 61 (ocr) -----
a 108 META-ANALYTIC PROCEDUREy | _ COMBINING PROBABILITIES 109 |
_ | __ Information relevant to the design of a meta-analytic study is provided by ae
TABLE 5.7 ae | the fact that the publication delays for the originally unpublished studies a
Mean Z’s in Two Conditions of Retrievability | sanged up to 13 years with a mode of 1 year and a median of about 3 years; ba
—_-o———" FF hlicationSiatus after 5 years, 33 of the 35 originally unpublished studies (94%) had been Le |
oo Published Never | __ published. I. L |
Retrieval Published Later Published Mean oo TABLE 5.8 i
Complete 1,087" -0.16" 0.6078 0670 Mean Z’s: Immediate Meta-Analysis |
| Incomplete 2,601 1.05% 1,3920 1464 ul |
_ Mean 1.5899 0.6775 1.0238 1.06193 Retrieval Published Unpublished Difference ie | !
| a. Number of studies on which Z is based. | __ Complete 1.0820 0.3129 0.77 |
_ | incomplete 2.6010 1.20*4 1.40 i [ |
Analysis of variance of the 103 studies’ Z’s castinto the 2x 3 tableshoweq | __ Mean BBO I |
the interaction (F(2, 97) = 0.49, p = .614) to be sufficiently small that the | |
_ following contrasts tell the story. The comparison of retrievability yielded | Ha
t(97) = 2.92, p = 0022, r = .17; the comparison of studies ever publisheg | __ TABLE 5.9 aE
with those never published yielded t(97) = 0.24, r = .05; the comparison fo Mean Z’s: Delayed Meta-Analysis /
a studies published at the time of the meta-analysis and those published latey | __ a I
yielded t(97) = 2.52, p = .0077, r = .16; and the comparison of Studies | _ Retrieval Published Unpublished Difference a
published versus not published at the time of the meta-analysis yielded t(97) | Complete 0.643! 0.60!8 0.04 ne
_ = 2.32, p= .011,r=.15. | __ Incomplete 1.50" z 1.39" 0.11 ia
_ Thus, the completely retrieved data meta-analysis yielded less significan | — __Mean 1.09 1.02 0.07 i
results on average than did the incompletely retrieved data meta-analysis, | __ i |
_ That result fits our suspicions that completely retrieved data show less ii
. significant results than do less completely retrieved data. However, inthis | _ EXERCISES AH
| table, retrievability is fully confounded with production by a particular |. 1. Foreach of the six studies summarized in the exercises of Chapter 4 compute t, ee
- . : : , we |__| df, the one-tail p, the Z associated with each of these p’s, and the quantity -2 log, p. a
the remaining labora. | ° e P a
_ labo ratory, which might et " he ee ways from uestion 8 i _____ 2. Combine the probabilities of the six studies using the methods of (a) adding a
a tories producing results bearing on the same researc q : _1_, | __ logs, (b) adding p’s, (c) adding t's, (d) adding Z’s, (e) adding weighted Z’s, (f) testing a
The publication status effects are more surprising. As expected, publisiel = mean p, and (g) testing mean Z. i
results were more significant than initially unpublished results. However, of | _ 3. Assume, forthe moment, that the seven combined p's computed for question 2 i
the initially unpublished studies, those published eventually were less signif- are independent. Perform a test of the heterogeneity of the seven obtained p levels Hi
| icant than were those never published. The impact of publication: status, _. and interpret the resulting statistic and its associated p level. li :
therefore, depends on whether we group the later published with the unpub- | 4. For the 50 studies you were able to retrieve for a meta-analysis, the mean | i
. lished (as would be done at the time of the original meta-analysis) or with | Standard normal deviate (Z) was 715 (associated with a one-tailed p of about .23). T |
the published studies (as would be done if we gave unpublished studies more | How many unretrieved studies averaging null results (Z = 0.00) must there be in a
ears to become published) _ the file drawers before the overall result would be brought to the brink of nonsig- He
y P : | __ nificance at p = .05? Ht
II.B.4.c. Immediate versus delayed meta-analysis. Table 5.8 shows that | —S Imagine that we had the raw data available for all six studies of the table given Hh L
| immediate meta-analysis led to substantial publication status bias with (97) | im the exercises of Chapter 4. Explain in prose and demonstrate by numerical exam- i | |
= 2,29, p = 012, r = .15. However, a meta-analysis delayed to allow for |” ple-how it might happen that if we pooled the raw data of all six studies we might Wl ce
_. eventual publication yielded essentially no publication status bias with @ = results opposite in direction to those we found in the exercises of Chapters 4 |
. = 0.24, p = .405, r= .05 (see Table 5.9). 7 a

----- Page 62 (ocr) -----
a 6 | usTRATIONS OF META-ANALYTIC PROCEDURES re a
a. Illustrations of. TABLE 6.1 :
_ : | _ Stem-and-Leaf Plot and Statistical Summary of Correlations |
|. Meta-A nalyt ic Procedur, eS | Between Encoding and Decoding Skill L
_ Correlations (r's) Summary Statistics
_ Based on the results of actual meta-analyses, illustrations are provided of a variety ofmet,, —-—=—_=St€M! Leaf (Based on r, not on zy) ——— a
_ analytic procedures. Examples are drawn from research on nonverbal communication | 6 35 Maximum 65 |
_ | skills, the validity of the PONS test, the detection of deception, the effects of interpersonal _ 5 5 Quartile 3 (Q3) 29 | | :
8 expectancies, the effects of psychotherapy, sex differences in cognitive performance, aa 4 Median (Q)) 16 - |
hit rates in ganzfeld studies for which a one-sample effect size index, 2, is especially 2 2 Quartile 1 (Q,) .00 fe
appropriate. | GC 00189 Minimum ~.80 a
| 9 | 0004559 6{.75(Q3 — Qy)] 22 Val
In earlier chapters when various meta-analytic procedures were describe 0 8 33 i
_ they were often illustrated with hypothetical examples in order to keepthe | | ee 9 ie
_ | computational examples small and manageable. In the present chapter, we | 6 Proportion positive sign? .88 i
_ will illustrate various meta-analytic procedures with real-life meta-analytic | __ 4 a
examples. In principle it would be possible to illustrate almost every meta. =| -5 , ul
analytic procedure described in this book for every meta-analysis we will be | & a
examining. For purposes of exposition, however, we will employ each meta. | 2 0 a
a lyti le to illustrate Il numb inci | oe uy
| analytic example to illus a small number of principles and procedures, |g ofthose having signs. ; [|
_ I. DISPLAYING AND COMBINING EFFECT SIZES ae a Cee We
| distribution is normal and, therefore, is similar to S when the distribution is ap |
DePaulo and Rosenthal (1979) conducted a meta-analysis of studies of _ | ermal. In the data of Table 6.1, Sis substantially larger than .75 (Q3 ~ Q)) A
| the relationship between skill at decoding nonverbal cues and skill at encod. ___ Suggesting that these effect sizes may not be normally distributed. {fa more lH i
2 ing nonverbal cues. Table 6.1 is an updated summary of the results of ig | formal test of normality is required, the Kolmogorov-Smirnov test can be a
/ _ studies. The stem-and-leaf display and its statistical summary were encoun | employed (Lilliefors, 1967; Rosenthal, 1968). i fi
. tered earlier (Tables 3.8 and 3.9) as useful ways of displaying the resultsofa. | stem-and-leaf plot and its statistical summary can, of course, be made ae
 . meta-analysis. It is more informative for the eye to follow the stem-and-leaf | fF any type of effect size estimator. For example, Rosenthal and Rosnow nH |
: display than simply to be told that the median r was .16 or that the mean | (0, p. 23) compared the rates of volunteering for behavioral research in al
_ was 13 || general of females and males. Their stem-and-leaf display was of 51 such He
_ The first five entries of the summary statistics require no explanation. | _ fllfferences in volunteering rates, i.e., the effect size estimator a’. In that i
The quantity Q3 ~ Qy gives the range for the middle 50% of the effet |  amalysis the median qd’ was mel with females volunteering more than males Uo
. sizes. The quantity .75 (Q3 ~ Q)) estimates o quite accurately when the | onthe average. This direction of difference was found in 84% of the studies aL
_. . __| of volunteering for behavioral research in general. i
. 110 IL COMBINING EFFECT SIZES I
/ - 7 As part of the construct validation of the PONS test, an instrument de- i
: |. | Signed to measure sensitivity to nonverbal cues, Rosenthal, Hall, DiMatteo, ih
| Rogers, and Archer (1979) conducted or located 22 studies in which the Me |
 . __| PONS test total score was correlated with judges’ ratings of subjects’ inter- a
_ _ __ personal or nonverbal sensitivity. Table 6.2 shows the results of the meta- aM |

----- Page 63 (ocr) -----
112 META-ANALYTIC PROCEDURES | __ 4, LUSTRATIONS OF META-ANALYTIC PROCEDURES 113 A |
TABLE 6.2 | where P is the number of positive effect sizes obtained and N is the number L |
| Stem-and-Leaf Plot and Statistical Summary of | of positive plus negative effect sizes obtained. Note that unsigned effect a |
a Validity Coefficients (r) for the PONS Test | sizes are excluded from this analysis. ca
Correlations (Fs) Summary Statistics 7 For the data of Table 6.2, P = 19 and N = 22. Therefore |
cm caf —___—Basedonr note) 7 = PEN _ 20%=22 4 y 5 0003 one-tailed me
5 5 Maximum 55 TN ma p=. | i |
_ 4 | 569 Quartile 3 (Qs) 33 _ He
3 1234 Median (Q,) 22 oe . . ., ; ye
_ : 3 02269 Quartile | (Q) Os | Inthis case, if we had employed the more laborious binomial expansion the l | |
Al 015 Minimum ~.35 a p value would have been .0004 instead. In using the Z test we assign Z a yo
0 0* 16 Q;-Q, .28 7 positive value if the direction of the effect is in the predicted direction and a LL :
~.0 4 31-75(Q3 ~ Qu} 21 | negative value if the direction is not the predicted one.
~ ° ; 22 | ___ Table 6.2 also gives the Z obtained employing Stouffer’s method. In this Ha
. ~? Mean 20 7 ; fore fi ion 5.4 |
. -3 5 N 29 a meta-analysis, the sum of the 22 Z’s was 17.48. Therefore from equation 5. | i
_ Proportion positive sign .86 _ we have : i
_ Z of proportion positive 3.414 . . ; | |
. Combined Stouffer Z 3.730 7 3Z 17.48 7 Gs
_ t test of mean Z 3.86¢ 4 - Vw = Von = 3.73 as our obtained Z, with p = .0001, one-tailed i; L
_ Correlation between r and Z .864 a a
- . p = 0003 a . TS ee
. "i p= 0001 | __ The third method of combining significance levels employed in Table | iF |
. c.p= 0005 || 6.2 was the method of testing the mean Z. In this meta-analysis, the mean i
a : d. p = .0000002 from equation 2.3. 8 5 . ie
_ *This r has a positive sign. || square for the 22 Z’s was .93. Therefore, from equation 5.7 we have ii
. _ SZIN 17.48/22 He
analysis. The display includes all the elements of Table 6.1 relevant tocom. __ 4
bining effect sizes. In addition, however, three methods of combining | _ |
probabilities were also employed and are summarized in the lower portion | which, with 21 df, is significant at p = .0005 one-tailed. | i
. of the summary statistics. | __ Finally, Table 6.2 reports the correlation between the 22 effect sizes (1's) a
_ The first method of combining probabilities listed is one of the counting | _ and their degree of statistical significance (Z’s). The r of .86 is very large and A
. methods. Under the null hypothesis we expect 50% of the correlation coeffi. | canbe explained onthe grounds of this set of validity studies being carried out 1
. cients to have a positive sign. However, the present results show 86% ofthe | _ on fairly similar sample sizes. From equation 2.2 we know that the relation- a
studies to have a positive sign. Given that 19 of the 22 1’s were positive when LD ship between Z and r depends more on the square root of N than on N itself. a i
only 11 were expected to be positive under the null hypothesis, wecanemploy | _ Forthe 22 studies of this meta-analysis, the VN ranges from 2.4 to 9.3, witha Ha
the binomial expansion to calculate how often we expect a result that extreme, 1 median VN of 4.4. Because there is a certain homogeneity of sample sizes i |
or more extreme, if the null hypothesis were true. Tables also are available _| employed in various areas of research, we have often found very substantial al
_ to help us find the desired p (¢.g., Siegel, 1956; Siegel & Castellan, 1988). | _ correlations between effect sizes and levels of statistical significance (Rosen- Hp
For most practical applications, however, we can employ a normal ap- / ___ thal & Rubin, 1978a). This is a useful result, since it sometimes happens that W i
| proximation to the binomial distribution that will work quite well even for | we have access only to significance levels but would like to be able to make i tl |
modest sized samples: _| some guesses about effect sizes for a given area of research. On the basis of 0 in
_| _ this result it is likely that, typically, within a given research area, more signifi- aw
8 7 BRAN (6.1 _| cant results will also be larger in magnitude. vi

----- Page 64 (ocr) -----
. - 114 META-ANALYTIC PROCEDURE, | _ LLUSTRATIONS OF META-ANALYTIC PROCEDURES 115 ell
. Hl. COMBINING EFFECT SIZES AND BLOCKING . TABLE 6.4
: In their meta-analytic work on the accuracy of the detection of deceptig | __ Accuracy of Detecting Deception (z,) for Eight Sources of Information | |
Zuckerman, DePaulo, & Rosenthal (1981) were able to retrieve 72 resi, Arranged as a2 x 2 x 2 Factorial Analysis of Variance | |
csumat the teres accuracy. The magnitude of the accuracy wo, |” Speech No Speech |
co ined by ran t e median of the 72 r’s was .32. In these 72 results, Subjects Face No Face Face No Face al
_ were provided with various sources of information or cues to deceptign | <q 38 62 oF 10. a
including cues from the face, the body, ordinary recorded speech, conten, = aobody 48 38 —.08 .008 a |
filtered speech (tone of voice), and written transcription of what was saiq Proportion of a |
oo Table 6.3 shows the median r obtained (and the corresponding Z,) fo Sources of Variance MS? Total Variance ce |
nine sources or channels of information or combinations. of channels. The | Face .0098 .02 By
a overall median r of .32 convinced us that deception was detectable, but told _ Body 0162 -04 ie |
_ us little about which channels might provide the best sources of information | Speech d ‘128 03 ul |
_ to permit detection of deception. It was to learn about the relative contriby = . speech 0005 00 i |
‘ : : . Face : . a
tion to detection of deception of various channels that we blocked or subd; | Body x speech 0025 00° a |
_ vided the 72 results into the nine subtypes of Table 6.3. | __ Face x body x speech 0220_* 05 Ee
A clearer picture of the relative contribution to the detection of decep. | __ Theoretical value a - i |
tion of the three major channels of face, body, and speech can be obtained by _ A taken since no significance testing was employeci no estimation of any MS for enor was | | |
rearranging the first seven sources of Table 6.3 into the 2 x 2 X 2 array of | _« More precisely, .0057. a
_ : Z,'s shown in Table 6.4. One can get a quick picture of the relative contriby. _ i
_ tion of these three channels and their combinations by performing an analy. _ TABLE 6.5 mp
_ sis of variance on just the eight means shown. Note that the entry fornoface, | Accuracy of Detecting Deception (z,) for Four Sources of Information i ,
. no body, no speech is .00, a theoretical value assuming that there canbeno | _ Arranged as a2 X 2 Factorial Analysis of Variance ue
accuracy when there is no information. ee p e a
The lower half of Table 6.4 shows the analysis of variance of the eightz,’s_ | __ tent No Content HL
of the upper half. Note that no tests of significance have been computed.Ou =| = ™ * one a)
S purpose here is merely to get an overview of the relative magnitude ofthe | __ Notone P 5; r a
: . : _ . roportion o rhe
C sources of variance shown. The analysis shows very clearly that speech is far | Sources of Variance Msb Total Variance |
and away the most important source of cues for the detection of deception. |  Coment 1369 98 ~ i
_ = 0001 00 i
. TABLE 6.3 . | Content x tone .0025 .02 a
Median Accuracy of Detecting Deception (r) Coo }z7™nmees ve il
/ . . . 4 ___a Theoretical value. Ain
in Nine Samples of Studies | obalidi=1. i
Sample «Source of Information _Nof Studies ____Medianr zy | __ Once we have seen that speech is the major source of information rele- al
_ I Face and body and speech 21 33 35 | __ vant to deception detection, we may want to get some idea of what aspect of WE
; race ane body h ; 07 07 | _ speech (e.g., tone versus content) may be most important in providing rele- i
. 4 Face pest 1 _ te <4 | ___ vant cues. Fortunately, we can address this question by employing samples ie
5 Body and speech 3 ‘55 6 7, 8, and 9 of Table 6.3. Table 6.5 arrays these data analogously to Table a!
_ 6 Body 4 10 0 | 6.4. Again the results are clear. Of the two components of speech that could a
d | g Somp P |
7 Speech 12 .36 .38 | __ beexamined for their relative contribution to the detection of deception, it is ne
8 Tone of voice 4 .06 .06 |. content rather than tone that provides the bulk of the useful information. ak
9 Transcript 6 40 42 a Wi
- : Median 6 33 35 _ _
: So oT UIE NPE ERT SITES, 9 oot A AOMARRNNRE ER FR Aah RN, IEC YAN ES POTTER TIE Sete | - i ile

----- Page 65 (ocr) -----
116 META-ANALYTIC PROCEDUREy | _,USTRATIONS OF META-ANALYTIC PROCEDURES 17 |
: : on x recat this nbaninio went from an overall estimate of accuracy based | ainics and workplaces. Computing confidence intervals for our overall if |
: - theoretical interest to us. By arrangin th . bd ‘ ight on questions of | peta-analytic results and also on subdivided or blocked meta-analy ses ,
a analysis, we were able to “how thecal 8 ne el su wisions of our meta. | gives us a good indication of the likely value of effect sizes we might expect ; |
’ roe and farther to soe reer dominance © speech over Visual | tofind inthe relevant population and subpopulations. Computational details i |
source of cues te deception. Had we ‘sP - , con ent ommaes tone as a when the meta-analysis involves stratified random sampling are given in a |
: - ing, we could have employed fhe athedeat more orma significance test. | Rosenthal and Rubin (1978). The last column of Table 6.6 shows that, for all a |
/ ter 4. Later in this chapter, we provide additional illustearions of the oe ro research oe there isa substantial correlation between efieet Sve © |
: _ such more formal comparisons. <a ot |. level of significance (2). | i |
— Vv. COMPARING EFFECT SIZES: i
IV. COMBINING EFFECT SIZES, BLOCKING, _ EARLY COMPARISONS, FOCUSED AND DIFFUSE Lae
AND CONFIDENCE INTERVALS _ |
i | __A much earlier meta-analysis of studies of interpersonal expectancy ef- a
In their meta-analysis of 345 studies of interpersonal expectancy effects | acts was conducted on 10 samples of experimenters who had stated before | |
Rosenthal and Rubin (1978) subdivided the studies into eight areas in which | their research began the mean data they expected to obtain (Rosenthal, a
such studies had been conducted. They employed a stratified random sam. _ (961, 1963). For each sample, then, the correlation was computed between ul
pling procedure to estimate effect sizes (Cohen’s d) and confidence intervals | the data the experimenters expected to obtain and the data the experi- af
for each of the eight areas of research. Table 6.6 shows that the entire 95% _ enters actually did obtain. It should be noted that each of these samples Hi
confidence interval for the area of animal learning lies above the confidence | ans homogenous with regard to expectations that had been experimentally / |
| intervals for the areas of reaction time and laboratory interviews. The effects | induced. Therefore, the obtained correlations do not assess the effects of i
_ of experimenters’ expectancies on the performance of their animal subjects | experimentally-induced expectancies but only individual differences in ex- ‘i
appears dependably greater than the effects of experimenters’ expectancies on a __ pectancies after experimenters had been given an expectancy. The range of ut
the reaction time and interview responses of their human subjects. | expectancies held by experimenters was very much restricted because of the VW |
_ Table 6.6 also shows that the widest confidence interval is around the | expectations that had been induced by the investigator. i |
| : mean effect size of studies carried out in everyday contexts such as schools, a Table 6.7 shows the correlations between expected and obtained data for it 4
D _ | the 10 samples of experimenters. The purpose of this analysis was to com- / |
- Effects of Inter personal Expectancy 7 | pare two subsets of the 10 samples. The first five samples (1-5) of Table 6.7 A
Obtained in Eight Areas of Research | were obtained under ordinary conditions of data collection; the last five i
. a | samples (6-10) were obtained under conditions of high reactance (Brehm, ae
_ ee Correlation _| 1966). The latter samples of experimenters had been offered special incen- i
_ 95 Confidence Betweend / | tives to obtain the data they had been led to expect or had been more explic- i] ; i
- - Research Area ____—_Nof Studies _@ a sin ny 7 2 itly instructed to bias the results of their research. The question was whether i
_ Reaction time en rr = __itese hyper-motivated experimenters might show a higher or a lower Hi
Inkblot tests 9 R408 194 as 7 correlation of their expected with their obtained data than the more ordinar- Hi
e Animal learning 15 173 97 > 49 ‘69 . ily motivated (i.e., control) samples of experimenters. i f
| Laboratory interviews 298 14 ~.36 64 89 | The last column of Table 6.7 shows the contrast weights (\’s) required to ‘ iN
2 - Psychophysical judgments 23a 1.05 49 1.61 62 | L __ test the question of whether the first five r’s differ from the last five r’s. From i |
_ Learning and ability 34a 54 ~13 1.21 .66 | equation 4.27 we have that if
_ erson perception 1198 55 10 1.00 69 a. a
_ Everyday situations 112> .88 ~ 34 2.10 46 Suz a
Estimated mean 70 30 1.10 72 7 g = LL = WG-5) + O68) + +. + (NG32) 5:39 a
a. Analyses were based on a stratified random sample of 15 studies. / ae / (ay , (1)? (=1)? VBA i '
b. Analyses were based on a stratified random sample of 20 studies. 7 Js Wj 3  t3T Tt tT i
S : Rote walebin were based on the number of studies available (not on the number of sub- 7 . i]

----- Page 66 (ocr) -----
118 META-ANALYTIC PROCEDURES | jjLUSTRATIONS OF META-ANALYTIC PROCEDURES i19 i |
LC TABLE 6.7 _ | A. Some Useful (Probably Low Power) Alternatives He
es Correlations between Data Expected and | ___ The procedure we employed for comparing the first 5 to the last 5 effect || |
: _ Data Obtained in 10 Samples of Experimenters Fos of Table 6.7 used all of the information in our data. That is, it was able uy
S _ Sample NofExperimeners sor So Sw 4 « make use of the actual size of the sample employed in each of the studies \
_ 1 rr “9 265°~2~CO*«SSStCS ei summarized. In this section, we note briefly some procedures that
2 6 59 68 3 ‘ | eat each study as a single observation so that the same result would be i
3 6 43 46 3 : | obtained whether each study employed a sample size of 10, or 100, or 1,000. i |
4 6 Al 44 3 1 | Asa first example, suppose we simply computed a t test comparing the a
; v af oo ; |_| mean z,'s of the first and last five studies of Table 6.7.That t would be 2.44, al
| : , . to 10 3 qj a which, with 8 df, would be significant at p< .05 two-tailed. If each of the z,’s i |
8 6 24 41 3 4 | of Table 6.7 had been based on an N of 100, this t would be unaffected. i
9 6 24 — 21 3 =I __ However, equation 4.27, which we applied to these data, would continue to i
10 6 31 —.32 3 =] | yield more and more significant results as our sample sizes per study in- Hl
a.(N~3) ____ Asa second example we can apply the Mann-Whitney U test (Siegel, le
| 4956; Siegel & Castellan, 1988). This test asks whether the bulk of one i
_ For these 10 samples, therefore, experimenters exposed to greater reactance population is greater than the bulk of'a second population. (For a general i
a obtained significantly lower correlations between expected and obtained data, 7 discussion of this test see Siegel, 1956, or Siegel & Castellan, 1988.) For the | /
: When a specific contrast is to be tested, it is not necessary first to test for 7 very special case we have for the data of Table 6.7, i.e., the case of completely |
the heterogeneity of effect sizes, just as it is not necessary to compute an | _ nonoverlapping distributions, and equal n per sample (5 and 5 for these data), Lo
— overall F test in the analysis of variance when a contrast has been planned | we can estimate Z, from |
(Rosenthal & Rosnow, 1984a, 1985, 1991). However, if we had wantedatet | __ . | |
for the heterogeneity of these 10 effect sizes, here is how we would have _ [3m i
done it. First, employing equation 4.16 we would have obtained the weighted _ 2n+1 (6.21 i i
a mean Z,: | ae
a | X(N;-3)2y, _| This estimate works well even for samples as small as the present ones. For a
_ we, B89) F 368) FF 36.32) 13.05 | ourdatan, =n, =n=5, so we find Z as i
* = S(N3) 3434+...43 = 36 = 36 _ i
_ a quantity required for use in equation 4.15, the x’ test for the heterogeneity _ Z=/Gna4l = V 26) +1 = 2-61 P= -009 WW
_ of effect size estimates: . Wn
BUN BN ig T)? = 312.65 -.36)2 +b 368-36"? +... + 36.32.36)? = 20.46 | two-tailed. Employing Siegel’s more precise tables yields a two-tailed pof i
| J | 008, a result that agrees very well with that of our approximation. | |
_. = KD = x79), p< .02. | ___ The use of these two methods is not recommended as a substitute for I
So | equation 4.27. However, they are useful for a quick preliminary view of the i i
_ Thus, the 10 effect sizes differ significantly among themselves. Note that | difference between two samples of studies. In addition to the disadvantage Wl
/ a disproportionate share of this 9 dfx? of 20.46 is associated with the conttas! | that these methods cannot profit from increasing n’s per study, they also do it
of the first five r’s versus the last five r’s. That contrast Z was 3.06 | not have the flexibility of equation 4.27 in permitting any kind of compari- He
its corresponding x?(1) was Zz = (3.06) = 9.36, which represents 46% a Fon one might wish to make. i |
. the total x2(9) of 20.46. The difference between the x?(9) and the y?(1) so al
20.46 — 9.36 = 11.10, the value of the resulting 8 dfx? which is not significant | | | |
(p = .196). . y

----- Page 67 (ocr) -----
Lo 120 META-ANALYTIC PROCEDURgs | i |
CL VI. COMPARING EFFECT SIZES: _ 2 8 a |
LD - MORE RECENT COMPARISONS _ F s Sy 3 i |
. - For our most comprehensive meta-analytic illustration, we consider 1 BS 3 8 4 a
[ “tertiary” analysis. The analysis will be of a re-analysis by Prioleau, Mus 5 3 : wee Ee : , |
dock, and Brody (1983) of the seminal meta-analysis by Smith et al. (1980), § 8 3 83 38 5 5 33. He |
- In general terms, the re-analysis by Prioleau et al. as well as an earl, “ o 8 asgesee8 ca a
re-analysis by Landman and Dawes (1982) support the conclusions draw, _ , SYSESESER LL |
by Smith et al. (1980). a % ex i |
Prioleau et al. (1983) examined a subset of studies comparing the effects | = 3 = ag SSagasegh a

of psychotherapy to the effects of placebo treatments. In what follows | p Se 5 + Ss “ToT Le
we examine this subset of studies within the framework of meta-analytic | a xe i
procedures described in this book and as presented elsewhere (Rosenthal, | Z 2 it
_ 19850). | Z| jgal-esesgreasels & i
Table 6.8 summarizes the result of the present meta-analysis. The 2 5 = 8 AT“ SS o yy v i
studies were divided (or blocked) into five groups. The first three groups Ble =a , S ie i
were entirely comprised of students divided on the basis of age level into | ° s ‘ bd a i
elementary, secondary, and college level. The last two groups were entirely | ge i</8 > Bs = a
_ comprised of patients divided on the basis of the type of placebo em. | __ ‘a 23 © Sree “gy ge xe *s i |
ployed—psychological versus medical. The psychological placebo patients | @ < ga) NU ata Sally ES L i
2 (as well as all the student groups) were those who received some form of a $ £ : H a i
placebo that could have been viewed by patients as psychological in some | 3 = . 2 £ i |
sense. The medical placebo patients were those who received only a pill . & As Sy SARL SaMIS 2 2 i
placebo, i.e., they received only a “medical” placebo treatment. 2 8 Ac eee ° ¢ “TEE 8 i
aS The first two rows of Table 6.8 show the number of studies summarized | __ = 2 = Le
and the total number of persons whose data entered into a determinationof | 8 g a
the average size of the effect (Hedges’s g). The third row gives the mean g for _ rr 2 os ecounotnonan i) 2 4 |
. each group, the fourth and fifth rows give the standard normal deviate (Z) and | __ 3 8 g 3 BLEU A Aare 3 E ni
| | the p level associated with each mean g. The college students and the patients . S 14s? | S 3 if
i who, like the college students, were given psychological placebos both | __ a 3 8 al
| showed substantial benefits of psychotherapy relative to placebo controls and |__ 3 a 3 2 ia
these differences were significant at p well less than .001. The grand mean 5 Sslegeezy+ sxe als Fs |

/ effect size of .24(p < .000004, one-tailed) was smaller than that obtained by 7 E BS) “= 3 CF 8 ES E i
Prioleau et al. and by Smith et al. because it was computed with weighting | __ 5 ry = 2 le
inversely as the variance of g as shown in equations 4.18 and 4.3. ” n 9 3 Tl

_ Rows 6, 7, and 8 of Table 6.8 give the results of tests of heterogeneityof | __ 2 i) ae 2 a
- effect sizes, i.e., tests of whether the g’s in each set of studies differ signifi . e > 3 & Bu 35 i
cantly among themselves. Studies of elementary school children and of pe | ge 2 2. & 4 gr Vs dl
_ tients receiving psychological placebo yielded g’s that were significantly | a BB og 2 B 2 8 g £ Bn =8 ah
| heterogeneous. (See equations 4.17 and 4.18.) . 5 E 3 3 23 wes 3 4 S| a8, go a
Lines 9, 10, and 11 address the question of the relationship between the | __ a = 5 ‘ Es - ‘ 3 g 3 z £8 i
size of the study and the size of the g. The Z’s and p’s for linear contrasts. | ZEN DRS ON atl eek ese ay
show that among elementary school children larger g’s were found in) - SeeTeecees e = res? a i

----- Page 68 (ocr) -----
122 META-ANALYTIC PROCEDURES | y1uSTRATIONS OF META-ANALYTIC PROCEDURES 123 il

2S - TABLE 6.9 | pctioning investigated by Hyde (1981), effect sizes were significantly het- L 1
_ Contrasts Among Five Groups of Studies oo enous. In addition, we showed that in all four areas, studies conducted Hh

: - of Psychotherapy Effects | = recently showed a substantial gain in cognitive performance by fe- i [
Students versus patients ee VII. A ONE-SAMPLE EFFECT SIZE INDEX: I Ve
_ Linear trend in age of students 2.27 .012 li,
Quadratic trend in age of students 2.08 019 _ | All of the effect size indices we have employed so far in this chapter were | 7

~~ svremlehovogical versus me medical placebo 2.18 ois _ jwo-sample or multi-sample indices, e.g., d, g, and r. Such effect size indices | i
te not directly applicable, however, to areas of research employing one- i |
smaller studies (p < .000004; r = --.24), but among patients receiyin 2 | sample studies in which performance is compared to some theoretical value; ull
psychological placebos, larger g’s were found in larger studies (p = .009;; - | often the level expected if performance is no better than that expected if the | a |

.16). Thus, although for all studies combined, larger g’s are associated wih ull hypothesis were true. In so-called ganzfeld studies, for example, subjects |
- smaller studies, there are statistically significant reversals of this overall —_ ate asked to guess which of four or five or six stimuli had been “transmitted” L

_ relationship. a by an agent or sender (Harris & Rosenthal, 1988, Honorton, 1985; Hyman, iH
. compared meaningfully within the framework of a set of four contrasts em | fis type of one-sample situation (Rosenthal & Rubin, 1989). This index is iL |
ploying equations 4.28 and either 4.3 or 4.21. The first contrast shows that | _ expressed as the proportion of correct guesses if there had been only two iF |
there is little difference between students and patients in the degree to | _ choices to choose from. When there are more than two choices, 7 converts a |
which psychotherapy is more effective than placebo. The second contrast |_ the proportion of hits to the proportion of hits made if there had been only uf
 . shows that with increasing age of students, greater g’s are obtained. The two equally likely choices: ° | \
third contrast shows that the average of the elementary and college student P(k - 1) / i |
groups yields a larger g than does the group of secondary students. (In inter n= Pk -2)41 [6.3]
preting these contrasts in age we should note that age is likely to be con ut
i founded here with such variables as IQ, type of treatment, type of placebo, . . ae . We

- control, and so forth.) The fourth contrast shows that psychotherapy i more 7 P= ne le proportion of hits and k is simply the number of alternative i
_ effective relative to psychological than to medical placebo controls. Perhaps pos manne f wis given by: . a

/ pill placebos are so effective that it is difficult for psychotherapy to be supe- _ The standard error of 1s given by: i
rior to them. _ _ 1 (a - 2) 64 He
To address this last question, to help understand the significant linearand SE in) = VN Ronee [6-4] a
quadratic contrasts in age, the meaning of the sometimes positive, some | |
SS times negative correlation between g and N, and the significant heteroge: oo that we can test the significance of a given m by means of the following Z il
neity of g’s found among studies of elementary school children and studies —_ gg. ui

-_ | of patients given psychological placebo, additional studies will be required. i
This section was designed to illustrate how the systematic application of _ z= 70 [6.5] a

' _ various meta-analytic procedures can lead to firmer inferences about a do- _ SE @ a

| main of research. At the same time, however, it should be clear that mete : |
| analyses need not close off further research in an area. Indeed, they maybe ____“oMfidence intervals are also readily available, e.g., for 95% confidence A

/ _ employed to help us formulate more clearly just what that research should be _ Intervals we employ: | i

A similar set of meta-analytic procedures was recently carried out in the | i
_ research area of sex differences in cognitive functioning (Rosenthal & Re m+ 1.96 SE. [6.6] 1
bin, 1982b). In that analysis, we showed that, in the four areas of cognitive | |

----- Page 69 (ocr) -----
c META-ANALYTIC PROCEDURES | 4, USTRATIONS OF META-ANALYTIC PROCEDURES 125 Hl
i Contrasts among independent x’s can be tested by: 4 TABLE 6.11 a

- ZAj 3; _ Statistical Summary of 28 “Direct Hit” Ganzfeld Studies a
V=(A? SEG) ley Sentral tendency (x) Variability a

i __ pnweighted mean 62 Maximum 96 aL
Finally, we can test the heterogeneity of a set of m’s from the following | Neh” mean te Median ( o> on i)
relationship: [portion > .50 82 Quartile 1 (Q,) 54 il

- X(K = 1) = E) oe (6.3) | Sietitcan fe oo | Bc rf ile

(nj | | combined Stouffer Z 6. 3: [.75(Q3 - Q1)] IL

| test of mean x - 50 3.39 S NGS ) 19 uy

| | 4 of proportion > .50 3.40 -E) p= .04 a

_ where 2. prop Robustness (t= 239) 63 a
_ EW; confidence intervals” Ue |
99% 52 72 ° | |
. and 9.5% 51 3 |
_ : 7 3 By number of trials per study; total number of trials = 835. i |
_ 1 6 Based on N of 28 studies. ‘a
| | ___ For examples of the application of all these equations see Rosenthal and 7 i
.. TABLE 6.10 | Rubin (1989) i |
_. Stem oe ioe C ae aes (x) for 28 | ___ Table 6.10 shows the stem-and-leaf plot of effect sizes (st) for 28 ganzfeld Af |
_ it Ganzfeld Studies ___ studies, and Table 6.11 shows the statistical summary of these data. a
Stem Leaf ~~ | ___Asaslightly more complex index of the stability, replicability, or clarity i 7
90 5 6 __|_ ofthe average effect size found in the set of replicates, one could employ the ye
oo 90 | mean effect size divided either by its standard error (S/N where N is the iF |
5 5 total number of replicates), or simply by S. The latter index of mean effect lie

70 7 ____ size divided by its standard deviation (S) is the reciprocal of the coefficient i
a 7 |o 4 4 1 4 2 | of variation or a kind of coefficient of robustness (Rosenthal, 1990). Hee

6 | 7 7 8&8 8&8 8g 9 _ bal

_ 50 5 ? . | VILA. The Coefficient of Robustness of Replication | a

i 50 4 4 Average Value expected if Hp were true 2 : : : i ie

740 FT _|___ Although the standard error of the mean effect size along with confidence ie
40 3004 ____ intervals placed around the mean effect size are of great value (Rosenthal & i) |

0 ____ Rubin, 1978), it will sometimes be useful to employ a robustness coefficient i
50 5 ___ that does not increase simply as a function of the increasing number of |
20 __|_feplications. Thus, if we want to compare two research areas for their 1
oo 10 | fobustness, adjusting for the difference in number of replications in each Co
_ A |_| fesearch area, we may prefer the robustness coefficient defined as the L
00 0 | feciprocal of the coefficient of variation. Lh
8 a, Probability of a direct hit =.50 for the effect size index x. | ii |

----- Page 70 (ocr) -----
126 META-ANALYTIC PROCEDURES | 4 i
. The utility of this coefficient is based on two ideas —first, that replicatign | |
success, clarity, or robustness depends on the homogeneity of the obtaingy) . . i
effect size, and second, that it depends also on the unambiguity or clarity - The Evaluation of M. et. a-A naly Lic | |
| the directionality of the result. Thus, a set of replications grows in robustneg _ P roc ed ures and Meta -A nalyt ic Res ult S [ i
a as the variance of the effect size decreases and as the distance of the mea, | _
effect size from zero increases. Incidentally, the mean may be weighteg |
- unweighted, or trimmed (Tukey, 1977). Indeed, it need not be the mean i | Criticisms of the meta-analytic enterprise are described and discussed under the general |
_ all but any measure of location or central tendency (e.g., the median), headings of sampling bias, information loss, problems of heterogeneity of method and of - iI
ce quality, problems of independence, exaggeration of significance levels, and the practical | |
_ importance of any particular estimated effect size. |
_ a We have had an opportunity to examine a variety of meta-analytic proce- |
dures so that we would now be able to carry out meta-analyses of research We
| areas. But should we want to? The purposes of this final chapter are to { i
examine some negative evaluations of meta-analysis and to evaluate the i
| _ merits of these evaluations. a I /
_ | ___In the years 1980, 1981, and 1982 alone, well over 300 papers were il
__ published on the topic of meta-analysis (Lamb & Whitla, 1983), and the rapid [ i
| growth continues (Hunter & Schmidt, 1990). Does this represent a giant i
_____ stride forward in the development of the behavioral and social sciences or t i |
_____ does it signal a lemming-like flight to disaster? Judging by the reactions to a
| __ past meta-analytic enterprises, there are some who take the more pessimistic Ve
view. Some three dozen scholars were invited to respond to the meta-analysis | i
a | __ of studies of interpersonal expectancy effects (Rosenthal & Rubin, 1978). ii)
_____ Although much of the commentary dealt with the substantive topic of a
___ interpersonal expectancy effects, a good deal of it dealt with methodological ri
—_ aspects of meta-analytic procedures and products. Some of the criticisms a
—____ offered were accurately anticipated by Glass (1978) who had earlier received il
___commentary on his meta-analytic work (Glass, 1976) and that of his col- | |
____leagues (Smith & Glass, 1977; Glass et al., 1981). In this chapter, the a
_____ atiticisms of our commentators are grouped into a half-dozen conceptual a
_ _____ categories, described, and discussed. i

----- Page 71 (ocr) -----
128 META-ANALYTIC PROCEDURRs _|_gyALUATION OF PROCEDURES AND RESULTS 129 i
L. £ SAMPLING BIAS AND THE | gpiribution of scores by the mean and o we have nearly described the 7
_ FILE DRAWER PROBLEM __ gjstribution perfectly. If the distribution is quadrimodal, the mean and o will a
This criticism holds that there is a retrievability bias such that studies _ _ pot do a good job of summarizing the date It is the care an atysts 100 m " . yl
_ retrieved do not reflect the population of studies conducted. One version of _ _ individual study, and the meta-analyst’s job in meta-ana ysis, to 8 oss we" ; i
this criticism is that the probability of publication is increased by the statis. | Providing the reader with all the raw da taofall th ° studies summarized avoids i
: 2 tical significance of the results so that published studies may not be repre- ____ this criticism but serves no useful teview function. Providing the reader with |
- sentative of the studies conducted. This criticism is well taken although jt | _ astem-and-leaf display of the effect sizes obtained, along with the results of |
/ applies equally to traditional narrative reviews of the literature. One set of _ the diffuse and focused comparisons of effect sizes, does some glossing but ll
procedures that can be employed to address this problem was described ip _. it does a lot of informing besides. . i
Chapter 5 when the file drawer problem was discussed. | __ There is, of course, nothing to prevent the meta-analyst from reading i
_ A bizarre version of this criticism simply holds that the unretrieved studies each study as carefully and assessing it as creatively as might be done bya i
are essentially a mirror image of the retrieved studies (Rosenthal & Rubin _ _ more traditional reviewer of a literature. Indeed, we have something of an i
1978). Thus if the combined Z for 100 studies is + 6.50, there is postulated t 6 7 _ operational check on reading articles carefully in the case of meta-analysis. i
: be, in the file drawers, another set of studies with combined Z = —6.50! No _ i we do not read the results carefully, we cannot obtain effect sizes and | |
- mechanism whereby this phenomenon may operate has been proposed andno _ significance levels. In traditional reviews, results may have been read care- LS
_ reply to this criticism seems possible. One can too easily postulate a universe fully or not read at all with the abstract or the discussion section providing uy
in which for every observed outcome there is an unobserved outcome equal _ “the results” to the more traditional reviewer. i
and opposite in magnitude and/or in significance level. 7 I, PROBLEMS OF HETEROGENEITY i i
/ - Ii. LOSS OF INFORMATION ILA. Heterogeneity of Method ’ | |
. . ______ The first of two criticisms relevant to problems of heterogeneity notes i
I1.A. Overemphasis on Single Values _ -anal tudies in which the independent variables ap
The first of two criticisms relevant to information lo tes the d & meta-ana yses average overs ; . pet , | |
. : : non ss notes anger of the dependent variables, and the sampling units are not uniform. How can |
trying to summarize a research domain by a single value such as a meat —_|_wespeak of interpersonal expectancy effects, meta-analytically, when some a |
_ effect size. This criticism holds that defining a relation in nature by a single a _ of the independent variables are operationalized by telling experimenters |
value leads ‘ein motacanalvels is seen as including wot say o oe that tasks are easy versus hard or by telling experimenters that subjects are i _
- combining  _ . . i ad
effect sizes (and significance levels) but also comparing effect sizes in both | in aia i ‘ performers? now th "k Spedk aoe lytically of iL |
_ diffuse and, especially, in focused fashion. - _ these expectancy e ects when sometimes the dependent variables are reac- |
oo tion times, sometimes IQ test scores, and sometimes responses to inkblots? Le
C ILA.1. Overlooki tive i A ial f the critici | How can we speak of these effects when sometimes the sampling units are | |
_ -A.l. Overlooking negative instances. ‘\ special case OF the criticism ____ tats, sometimes college sophomores, sometimes patients, sometimes pu- ae
8 under discussion is that, by emp hasizing average values, negative cases are = =—_ils? Are these not all vastly different phenomena? How can they be pooled L | |
overlooked. There are several ways in which negative cases can be defined; 1 fosether ina single meta-analysis? | |
€-84 P= T= 0, r negative, r significantly negative, and so on. Ho we _- Glass (1978) has eloquently addressed this issue—the apples and or- | | ,
_ we may define negative cases, when we divide the sample of studies into anges issue. They are good things to mix, he wrote, when we are trying to i i
negative and positive cases we have merely dichotomized an underly M§ _ generalize to fruit. Indeed, if we are willing to generalize over subjects i
continuum of effect Sizes OF significance levels and accounting for negative within studies, why should we not be willing to generalize over studies? If 7
cases is simply a special case of finding moderator variables. | subjects behave very differently within studies we block on subject charac- i
ILB. Glossing Over Details ___ teristics to help us understand why. If studies yield very different results L i
Although it is accurate to say that meta-analyses gloss over details, it is | ftom each other, we block on study characteristics to help us understand
_ equally accurate to say that traditional narrative reviews do so and that data_—_—_—Why. It is very useful to be able to make general statements about fruit. If, in i,
. analysts do so in every study in which any statistics are computed. Summa _Addition, it is also useful to make general statements about apples, about a
oe rizing requires us to gloss over details. If we describe a nearly normal _ i |

----- Page 72 (ocr) -----
f 130 META-ANALYTIC PROCEDURgg | gyALUATION OF PROCEDURES AND RESULTS 131 | |
_ oranges, and about the differences between them, there is nothing in Meta. - ws. Studies within Sets of Studies | | |
analytic procedures to prevent us from doing so. Indeed, Chapter 4 espe. | Even when all studies yield only a single effect size estimate and level of : i
cially deals with these procedures in detail. | significance and even when all studies employ sampling units that do not il
_ ; ___ so appear in other studies, there is a sense in which results may be non- Al
__ HIB. Heterogeneity of Quality _ | jndependent. That is, studies conducted in the same laboratory, or by the on
a One of the most frequent criticisms of meta-analyses is that bad studies _ . ame research group, may be more similar to each other (in the sense of an lA

_ are thrown in with good. This criticism must be broken down into two __jntraclass correlation) than they are to studies conducted in other laborato- | |
questions: (1) What is a bad study? and (2) What shall we do about bad | fies or by other research groups (Jung, 1978; Rosenthal, 1966, 1969, 1979, i |
_ studies? | 4990b; Rosenthal & Rosnow, 1984a). The conceptual and statistical impli- |
| cations of this problem are not yet worked out. However, there are some U |
- HLB1. Defining “bad” studies. Too often, deciding what is a bad study | preliminary data bearing on this issue that are at least moderately reassuring. / |
is a procedure unusually susceptible to bias or to claims of bias (Fiske, | _Table 7.1 shows a series of 94 studies blocked or subdivided into seven | |
/ 1978). Bad studies are too often those whose results we do not like or, ag areas of research on interpersonal expectancy effects (Rosenthal, 1969). For / |
Glass et al. (1981) have put it, the studies of our “enemies.” Therefore when a each area, the combined Z was computed; once based on the n of studies in a
_ reviewers of research tell us they have omitted the bad studies, we should __ that area, and once based on the n of laboratories or principal investigators. Bo
| satisfy ourselves that this has been done by criteria we find acceptable, 4 | For most of the research areas there is little difference in n between studies | |
discussion of these criteria (and the computation of their reliability) can be _ and laboratories so there is little difference in their Z’s. The only noticeable | | |
found in Chapter 3. difference in Z’s is for the research area in which there were substantially il |
co more studies (n = 57) than there were laboratories (n = 20). Even there, i |
U1.B.2, Dealing with bad studies. The distribution of studies on a dimen- 7 _ however, it seems unlikely that we would have drawn very different conclu- | L |
sion of quality is of course not really dichotomous (good versus bad) but _ sions from these two methods of analysis. Hh ;
: continuous with all possible degrees of quality. Because we dealt with the _ Perhaps the most important result, however, is seen when we compare iL

/ issue in detail in Chapter 3, we can be brief here: The fundamental method _ the overall Z for all 94 studies with the overall Z for the 48 laboratories. i
i of coping with bad studies or, more accurately, variations in the quality of | There is less than a 3% decrease in the combined Z when we go from the l, i
research, is by differential weighting of studies. Dropping studies is merely 7 analysis per study to the analysis per laboratory. It would be useful if similar | ia
the special case of zero weighting. analyses employing effect size estimates were available. .
The most important question to ask about study quality is asked by Glass __ Z | L
(1976): Is there a relationship between quality of research and effect size _. A
obtained? If there is not, the inclusion of poorer quality studies willhaveno TABLE 7.1 i
effect on the estimate of the average effect size though it will help to de- 7 Significance Levels Computed Separately J
crease the size of the confidence interval around that mean. If there isa for Studies and for Laboratories Hl |
relationship between the quality of research and effect size obtained, wecan | —————————__________ i,
employ whatever weighting system we find reasonable (and that we CaM gy areg pues , Faborator ies Difference it

_ persuade our colleagues and critics also to find reasonable). —O—— TT TO ne

| Animal learning 8.64 9 8.46 5 18 HT ‘
a ____ learning and ability 3.01 9 2.96 8 05 He

_ Psychophysical judgments 2.55 9 2.45 6 .10 He
IV. PROBLEMS OF INDEPENDENCE | Reaction time 193 3 193 3 00 i
IV.A. Responses within Studies . ___ laboratory interviews 3 30 6 530 ‘ 00 Hi |
- The first of two criticisms relevant to problems of independence notes | _ Person perception 407.57 377 00 130 | |
_ that several effect size estimates and several tests of significance may be | |
generated by the same subjects within each study. This can be a very apt _Alllstudies 9.82 94a 9.55 488 27 / |
8 criticism under some conditions. Chapter 2 deals with the problem in detail, ie entries were nonindependent and the mean Z across areas was used for the single |

----- Page 73 (ocr) -----
L 132 META-ANALYTIC PROCEDURg, VALU ATION OF PROCEDURES AND RESULTS 133

V. EXAGGERATION OF SIGNIFICANCE LEVELS _ S point of view of practical usefulness (Cooper, 1981). Rosenthal and Ru- J

LS V.A. Truncating Significance Levels TT in (1979b, 1982c) found that neither experienced behavioral researchers |

It has been suggested that all p levels less than .01 (Z values gteate | “pot experienced statisticians had a good intuitive feel for the practical mean- ! :

2.33) be reported as .01 (Z = 2.33) because p’s less than .01 are like! vy _ ing of such common effect size estimators as r?, omega’, epsilon’, and simt- a)

: 2 in error (Elashoff, 1978). This truncating of Z’s cannot be recomiiénded. tT lat estimates. wg Pere ‘ A

will, in the long run, lead to serious errors of inference (Rosenthal & R oe Accordingly, Rosenthal and Rubin introduced an intuitively appealing Hy

: _ 1978). If there is reason to suspect that a given p level < .01 is in S “7 _ general purpose effect size display whose interp retation SP erfecth y trans- |
should, of course, be corrected before employing it in the meta-anal we | parent: the binomial effect size display (BESD). There is no sense in which a
should not, however, be changed to p = .01 simply because it is len the tte claim to have resolved the differences and controversies surrounding ie
S| OL. an the use of various effect size estimators but their display is useful because it a

Ue VB. Too Many Studies 7 is easily understood by researchers, students, and lay persons, applicable in |

| Tt has been noted as a criticism of met ; __awide variety of contexts, and conveniently computed. i
studies increases, there is a oroater re a-analyses that as the number of The question addressed by BESD is: what is the effect on the success rate | }
null hypothesis (Ma ° 1 978) Whe “th ee probability of rejecting the | 8. survival rate, cure rate, improvement rate, selection rate, and so on) of i)

fo ht to b AYO, B7718). Wen the nu ypothesis is false and, there. —_ the institution of a new treatment procedure, a new selection device, or a a

S Te, ougnt to be rejected, it is indeed true that adding observations (either ___ new predictor variable? It therefore displays the change in success rate (e.g., i

_ empling ‘tis wenn studies or new studies) increases Statistical power, | survival rate, cure rate, improvement rate, accuracy rate, selection rate, etc.)

: ohantetiate rn Ae aceee os 2 seeitimate triticism of a procedure, a __ attributable to the new treatment procedure, new selection device, or new a

_ . this case, type Il errors. When the “ an ‘ ecreases its error rate—in | predictor variable. An example shows the appeal of the display. Suppose the \ !

- adding ites dose vor Jeng ac emu Ypot esis is really true, of course, estimated mean effect size were found to be an r of .32, approximately the |

- hypothesis, Addi sead reased probability of r ejecting the null __size of the effects reported by Smith and Glass (1977) and by Rosenthal and we
- pothesis. Adding studies, it should also be noted, does not increase the __ Rubin (1978) for the effects of psychotherapy and of interpersonal expect- |
- size of the estimated effect. _ i
a ancy effects, respectively. i

_ | A related feature of meta-analysis is that it may, in general, leadtog | —_—‘Table 7.2 is the BESD corresponding to an r of .32 or an r? of .10. The we
L - decrease in type I errors even when the number of studies is modest. The table shows clearly that it is absurd to label as modest an effect size equiva- al

_ empirical support for this was described in Chapter 1 when the research by lent to increasing the success rate from 34% to 66% (€.g., reducing a death Te
Cooper and Rosenthal (1980) was summarized. Procedures requiring the sate from 66% to 34%). Even so small anr as .20, accounting for “only” 4% |
. research reviewer to be more systematic and to use more of the information of the variance is associated with an increase in success rate from 40% to tg
_ in the data seem to be associated with increases in power, i.e., decreases in _ 60%, e.g., a decrease in death rate from 60% to 40%, hardly a trivial effect. it :
type IL errors. | _ It might be thought (e.g., Hunter & Schmidt, 1990, p. 202) that the BESD can
_ be employed only for dichotomous outcomes (e.g., alive vs. dead) and not a |
vi. ae RS TIMATED ee RCr ete OF ce for continuous outcomes (e.g., scores on a Likert-type scale of improvement | |
_ due to psychotherapy, or gains in performance due to favorable interpersonal i | (|
Mayo (1978) criticized Cohen (1977) for calling an effect size large (d = a ___ expectations). Fortunately, however, the BESD works well for both types of | 7
-80) when it accounted for “only” 14% of the variance. Similarly, Rimland outcomes under a wide variety of conditions (Rosenthal & Rubin, 1982c). i
_ (1979) felt that the Smith and Glass (1977) meta-analysis of psychotherapy _—_—~-A great convenience of the BESD is how easily we can convert it to r (or ah
outcome studies sounded the death knell for psychotherapy because the _____§)and how easily we can go from r (or) to the display. i
_ effect size was equivalent to an r of .32 accounting for “only” 10% of the Table 7.3 shows systematically the increase in success rates associated ue |
_ variance. | __with various values of r? and r. For example, an r of .30, accounting for ay
VLA. The Binomial Effect Size Display (BESD) 7 oy 9% of the variance is associated with a reduction in death rate from | |
Despite the growing awareness of the importance of estimating effect 6 ® to 35%, or more generally with an increas tn success rate from 35% to Ul
sizes, there is a problem in evaluating various effect size estimators from _ 5%. The last column of Table 7.3 shows that the difference in success rates ‘A |

----- Page 74 (ocr) -----
i 134 META-ANALYTIC PROCEDURES | yal ATION OF PROCEDURES AND RESULTS 135 a
i a. TABLE 7.2 _ Institute to break off its study? Was the use of propranolol accounting Tl
ao The Binomial Effect Size Display (BESD) for an r of .32 | ie 90% of the variance in death rates? Was it 50% or 10%, the overly a
ao at Accounts for “Only” 10% of the Variance _ | modest effect size that should prompt us to give up psychotherapy? From a
~ ‘Treatment niRealt equation 2.15, we find the proportion-of-variance-accounted-for (r*): |
Condition _ Alive Dead s 7 x 4.2 i
Treatment 66 34 100 _ . P =A = gi0s 7 002 i
Control 34 66 100  -. (i
x 100 100 200 _. . :
_ , ee: ound propranolol study was discontinued for an effect accounting for it
: is identi , | 4 /5th of 1% of the variance! To display this result as a BESD we take the ce
_ BESD is computed ae nity, he ne mental Broup success rate inthe Fate root of r2 to obtain the r we use for the BESD. That r is about .04 li
_ computed as .50 ~ 1/2 whereas the control group success rate jg ____ghich displays as shown in Table 7.4. As behavioral researchers we are not us
o ___gecustomed to thinking of 1’s of .04 as reflecting effect sizes of practical A
- VLB. The Propranolol Study and the BESD | importance. If we were among the 4 per 100 who moved from one outcome | i
- On October 29, 1981, the National Heart, Lung, and Blood Institute _ tothe other, we might well revise our view of.the practical import of small [i
officially discontinued its placebo-controlled study of propranolol because _ effects! a
: the results were so favorable to the treatment that it would be unethical tg a
| keep the placebo control patients from receiving the treatment (Kolata _ I |
- 1981). The two-year data for this study were based on 2108 patients and _ . TABLE 74 : a
_ x*(1) was approximately 4.2. What then, was the size of the effect that led |. The Binomial Bit ect Size Display lf Ly
_ for the Discontinued Propranolol Study J
- TABLE 7.3 | me
Changes in Success Rates (BESD) 7 iti ive = d p a
. Corresponding to Various Values of r? and r _ __—_Londition ___Alite _Degt = __ — =
_ —_ ne | __. Propranolol 52 48 100 ia
_ ; Equivalent to a a Placebo 48 52 100 1}
Effect Sizes Success Rate Increase Difference i | 00100 i |
- Po from To Success Rates? _ ; i
- .00 02 49 51 02 |. ; ; i
. ‘oo ‘oa 48 0 ‘4 _____ This type of result seen in the propranolol study is not at afl unusual in i
2 00 0 47 53 06 ____ biomedical research (Rosenthal, 1990a). Some years later, on December 18, .
| _ ‘01 10 a 3s 08 ___ 1987, it was decided to end prematurely a randomized double blind experi- i |
01 12 “Ad x6 im | ment on the effect of aspirin on reducing heart attacks (Rosnow & Rosenthal, i
03 16 ‘40 58 16 1989; Steering Committee of the Physicians Health Study Research Group, /
08 20 40 60 20 1988). The reason for this termination of this large study (N = 22,071) was i bo
7 . 24 38 62 24 | that aspirin was so effective in preventing heart attacks (and deaths that it 1
09 30 35 65 asp P ie
_ 6 0 30 0 0 would be unethical to continue to give half the physician subjects a placebo. cE
25 50 25 95 50 ___ The 1? for this important effect was about half the size of the r? for the ie
a 36 60 a0 "80 60 ____ propranolol effect, .0011 versus .0020, and r for the aspirin effect was .034. We
o “9 70 AS 85 70 ___ Table 7.5 summarizes these results and presents several others that also yield Hl
‘Bl 0 : os ff .80 “small” r’s despite being results of major medical, behavioral, and/or eco- a
Hoo 100 OO 00 eo a importance. i]
. : a. The difference in success rates in a BESD is identical to r. “ i |

----- Page 75 (ocr) -----
META-ANALYTIC PROCEDURE, _ _ |
Effect Sizes of Seven Independent Variables | i
Independent Variable Dependent Variable ; _ i
Propranolol* Death Sm FF i
vicinam veteran status” Alcohol problems fe co L
a estosterone Adult deli . oO | ih
Cyclosporine" Death 2 oF REFERENCES i
Psychotherapy* Improvement . os |
. ®. iain x Crore ase Control Vietnam Experience Study, 1988, - a | ageock, C. J. (1960). A note on combining probabilities. Psychometrika, 25, 303-305. i a
d. Canadian Multicentre Transplant Study G _ dlexander, R. A., Scozzaro, M. J., & Borodkin, L. J. (1989). Statistical and empirical Be
e. Barnes, 1986. Plant Study Group, 1983. examination of the chi-square test for homogeneity of correlations in meta-analysis. He
_ _ psychological Bulletin, 106, 329-331. ee
_ American Psychological Association. (1983). Publication manual of the American Psycho- i /
: ; : ___ iogical Association (3rd ed.). Washington, DC: Author. ie
. VLC. Concluding Note on Interpreting Effect Sizes _ En. 5. (1974). Theta reliability and factor scaling. In H. L. Costner (Ed.), Sociological i
Rosenthal and Rubin (1982c) proposed that the reporting of effect methodology 1973-1974. San Francisco: Jossey-Bass. a
: could be made more intuitive and more informative by using the BEsn yt “Atkinson, D. R., Furlong, M. J., & Wampold, B. E. (1982). Statistical significance, reviewer 1 oe
was their belief that the use of the BESD to di : : to __ evaluations, and the scientific process: Is there a (statistically) significant relationship? ae
to display the increase in success Counseling Psychology, 29, 189-194 a
| rate due to treatment would more clearl 8 _Journal of Counseling & syclo'ogy, " ce
| f ore Clearly convey the real-world importance _gakan, D. (1967). On method. San Francisco: Jossey-Bass. lo.
/ of treatment effects than would the commonly used descriptions of fer | _pumes, D. M. (1986). Promising results halt trial of anti-AIDS drug. Science, 234, 15-16. a
- size, especially those based on the proportion of variance accounted for | pireh, M. W. (1963). Maximum likelihood in three-way contingency tables. Journal of the i |
One effect of the routine employment of a display procedure such . the __ Royal Statistical Society, B, 25, 220-233. |)
_ BESD to index the practical meanin g of our research results would be to a Spam, A eae Combining independent tests of significance. Journal of the American [| i.
- oat € ____ Statistical Association, 49, 559- 74, rae
me more useful and realistic assessments of how well we are doing as __ Blackmore, S. (1980). The extent of selective reporting of ESP ganzfeld studies. European Hl
chers in app ied social and behavioral science and in the social and _‘Journal of Parapsychology, 3, 213-219. Wa
behavioral sciences more generally, Employment of the BESD has, in fac | _ Bloom, B. S. (1964). Stability and change in human characteristics. New York: John Wiley. Me 4
. shown that we are doing considerably better in our “softer” scie ncés than wo __ Blyth, C. R. (1972). On Simpson’s paradox and the sure-thing principle. Journal of the an oo
thought we were. 7 American Statistical Association, 67, 364-366. a /
_ ___ Brehm, J. W. (1966). A theory of psychological reactance. New York: Academic Press. a
____Brozek, J., & Tiede, K. (1952). Reliable and questionable significance in a series of statistical a
oo. _ tests. Psychological Bulletin, 49, 339-341. i
| Canadian Multicentre Transplant Study Group. (1983). A randomized clinical trial of cyclo- He
_ _____ sporine in cadaveric renal transplantation. New England Journal of Medicine, 309, Ha _
oo __ Centers for Disease Control Vietnam Experience Study. (1988). Health status of Vietnam i ie
_ : . veterans: 1. Psychosocial characteristics. Journal of the American Medical Association, | i. :
LC Chan, S. S., Sacks, H. S., & Chalmers, T. C. (1982). The epidemiology of unpublished ie
i ___ randomized control trials. Clinical Research, 30, 234A. ie
oe Cochran, W. G. (1937). Problems arising in the analysis of a series of similar experiments. ATL
i . Journal of the Royal Statistical Society, Supplement 4(1), 102-118. i
_ (ochran, W. G. (1943). The comparison of different scales of measurement for experimental a
_ | __tesuits. Annals of Mathematical Statistics, 14, 205-216. a

----- Page 76 (ocr) -----
Cochran, W. G. (1954). Some methods for strengthening the common ¥’ tests. Biome Fpoctot R. M. (1968). Bias effects and awareness in studies of verbal conditioning. Doctoral |
- 10, 417-451. 0, jssertation, University of Illinois. |
Cochran, W. G., & Cox, G. M. (1957). Experimental designs (2nd ed.). New York: —  cuck, J. B., & Joseph, G. (1983), The bases of teacher expectancies: A meta-analysis. ;
Wiley. (First corrected printing, 1968). : “ 2 Journal of Educational Psychology, 75, 327-346. | L /
Cohen, J. (1962). The statistical power of abnormal-social psychological research: Awe |  eaely, A. H., & Carli, L. L, (1981). Sex of researchers and sex-typed communications as | |
_ Journal of Abnormal and Social Psychology, 65, 145-153. = | determinants of sex differences in influenceability: A meta-analysis of social influence i
_ Cohen, J. (1965). Some statistical issues in psychological research. In B. B, Wolman (ay studies. Psychological Bulletin, 90, 1-20. .
Handbook of clinical psychology. New York: McGraw-Hill. a | gdgington, E. S. (1972a). An additive method for combining probability values from i |
_ Cohen, I. (1969). Statistical power analysis for the behavioral sciences. New York: Academie independent experiments. Journal of Psychology, 80, 351-363. i i:
- Press. | gézington, E. S. (1972b). A normal curve method for combining probability values from Lo :
Cohen, J. (1977). Statistical power analysis for the behavioral sciences (rev. ed.), New Yor |. independent experiments. Journal of Psychology, 82, 85-89. |
- Academic Press. Eisner, D. A., Kosick, R. R., & Thomas, J. (1974). Investigators’ instructions and experi- L / i
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale . - menters’ misrecording of questionnaire data. Psychological Reports, 35, 1278. i | 7 Ez
NJ: Lawrence Erlbaum Associates. - | Blashoff, J. D. (1978). Box scores are for baseball. The Behavioral and Brain Sciences, 3, i
: Collins, H. M. (1985). Changing order: Replication and induction in scientific Practice = 392.
Beverly Hills, CA: Sage. Ennis, J. G. (1974). The bias effect of intentionality and expectancy on operant acquisition a
Compton, J. W. (1970). Experimenter bias: Reaction time and types of expectancy inform. in rats, and on the accuracy of student experimental.reports. Unpublished manuscript, i |
tion. Perceptual and Motor Skills, 31, 159-168. | Middlebury College. ad :
Cook, T. D., & Leviton, L. C. (1980). Reviewing the literature: A comparison of tradition __Federighi, E, T. (1959). Extended tables of the percentage points of Student’s t-distribution. | :
: methods with meta-analysis. Journal of Personality, 48, 449-472.  . __ Journal of the American Statistical Association, 54, 683-688. / i:
- Cooper, H. M. (1979). Statistically combining independent studies: A meta-analysis of | | Feldman, K. A. (1971). Using the work of others: Some observations on reviewing and | a
_ differences in conformity research. Journal of Personality and Social Psychology, 37, ___ integrating. Sociology of Education, 44, 86-102. | | :
_ 131-146. __ Hienberg, S. E. (1977). The analysis of cross-classified categorical data. Cambridge, MA: te
| Cooper, H. M. (1981). On the significance of effects and the effects of significance. Journal __ The MIT Press. ° i | |
_ . of Personality and Social Psychology, 41, 1013-1018. | Fisher, R. A. (1928). Statistical methods for research workers (2nd ed.). London: Oliver & | |
Cooper, H. M. (1982). Scientific guidelines for conducting integrative research review, Boyd. : i :
- Review of Educational Research, 52, 291-302. | Fisher, R. A. (1932). Statistical methods for research workers (4th ed.). London: Oliver & il i EE
Cooper, H. M. (1984). The integrative research review: A social science approach. Bevely __‘~Boyd. il (og
: Hills, CA: Sage. = — Fisher, R. A, (1938). Statistical methods for research workers (7th ed.). London: Oliver & | | co _
Cooper, H, M. (1989a). Homework. NY: Longman.  . Boyd. [| i.
Cooper, H. M. (1989b). Integrating research: A guide to literature reviews (2nd ed), —_ Fiske, D. W. (1978). The several kinds of generalization. The Behavioral and Brain Sciences, | [
_ Newbury Park, CA: Sage. _ 3, 393-394. ; |
a Cooper, H. M., & Hazelrigg, P. (1988). Personality moderators of interpersonal expectancy __ Fiske, D. W. (1983). The meta-analytic revolution in outcome research. Journal of Consulting io og
effects: An integrative research review. Journal of Personality and Social Psychology _ and Clinical Psychology, 51, 65-70. i| | '
55, 937-949, ___ Fleming, E. S., & Anttonen, R. G. (1971). Teacher expectancy as related to the academic and | | do
Cooper, H. M., & Rosenthal, R. (1980). Statistical versus traditional procedures for summa personal growth of primary-age children. Monographs of the Society for Research in i
_ rizing research findings. Psychological Bulletin, 87, 442-449. ___ Child Development, 36(S, Whole No. 145). | [
Coursol, A., & Wagner, E. E. (1986). Effect of positive findings on submission and acces _friedman, H. (1968). Magnitude of experimental effect and a table for its rapid estimation. i
tance rates: A note on meta-analysis bias. Professional Psychology.. Research ad Psychological Bulletin, 70, 245-251. A ,
_ : Practice, 17, 136-137. _ Glass, G. V (1976). Primary, secondary, and meta-analysis of research. Educational Re- i
Dabbs, J. M., Jr., & Morris, R. (1990). Testosterone, social class, and antisocial behavior ia searcher, 5, 3-8. ! | i
_ a sample of 4,462 men, Psychological Science, 1, 209-211. | wo G. V (1977). Integrating findings: The meta-analysis of research. Review of Research i |
DePaulo, B. M., & Rosenthal, R. (1979). Ambivalence, discrepancy, and deception in in Education, 5, 351-379. :
_ nonverbal communication. In R. Rosenthal (Ed.), Skill in nonverbal communication Glass, G. V (1978). In defense of generalization. The Behavioral and Brain Sciences, 3, ie :
(pp. 204-248). Cambridge, MA: Oelgeschlager, Gunn & Hain. __ 394-395. | |
_ DePaulo, B. M., Zuckerman, M., & Rosenthal, R. (1980). Detecting deception: Modally | Glass, G. V (1980). Summarizing effect sizes. In R. Rosenthal (Ed.), New directions for i |
. : effects, In L. Wheeler (Ed.), Review of personality and social psychology. Beverly tls methodology of social and behavioral science: Quantitative assessment of research ii ;
_ CA: Sage. _ domains. San Francisco: Jossey-Bass. | i |

----- Page 77 (ocr) -----
SS 140 META-ANALYTIC PROCEDURES _gpFERENCES 141 i

o Glass, G. V, & Kliegl, R. M. (1983). An apology for research integration in the study or | gates L. V., & Olkin, I. (1983b). Regression models in research synthesis. The American |

psychotherapy. Journal of Consulting and Clinical Psychology, 51, 28-41. Statistician, 37, 137-140. |

Glass, G. V, McGaw, B., & Smith, M. L. (1981). Meta-analysis in social research, Beverly edges. L. V., & Olkin, I. (1985). Statistical methods for meta-analysis. New York: Academic LC

Hills, CA: Sage. press. [

Glass, T. R. (1971), Experimenter effects in a measure of intelligence. Unpublished manu. | gvely, W. (1989). Cold fusion confirmed. Science Observer, July-August, 327. i

script, Fairleigh Dickinson University. _ poaglin, D. C., Mosteller, F., & Tukey, J. W. (Eds.). (1983). Understanding robust and |
_ Goldberg, M. (1978). Acoustical factors in the perception of stress. Unpublished manuscripy, _aploratory data analysis. New York: John Wiley. LE |
Harvard University, Cambridge. co on orton, C. (1985). Meta-analysis of psi Ganzfeld research: A response to Hyman. Journal | |

Green, B. F., & Hall, J. A. (1984). Quantitative methods for literature reviews. AnnualReviey __ of Parapsychology, 49, 51-91. 2
of Psychology, 35, 37-53. dowland, C. W. (1970). The influence of knowledge of results and experimenter and subject Le /
_ Guilford, J. P. (1954). Psychometric methods (2nd ed.). New York: McGraw-Hill, ___ personality styles upon the expectancy effect. Masters thesis, University of Wisconsin. a
Guilford, J. P., & Fruchter, B. (1978). Fundamental statistics in psychology and education sts L. M. (1980). Tests of differences in p levels as tests of differences in effect sizes. i i
_ (6th ed.). New York: McGraw-Hill. ___ Psychological Bulletin, 88, 705-708. i

Hall, J. A. (1979). Gender, gender roles, and nonverbal communication skills. In R. Rosenthal _ unter, J. E., & Schmidt, F. L. (1990). Methods of meta-analysis: Correcting error and bias |
(Ed.), Skill in nonverbal communication: Individual differences (pp. 32-67). Cambridge, sit research findings. Newbury Park, CA: Sage. it

MA: Oelgeschlager, Gunn & Hain. _ saunter, J.E., Schmidt, F. L., & Jackson, G. B. (1982). Meta-analysis: Cumulating research i : |
Hall, J. A. (1980). Gender differences in nonverbal communication skills, In R. Rosenthal __ findings across studies. Beverly Hills, CA: Sage. [ |

(Ed.), New directions for methodology of social and behavioral science: Quantitative _fyde, J. S. (1980). How large are cognitive gender differences? A meta-analysis using w? i
assessment of research domains (pp. 63-77). San Francisco: Jossey-Bass. | __andd. American Psychologist, 36, 892-901: a
Hall, J. A. (1984). Nonverbal sex differences. Baltimore, MD: The Johns Hopkins University “Hyman, R. (1985). The Ganzfeld psi experiment: A critical appraisal. Journal of Parapsy- L
_ Press. chology, 49, 3-49. I
Harris, M. J., & Rosenthal, R. (1985). Mediation of interpersonal expectancy effects: 1 _ jackson, G. B. (1978). Methods for reviewing and integrating research in the social sciences. iL

oo meta-analyses. Psychological Bulletin, 97, 363-386. __sNSF Report for Grant DIS 76-20398). Washington, DC: National Science Foundation. a
- Harris, M. J., & Rosenthal, R. (1988). Human performance research: An overview, Back __(NTIs No. PB-283-747). ° .

ground paper commissioned by the National Research Council, Washington, DC: Na _ Jacob, T. (1969). The emergence and mediation of the experimenter-bias effect as a function Ll

_ tional Academy Press. oa “demand characteristics,” experimenter “investment” and the nature of the experi- L
Hawthorne, J. W. (1972). The influence of the set and dependence of the data collector on mental task. Unpublished manuscript, University of Nebraska. ie

i the experimenter bias effect. Doctoral dissertation, Duke University. _ Johnson, H. G. (1944). An empirical study of the influence of errors of measurement upon re
_ Hedges, L. V, (1981). Distribution theory for Glass’s estimator of effect size and related __ correlation. American Journal of Psychology, 57, 521-536. | ia
estimators. Journal of Educational Statistics, 6, 107-128. __hohnson, R. W., & Adair, J. G. (1970). The effects of systematic recording error vs. ex-

oe Hedges, L. V. (1982a). Estimation of effect size from a series of independent experiments, ___ perimenter bias on latency of word association. Journal of Experimental Research in /

i : Psychological Bulletin, 92, 490-499. _ ___ Personality, 4, 270-275. : | :
i Hedges, L. V. (1982b). Fitting categorical models to effect sizes from a series of experiments, _Johnson, R. W., & Adair, J. G. (1972). Experimenter expectancy vs. systematic recording it

i : Journal of Educational Statistics, 7, 119-137. : ss arror under automated and nonautomated stimulus presentation. Journal of Experimental I

_. : Hedges, L. V. (1982c). Fitting continuous models to effect size data. Journal of Educational __‘Research in Personality, 6, 88-94. i

Lo : Statistics, 7, 245-270. _fohnson, R. W., & Ryan, B. J. (1976). Observer recorder error as affected by different tasks ce
Hedges, L. V. (1983a). Combining independent estimators in research synthesis. British _and different expectancy inducements. Journal of Research in Personality, 10, 201-214. |
Journal of Mathematical and Statistical Psychology, 36(1), 123-131. _lhnes, L. V., & Fiske, D. W. (1953). Models for testing the significance of combined results. a
Hedges, L. V. (1983b). A random effects model for effect sizes. Psychological Bulletin, 93, ___ Psychological Bulletin, 50, 375-382. i

LC 388-395. __ lung, J. (1978). Self-negating functions of self-fulfilling prophecies. The Behavioral and L

Hedges, L. V. (1987). How hard is hard science, how soft is soft science? American ‘Brain Sciences, 3, 397-398. il

oo Psychologist, 42, 443-455. _ _ Kaplan, A. (1964). The conduct of inquiry: Methodology for behavioral science. Scranton, i / |
Co Hedges, L. V., & Olkin, I. (1980), Vote counting methods in research synthesis. Psychological TA Chandler. a |
_ Bulletin, 88, 359-369. : _ Kennedy, J.L., & Uphoff, H. F. (1939). Experiments on the nature of extrasensory perception: | |
rT Hedges, L. V., & Olkin, I. (1982). Analyses, reanalyses, and meta-analysis. Contemporary ___ Ill The recording error criticism of extra-chance scores. Journal of Parapsychology, 3, al |
Education Review, 1, 157-165. | 6-245. i |
_. Hedges, L. V., & Otkin, I, (1983a). Clustering estimates of effect magnitude from indepen’ _—_Kolata, G. B, (1981). Drug found to help heart attack survivors. Science, 214, 774-775. a
_ dent studies. Psychological Bulletin, 93, 563-573. _ Aoshland, D.E., Jr. (1989). The confusion profusion. Science, 244, 753. | i

----- Page 78 (ocr) -----
- 142 META-ANALYTIC PROCEDURES REFERENCES 143 |

_ Kraemer, H. C., & Andrews, G. (1982). A nonparametric technique for meta-analysis effect - pearson, K. (1933a). Appendix to Dr. Elderton’s paper on “The Lanarkshire milk experi- i

o size calculation. Psychological Bulletin, 91, 404-412. | ment.” Annals of Eugenics, 5, 337-338. il

_ Krauth, J. (1983). Nonparametric effect size estimation: A comment on Kraemer ang __ pearson, K. (1933b). On a method of determining whether a sample of size n supposed to

| Andrews. Psychological Bulletin, 94, 190-192. i have been drawn from a parent population having a known probability integral has iL

L Kulik, J. A., Kulik, C. C., & Cohen, P. A. (1979). A meta-analysis of outcome studies of probably been drawn at random. Biometrika, 25, 379-410. al
Keller’s Personalized System of Instruction. American Psychologist, 34, 307-318, L persinger, G. W., Knutson, C., & Rosenthal, R. (1968). Communication of interpersonal i |

. Lamb, W. K., & Whitla, D. K. (1983). Meta-analysis and the integration of research findings: expectations of ward personnel to neuropsychiatric patients. Unpublished data, Harvard | iW i

A trend analysis and bibliography prior to 1983. Unpublished manuscript, Harvard University. a

: University, Cambridge. | Pillemer, D. B., & Light, R. J. (1980a). Benefiting from variation in study outcomes. In A ;
_ Lancaster, H. O. (1961). The combination of probabilities: An application of orthonormal R. Rosenthal (Ed.), New directions for methodology of social and behavioral science: (a
_ functions. Australian Journal of Statistics, 3, 20-33. | Quantitative assessment of research domains (pp. 1-11). San Francisco: Jossey-Bass. lo

/ Landman, J. T., & Dawes, R. M. (1982). Psychotherapy outcome: Smith and Glass’ conclu. | __ Pillemer, D. B., & Light, R. J. (1980b). Synthesizing outcomes: How to use research evidence i
sions stand up under scrutiny. American Psychologist, 37, 504-516. . from many studies. Harvard Educational Review, 50, 176-195. L ;
a Lewin, L. M., & Wakefield, J. A., Jr. (1979). Percentage agreement and phi: A conversion | Pool, R. (1988). Similar experiments, dissimilar results. Science, 242, 192-193. i

_ table. Journal of Applied Behavior Analysis, 12, 299-301. | Pool, R. (1989). Will new evidence support cold fusion? Science, 246, 206. | ie
Light, R. J. (1979). Capitalizing on variation: How conflicting research findings. can be Prioleau, L., Murdock, M., & Brody, N. (1983). An analysis of psychotherapy vs. placebo a
helpful for policy. Educational Researcher, 8, 7-11. : studies. The Behavioral and Brain Sciences, 6, 275-310. oo
- Light, R. J., & Pillemer, D. B. (1982). Numbers and narrative: Combining their strengths in / Rimland, B. (1979). Death knell for psychotherapy? American Psychologist, 34, 192. |

: research reviews. Harvard Educational Review, 52, 1-26. | Rosenthal, M. (1985). Bibliographic retrieval for the social and behavioral scientist. Re- cs
o Light, R. J., & Pillemer, D. B. (1984). Summing up: The science of reviewing research, | search in Higher Education, 22, 315-333. i
_ Cambridge, MA: Harvard University Press. L Rosenthal, R. (1961, September). On the social psychology of the psychological experiment: i

/ Light, R. J., & Smith, P. V. (1971). Accumulating evidence: Procedures for resolving | With particular reference to experimenter bias. In H. W. Riecken (Chair), On the social | |
_ contradictions among different research studies. Harvard Educational Review, 41, 429. . psychology of the psychological experiment. Symposium conducted at the meeting of the :

: 471. . American Psychological Association, New York. | i

- Lilliefors, H. W. (1967). On the Kolmogorov-Smirnov test for normality with mean and Rosenthal, R. (1963). On the social psychology of the psychological experiment: The yo
3 variance unknown. Journal of the American Statistical Association, 62, 399-402. | experimenter’s hypothesis as unintended determinant of experimental results. American | 1

Lush, J. L. (1931). Predicting gains in feeder cattle and pigs. Journal of Agricultural _ Scientist, 51, 268-283. l |
Research, 42, 853-881. __ Rosenthal, R. (1964). Effects of the experimenter on the results of psychological research. a ;
- : Mann, C. (1990). Meta-analysis in the breech. Science, 249, 476-480. / In B. A. Maher (Ed.), Progress in experimental personality research, Vol. 1 (pp. 79-114). a q
_ : Marvell, T. B. (1979). Personal communication, National Center for State Courts, Williams- . New York: Academic Press. i

_ burg, VA. Rosenthal, R. (1966). Experimenter effects in behavioral research. New York: Appleton- Li

a : Mayo, C. C. (1972). External conditions affecting experimental bias. Doctoral dissertation, / Century-Crofts. li |

_ University of Houston. ___ Rosenthal, R. (1968a). An application of the Kolmogorov-Smirnov test for normality with |
LZ Mayo, R. J. (1978). Statistical considerations in analyzing the results of a collection of | estimated mean and variance. Psychological Reports, 22, 570. ia

S experiments. The Behavioral and Brain Sciences, 3, 400-401. |_| Rosenthal, R. (1968b). Experimenter expectancy and the reassuring nature of the null i -

_ McConnell, R. A., Snowdon, R. J., & Powell, K. F. (1955). Wishing with dice. Journal of . hypothesis decision procedure. Psychological Bulletin Monograph Supplement, 70,
Experimental Psychology, 50, 269-275. 1 30-47. i

_ McNemar, Q. (1960). At random: Sense and nonsense. American Psychologist, 15, 295-300. __ Rosenthal, R. (1969). Interpersonal expectations. In R. Rosenthal and R. L. Rosnow (Eds.), 7

: Mintz, J. (1983). Integrating research evidence: A commentary on meta-analysis. Journalof | Artifact in behavioral research (pp. 181-277). New York: Academic Press. |

_ Consulting and Clinical Psychology, 51, 71-75. . Rosenthal, R. (1976). Experimenter effects in behavioral research. Enlarged edition. New I
2 Mosteller, F. M., & Bush, R. R. (1954). Selected quantitative techniques. In G. Lindzey (Ed.), . York: Irvington. | I
Handbook of social psychology: Vol. 1. Theory and method (pp. 289-334). Cambridge, ___ Rosenthal, R. (1978a). Combining results of independent studies. Psychological Bulletin, 85, il |
_ MA: Addison-Wesley. 7 185-193. |
Mosteller, F, & Rourke, R. E. K. (1973). Sturdy statistics. Reading, MA: Addison-Wesley. Rosenthal, R. (1978b). How often are our numbers wrong? American Psychologist, 33, i |
Mullen, B. (1989). Advanced BASIC meta-analysis. Hillsdale, NJ: Erlbaum. a 1005-1008. L |
_ Mullen, B., & Rosenthal, R. (1985). BASIC meta-analysis: Procedures and programs. _ Rosenthal, R. (1979a). The “file drawer problem” and tolerance for null results. Psycholog- il
|. Hillsdale, NJ: Erlbaum. _ ical Bulletin, 86, 638-641. CL |
Nelson, N., Rosenthal, R., & Rosnow, R. L. (1986). Interpretation of significance levelsand__—| Rosenthal, R. (1979b). Replications and their relative utilities. Replications in Social Psy- |

effect sizes by psychological researchers. American Psychologist, 41, 1299-1301. 7 chology, 1(1), 15-23. |

----- Page 79 (ocr) -----
| 144 META-ANALYTIC PROCEDURES : REFERENCES 145 |
: Rosenthal, R. (Ed.). (1980). New directions for methodology of social and behavioral / Rosenthal, R., & Hall, C. M. (1968). Computational errors in behavioral research. Unpub- |
: science: Quantitative assessment of research domains (No. 5). San Francisco: Jossey- : lished data, Harvard University. |
| Bass. : Rosenthal, R., Hall, J. A., DiMatteo, M. R., Rogers, P. L., & Archer, D. (1979). Sensitivity a
Rosenthal, R. (1982a). Conducting judgment studies. In K. R. Scherer & P. Ekman (Eds.), : to nonverbal communication: The PONS Test. Baltimore: The Johns Hopkins University i |
Handbook of methods in nonverbal behavior research (pp. 387-361). New York: Cam- / Press. | |
bridge University Press. | Rosenthal, R., & Rosnow, R. L. (1975). The volunteer subject. New York: John Wiley. re :
Rosenthal, R. (1982b). Valid interpretation of quantitative research results. In D. Brinberg & Rosenthal, R., & Rosnow, R. L. (1984a). Essentials of behavioral research: Methods and i
L. H. Kidder (Eds.), Forms of validity in research (pp. 59-75). San Francisco: Jossey- data analysis. New York: McGraw-Hill. li
Bass. ' Rosenthal, R., & Rosnow, R. L. (1984b). Understanding behavioral science. New York: i
Rosenthal, R. (1983a). Assessing the statistical and social importance of the effects of 1 McGraw-Hill. |
psychotherapy. Journal of Consulting and Clinical Psychology, 51, 4-13. Rosenthal, R., & Rosnow, R. L. (1985). Contrast analysis: Focused comparisons in the [|
: Rosenthal, R. (1983b). Improving meta-analytic procedures for assessing the effects of | analysis of variance. New York: Cambridge University Press. if
: psychotherapy vs. placebo. The Behavioral and Brain Sciences, 6, 298-299. | Rosenthal, R., & Rosnow, R. L. (1991). Essentials of behavioral research: Methods and data :
Rosenthal, R. (1983c). Meta-analysis: Toward a more cumulative social science. In L. Bick- / analysis (2nd ed.). New York: McGraw-Hill. i
man (Ed.), Applied and social psychology annual (Vol. 4) (pp. 65-93). Beverly Hills, CA: | Rosenthal, R., & Rubin, D. B. (1978a). Interpersonal expectancy effects: The first 345 i
Sage. | studies. The Behavioral and Brain Sciences, 3, 377-386. ik i
| Rosenthal, R. (1983d). Methodological issues in behavioral sciences. In B. B. Wolman (Ed.), Rosenthal, R., & Rubin, D. B. (1978b). Issues in summarizing the first 345 studies of LL
L Progress volume I: International encyclopedia of psychiatry, psychology, psychoanaly- | interpersonal expectancy effects. The Behavioral and Brain Sciences, 3, 410-415. f
sis, & neurology (pp. 273-277). New York: Aesculapius Publishers. Rosenthal, R., & Rubin, D. B. (1979a). Comparing significance levels of independent studies. Wl
Rosenthal, R, (1984). Meta-analytic procedures for social research. Beverly Hills, CA: Sage, | Psychological Bulletin, 86, 1165-1168. ~ a :
: Rosenthal, R. (1985a), Designing, analyzing, interpreting, and summarizing placebo studies. | Rosenthal, R., & Rubin, D. B. (1979b). A note on percent variance explained as a measure |
. In L. White, B. Tursky, & G. E. Schwartz (Eds.), Placebo: Theory, research, and mech- : of the importance of effects. Journal of Applied Social Psychology, 9, 395-396. Al
: anisms (pp. 110-136). NY: Guilford. | Rosenthal, R., & Rubin, D. B. (1980a). Further issues in summarizing 345 studies of : | l
. - Rosenthal, R. (1985b). From unconscious experimenter bias to teacher expectancy effects. interpersonal expectancy effects. The Behavioral and Brain Sciences, 3, 475-476. i:
_ In J. B. Dusek, V. C. Hall, & W. J. Meyer (Eds.), Teacher expectancies (pp. 37-65). Rosenthal, R., & Rubin, D. B. (1980b). Summarizing 345 studies of interpersonal expectancy :
. : Hillsdale, NJ: Lawrence J. Erlbaum. : effects. In R. Rosenthal (Ed.), New directions for methodology of social and behavioral D
. Rosenthal, R. (1986). Meta-analytic procedures and the nature of replication: The Ganzfeld : science: Quantitative assessment of research domains (pp. 79-95). San Francisco: Jossey- - ;
debate. Journal of Parapsychology, 50, 315-336. ' Bass. a
Rosenthal, R. (1987a). Judgment studies: Design, analysis and meta-analysis. Cambridge, Rosenthal, R., & Rubin, D. B. (1982a). Comparing effect sizes of independent studies. / |
_ England: Cambridge University Press. | Psychological Bulletin, 92, 500-504. L
Rosenthal, R. (1987b). Pygmalion effects: Existence, magnitude, and social importance. Rosenthal, R., & Rubin, D. B. (1982b). Further meta-analytic procedures for assessing /
Educational Researcher, 16, 37-41. | cognitive gender differences. Journal of Educational Psychology, 74, 708-712. 7
Rosenthal, R. (1990a). How are we doing in soft psychology? American Psychologist, 45, Rosenthal, R., & Rubin, D. B. (1982c). A simple, general purpose display of magnitude of L
775-777. ; experimental effect. Journal of Educational Psychology, 74, 166-169. i
Rosenthal, R. (1990b). Replication in behavioral research, Journal of Social Behavior and __ Rosenthal, R., & Rubin, D. B. (1983). Ensemble-adjusted p values. Psychological Bulletin, |
Personality, 5, 1-30. : 94, 540-541. i
_ Rosenthal, R. (1990). Evaluation of procedures and results. In K. W. Wachter & M.L. Straf Rosenthal, R., & Rubin, D. B. (1984). Multiple contrasts and ordered Bonferroni procedures. |
(Eds.), The future of meta-analysis. NY: Russell Sage Foundation. | Journal of Educational Psychology, 76, 1028-1034. i:
Rosenthal, R., & DePaulo, B. M. (1979). Sex differences in accommodation in nonverbal Rosenthal, R., & Rubin, D. B. (1986). Meta-analytic procedures for combining studies with i
_ communication. In R. Rosenthal (Ed.), Skill in nonverbal communication: Individual __ multiple effect sizes. Psychological Bulletin, 99, 400-406. i
differences (pp. 68-103). Cambridge, MA: Oelgeschlager, Gunn & Hain. | Rosenthal, R., & Rubin, D. B. (1988). Comment: Assumptions and procedures in the file |
Rosenthal, R., Friedman, C. J., Johnson, C. A., Fode, K. L., Schill, T. R., White, C.R.,& | drawer problem. Statistical Science, 3, 120-125. ||
_ Vikan-Kline, L. L. (1964). Variables affecting experimenter bias in a group situation. Rosenthal, R., & Rubin, D. B. (1989). Effect size estimation for one-sample multiple-choice- |
Genetic Psychology Monographs, 70, 271-296. : type data: Design, analysis, and meta-analysis. Psychological Bulletin, 106, 332-337. is
Rosenthal, R., & Gaito, J. (1963). The interpretation of levels of significance by psycholog- | Rosenthal, R., & Rubin, D. B. (1991). Further issues in effect size estimation for one-sample |
ical researchers, Journal of Psychology, 55, 33-38. : multiple-choice-type data. Psychological Bulletin, 109, 351-352. a
Rosenthal, R., & Gaito, J. (1964). Further evidence for the cliff effect in the interpretation _ Rosnow, R. L. (1981). Paradigms in transition. New York: Oxford University Press.
of levels of significance. Psychological Reports, 15, 570. Rosnow, R. L., & Rosenthal, R. (1989). Statistical procedures and the justification of |
_ / knowledge in psychological science. American Psychologist, 44, 1276-1284.
; _ ‘ ia

----- Page 80 (ocr) -----
_ 146 META-ANALYTIC PROCEDURES | REFERENCES 147 |
Rusch, F. R., Greenwood, C. R., & Walker, H. M. (1978). The effects of complexity, time and _ Stock, W. A., Okun, M.A., Haring, M. J., Miller, W., Kinney, C., & Ceurvorst, R. W. (1982). al
feedback upon experimenter calculation errors. Unpublished manuscript, University of / Rigor in data synthesis: A case study of reliability in meta-analysis. Educational Re- |

_ Illinois, Urbana~Champaign. / searcher, 11, 10-14. |

_ Rusch, F. R., Walker, H. M., & Greenwood, C. R. (1974). A systematic analysis of experi- / Stouffer, S. A., Suchman, E. A., DeVinney, L. C., Star, S. A., & Williams, R. M., Jr. (1949). ||
menter error responses in the calculation of observation data, Unpublished manuscript, : The American soldier: Adjustment during army life, Vol. I. Princeton, NJ: Princeton | :
University of Oregon, Eugene. | University Press. |
Shadish, W. R., Jr., Doherty, M., & Montgomery, L. M. (1989). How many studies are in the __ Strube, M. J. (1985). Combining and comparing significance levels from nonindependent i
file drawer? An estimate from the family/marital psychotherapy literature. Clinical : hypothesis tests. Psychological Bulletin, 97, 334-341. il :
Psychology Review, 9, 589-603. |. Strube, M. J., Gardner, W., Hartmann, D. P. (1985). Limitations, liabilities, and obstacles in i,
Shapiro, D. A., & Shapiro, D. (1983). Comparative therapy outcome research: Methodolog- reviews of the literature: The current status of meta-analysis. Clinical Psychology Review, |

ical implications of meta-analysis. Journal of Consulting and Clinical Psychology, 51, : 5, 63-78. |
42-53. | Strube, M. J., & Hartmann, D. P. (1983). Meta-analysis: Techniques, applications, and | |
_ Shoham-Salomon, V., & Rosenthal, R. (1987). Paradoxical interventions: A meta-analysis, i functions. Journal of Consulting and Clinical Psychology, 51, 14-27. it
Journal of Consulting and Clinical Psychology, 55, 22-28. : Sudman, S., & Bradburn, N. M. (1974). Response effects in surveys. A review and synthesis. |
Siegel, S. (1956), Nonparametric statistics. New York: McGraw-Hill. | Chicago: Aldine, Hf
_ Siegel, S., & Castellan, N. J., Jr. (1988). Nonparametric statistics for the behavioral sciences / Taubes, G. (1990). Cold fusion conundrum at Texas A & M. Science, 248, 1299-1304. : i
(2nd ed.). New York: McGraw-Hill. | Taveggia, T. C. (1974). Resolving research controversy through empirical cumulation: a /
Simes, R. J. (1987). Confronting publication bias: A cohort design for meta-analysis. : Toward reliable sociological knowledge. Sociological Methods & Research, 2, 395-407. |
Statistics in Medicine, 6, 11-29. __ Thorndike, R. L. (1933). The effect of the interval between test and retest on the constancy |
Simpson, E. H. (1951). The interpretation of interaction in contingency tables. Journal of the | of the IQ. Journal of Educational Psychology, 24, 543-549. | 7 i
_ Royal Statistical Society, B, 13, 238-241. | Tippett, L. H. C, (1931). The methods of statistics. London: Williams & Norgate. | |
Smart, R. G. (1964). The importance of negative results in psychological research. Canadian Tobias, P. (1978). Personal communication. Los Angeles: IBM.
Psychologist, 5a, 225-232. | Todd, J. L. (1971). Social evaluation orientation, task orientation, and deliberate cuing in |

oo : Smith, M. L. (1980). Integrating studies of psychotherapy outcomes, In R. Rosenthal (Ed.), | experimenter bias effect. Doctoral dissertation, University of California, Los Angeles. | |
New directions for methodology of social and behavioral science: Quantitative assess- | Tukey, J. W. (1977). Exploratory data analysis. Reading, MA: Addison-Wesley. i |
ment of research domains (pp. 47-61). San Francisco: Jossey-Bass. : Underwood, B. J. (1957). Interference and forgetting. Psychological Review, 64, 49-60. | :
a Smith, M. L., & Glass, G. V (1977). Meta-analysis of psychotherapy outcome studies. | Upton, G. J. G. (1978). The analysis of cross-tabulated data. New York: John Wiley. |
American Psychologist, 32, 752-760. | Viana, M. A. G. (1980). Statistical methods for summarizing independent correlational |
_ Smith, M. L., Glass, G. V, & Miller, T. J. (1980). The benefits of psychotherapy. Baltimore: ' results. Journal of Educational Statistics, 5, 83-104. yo
Johns Hopkins University Press. Wachter, K. W., & Straf, M. L. (Eds.) (1990). The future of meta-analysis. New York: Russel] i |
Snedecor, G. W. (1946). Statistical methods (4th ed.). Ames: Iowa State College Press. Sage. |
Snedecor, G. W., & Cochran, W. G. (1967). Statistical methods (6th ed.). Ames, Iowa State ' Wakefield, J. A., Jr. (1980). Relationship between two expressions of reliability: Percentage a
University Press. : agreement and phi. Educational and Psychological Measurement, 40, 593-597. a
Snedecor, G. W., & Cochran, W. G. (1980). Statistical methods (7th ed.). Ames: Towa State Walberg, H. J., & Haertel, E. H. (Eds.). (1980). Research integration: The state of the art. il
University Press. : Evaluation in Education, 4, Whole Number 1. |

_ Snedecor, G. W., & Cochran, W. G. (1989). Statistical methods (8th ed.). Ames: Iowa State | Walker, H. M., & Lev, J. (1953). Statistical inference. New York: Holt, Rinehart & Winston. L ;
_ University Press. 1 Wallace, D. L. (1959). Bounds on normal approximations to Student’s and the chi-square i
Sommer, B. (1987). The file drawer effect and publication rates in menstrual cycle research. | distributions. Annals of Mathematical Statistics, 30, 1121-1130. ;
Psychology of Women Quarterly, 11, 233-241. Weiss, L. R. (1967), Experimenter bias as a function of stimulus ambiguity. Unpublished Es :
oo Spearman, C. (1910). Correlation calculated from faulty data. British Journal of Psychology, | manuscript, State University of New York at Buffalo. |

3, 271-295. | Welkowitz, J., Ewen, R. B., & Cohen, J. (1982). Introductory statistics for the behavioral |
Steering Committee of the Physicians Health Study Research Group. (1988). Preliminary | sciences (3rd ed.). New York: Academic Press. : |
report: Findings from the aspirin component of the ongoing physicians’ health study. The ___ Wilkinson, B. (1951). A statistical consideration in psychological research. Psychological i |
_ New England Journal of Medicine, 318, 262-264. Bulletin, 48, 156-158. | |
_ Sterling, T. D. (1959). Publication decisions and their possible effects on inferences drawn Wilson, G. T., & Rachman, S. J. (1983). Meta-analysis and the evaluation of psychotherapy [ / |
2 from tests of significance -- or vice versa. Journal of the American Statistical Association, outcome: Limitations and liabilities. Journal of Consulting and Clinical Psychology, 51, || |
_ 54, 30-34. 54-64. | |
_ Winer, B. J. (1971). Statistical principles in experimental design (2nd ed.). New York:
_ McGraw-Hill. :
_ oO i

----- Page 81 (ocr) -----
148 META-ANALYTIC PROCEDURES ia
_ Wolf, F. M. (1986). Meta-analysis: Quantitative methods for research synthesis. QASS series |
07-059. Beverly Hills, CA: Sage. i
Wolins, L. (1962). Responsibility for raw data. American Psychologist, 17, 657-658. i
_ Yule, G. U. (1903). Notes on the theory of association of attributes in statistics. Biometrika, : i
2, 121-134. _
Zelen, M., & Joel, L. S. (1959). The weighted compounding of two independent significance : i |
tests. Annals of Mathematical Statistics, 30, 885-895. | |
Zuckerman, M., DePaulo, B. M., & Rosenthal, R. (1981). Verbal and nonverbal communi- | INDEX i |
cation of deception. In L. Berkowitz (Ed.), Advances in Experimental Social Psychology /
- (Vol. 14, pp. 1-59). New York: Academic Press. / i

_ Abstracting research results, 41-44 Brody, Nathan, 120, 143 | |
| Adair, John G., 48, 141 Brown, William, 52 bo
| Adcock, C. J., 92-93, 137 Brozek, Josef, 94, 137 |
_ / Adding logs, 90, 92, 98 Bush, Robert R., 7, 68-69, 85, 89, 92-94, L
_ | Adding probabilities, 90, 92, 98 142 a
_ Adding ts, 90, 92, 98 im
_ | Adding weighted zs, 90-91, 93, 98 ;

| Adding zs, 68-70, 90, 97-99, 101-103 Canadian Multicentre Transplant Study Hi

_ _ Aggregated analysis, 5, 8-10 Group, 137 .

_ Alexander, R. A., 63, 137 Carli, Linda, 11, 84, 139 i
_ _ American Psychological Association, 102, Castellan, N. J., Jr, 112, 119, 146 /

| 137 “ Ceurvorst, Robert W., 44, 147 i
| Alcohol treatment programs (in example), Chalmers, T. C., 107, 137 i
_ | 26-27 Chan, S. S. 107, 137 ia
/ Analysis of variance, 54-55 Chi-square (°) method, 6, 94-95, 102-103 i
| Andrews, Gavin, 34, 142 Cochran, William G., 6, 16, 21, 50, 63, 71, 1 ;
_ _ Anttonen, Ralph G., 48, 139 73, 87, 93, 95, 137-138, 146 ‘ae
__ Archer, Dane, 111, 145 Cohen, Jacob, 16, 18-20, 34, 59, 132, 138, 7
_ Amor, David J., 57, 137 147 ia
| Aspirin study (in example), 135 Cohen, Peter A., 11, 142
_ | Atkinson, D. R., 107, 137 Cohen’s d, 16-21, 66-67, 116-117 /
_ ' Collins, H. M., 4, 138 Lo
_ | Combining effect sizes
_ _ Bakan, David, 103, 137 and blocking, 114-116 ee
| Barnes, D. M., 137 and confidence levels, 116-117 i
BESD (binomial effect size display), 17- and significance levels, 111-114 |

: 18, 132-136 Combining studies, 60 a

_ | Birch, M. W., 100, 137 effect size estimation in, 70-72, 87-88 |

___ Birnbaum, A. 97, 137 and significance testing, 68-70, 85-87 a

; / Comparing effect sizes, 117-123 il

- ] Comparing studies 1

i __ Blackmore, S., 107, 137 and effect size estimation, 60, 63-68, 73- A

__ Blocking, 95-96, 98, 114-116 79, 80-85 |

- Bloom, Benjamin S., 11, 137 and significance testing, 60-63, 73, 79- i

_ __ Blyth, Colin R., 100, 137 80 |

J Borodkin, L. J., 63, 137 Comparing raw data, 99-100 '
7. Bradburn, N. M., 11, 147 Compton, J. William, 48, 138 | :
_ Brehm, Jack W., 117, 137 Cook, Thomas, D., 11, 138 i] :

----- Page 82 (ocr) -----
2 150 META-ANALYTIC PROCEDURES : INDEX 151 |
_ Cooper, Harris M., 4, 10-12, 43, 132-133, Fisher’s zr, 21-24, 63-64, 71-74, 80-81, 87 Hyde, Janet S., 123, 141 : |
_ ag Fiske, Donald W., 11, 51, 94, 139, 141 Hyman, R., 141 Marvell, Thee Redes ve
Correcting research results, 46-50 Fleming, Elyse, 48, 139 Mayo Cly deC 48. 1 2 2 i
Counting methods, 94, 98 Fode, Kermit L., 144 Mayo, Robert 132, 142 i
Coursol, A., 107, 138 Friedman, C. Jack, 144 : Information loss, 128-129 McConnell, R. A. 48, 142 i
Cox, Gertrude M., 95 Friedman, Herbert, 16, 19, 34, 139 Information sources, 37-41 McGaw, Barry, 4. ‘95 "140 |
_ Fruchter, Benjamin, 11, 140 Interpersonal expectancy research (in McNemar, Quinn, 103 142 |
Furlong, M. J., 107, 137 examples), 42-44, 111-119 Mean result method, 28-29 |
- df, 78-70, 8 oe Mean Zs, 104-109 i
Oo Dabbs, J. M., Jx., : : I
Dawes, Robyn M., 120, 142 g, 65-67, 74-78, 81-83, 85 : Jackson, Gregg B., 4, 24, 141 Milles Thomee hn wote ag i
_ DePaulo, Bella M., 11, 138, 110,114, 144, Gaito, John, 49, 63, 144 Jacob, Theodore, 48, 141 Miller, Wendy, 44, 147 !
_ 148 Ganzfeld studies, 124-125 / Joel, L. S., 93, 148 Mintz, Jim, 11, 142 I
_ Deception research (in example), 114-115 Gardner, W., 11, 147 / Johnson, Carleton A., 25, 144 Moderator variables 5, 7-10
DeVinney, L. C., 93, 147 Glass, Gene V., 4, 10-11, 25, 27, 37-39, 43- ; Johnson, H. G., 141 Montgomery, L. M "107 4 |
DiMatteo, M. Robin, 111, 145 4, 46, 48,51, 56, 59, 100,127,129, Johnson, Ronald W., 48, 141 Mortis, R138 /
octor, Ronald M., 48, 139 132-133, 139-140, 146 : Jones, Lyle V., 94, 141 fae : |
S Doheny, M., 107, 146 Glass, McGaw, and Smith adjustments, 25- Joseph, Gail, 11, 139 eo 9294 Ae 7,98, 88-69, 85, ho
_ Dusek, Jerome, B., 11, 139 26 : Judges’ ratings reliability, 51-58 : Mullen, Brian "4 11,142. |
Eagly, Alice H., 11, 84, 139 Glass’s A (delta), 18, 21 | Jung, John, 131, 141 Murdock Martha 120 143 |
Edgington, Eugene S., 93, 139 Goldberg, M., 48, 140 . , oe |
Effect sizes, 4-17 Green, Bert F., 140 a. 146 ; | ;
combining, 111-117 Greenwood, Charles R., : Kaplan, Abraham, 11, 141 Nel |
_ computing, 18-19 Guilford, J. P., 14, 25, 55-56, 140 : Kennedy, J.L., 48, 141 , Novertal owe research (in examples), 111 |
displaying, 110-111 Kinney, Clifford, 44, 147 116, 123-124 “ |
indicators, 34-35 Haertal, Edward H., 11, 147 / Kliegi, Reinhold M., 11, 140 |
and quality of research, 56-57 Hall, Clay M., 145 / Knutson, Clifford, 143 L
Effect size estimation Hall, Judith A., 11, 48, 84, 94, 111, 140, | Kolata, Gina B., 134, 141 Okun, Morris A., 44, 147 |
in combining studies, 60, 70-72, 87-88 145 Koshland, D.E., Jn, 4, 141 Olkin, 1., 4, 11, 22, 140-141 |
in comparing studies, 60, 63-68, 73-79, Haring, Marilyn J., 44, 147 : Kosick, Roger R., 139 coe’ ‘| i
80-85 Harris, Monica J., 11, 123, 140 | Kraemer, Helena C., 34, 142
_ Eisner, Donald A., 48, 139 Hermann, Donal pe i. io ah Joachim, 34, 142 Pearson, Karl, 1, 143 I
Elashoff, Janet D., 132, 139 awt orne, ames f., 40, i ullk, Chen Lin C., 11, 142 Pearson’s r, 19-20
Ennis, James G., 48, 139 Hazelrigg, P., 138 Kulik, James A., 11, 142 , Persinger, Gordon W., 48. 143 i
Ensemble adjustment of p method, 30-31 Hedges, Larry V., 4, 11, 21-24, 59, 63, 73- | Phi 19-20 ad L :
Equal N formula, 18 74, 140-141 / Pi (fH), 123-124 i |
Errors, 46-50 Hedge’s adjustment, 21-24 : Lamb, William K., 10, 127, 142 Pillemer, David, 4, 11, 142-143 | |
Estimate, single, 32-33 Hedge’s g, 65-67, 74-78, 81-83, 85 Lancaster, H. O., 93, 142 Placebo research (in exam le), 134-136 |
Ewen, R. B., 147 ; : randman, Janet T., 120, 142 Point biseriat r, 19-20 me i
_ earch (in example), | €v, Joseph, 52, 1 de, |
| Expectancy effects research ( pie) Heterogeneity of method, 129-130 | Leviton, Pee ct 138 PONS (Profile of Nonverbal Sensitivity) I
_ Hively, W., 4, 141 i Lewin, Lewis M., 142 aie (in example), 111-116, '
Hoaglin, David C.C., 58, 141 Light, Richard 1, 4, 11, 142-143 Pool, R., 4, 143 '
Federighi, Enrico T., 61, 102, 139 Homogeneity of quality, 130 Lilliefors, H. W., 111, 142 Pooling, 16 99-101 i
Feldman, K. A., 139 Honorton, Charles, 123, 141 ____ Loss of information, 128-129 Powell, K. F, 142 i
_ Fienberg, Stephen E., 100, 139 Howland, Charles W., 48, 141 | Lush, Jay L., 6, 142 Prioieau Leslie 120, 143 /
File drawer problem, 103-109, 128-129 Hsu, Louis M., 63, 73, 141 _ Probabilities addi ; 90. [ /
_ Fisher, Ronald A., 7, 21, 139 Hunter, John E., 4, 10-11, 24, 127, 133, 14) _ Product moneat cae Ot | |
_ Fisher adjustment, 21-24 Hunter adjustment, 24-25 _ Mann, C., 4, 142 relations, 54 i
re: ft

----- Page 83 (ocr) -----
_ 152 META-ANALYTIC PROCEDURES INDEX |
Psychotherapy research (in examples), 120-  Shadish, W. R., Jr., 107, 146 Thomas, Janet, 139 ; |
- 122 Shapiro, David A., 11, 146 Thorndike, Robert L., 8, 147 Wana James A. Ir, 45, 142, 147
_ Shapiro, Diana, 11, 146 Tiede, Kenneth, 94, 137 alberg, Herbert J., 11, 147 |
Shoham-Salomon, V., 11, 146 7 Tippett, L.H.C., 7, 147 Walker, Helen M., 52, 147 i
_ r, (effect size), 17-20, 78, 80-81, 85 Siegel, Sidney, 92, 112, 119, 146 ‘Tobias, Paul, 48, 147 Walker, Hill M., 146 |
| Rachmaa, S. J., 11, 147 Significance testing _ Todd, Judy L., 48, 147 Wallace, D. L., 102, 147 H
_ Recording errors, 46-50 in combining studies, 60, 68-70, 85-87 : Trimming judges, 58 Wampold, B.E., 107, 137 |
Reliability in comparing studies, 60-63, 73, 79-80 __ Truncating significance levels, 122 Weighted zs, 68-70, 85-87, 90, 93 |
and analysis of variance, 54-56 Simes, R. J., 107, 146 Tukey, John W., 16, 29, 47, 58, 126, 14 Weiss, Lillie R. 48, 147 i
and principal components, 57 Simpson, E. H., 100-101, 146 147 7089 OE 28, 128, 1A, Welkowitz, J., 147 |
_ reporting, 57-58 Simpson’s paradox, 100 Type I or II errors, 20-22 White, C, Raymond, 144 |
and retrieval of research results, 44-46 Single composite variable, 31-32 / Whitla, Dean K., 10, 127, 142 i
_ split sample, 58 Single estimate, 32-33 / Wilkinson, Bryan, 94, 147
Responses within studies, 130 Size of study, 14-16 / Unweighted means, 96-97 Williams, R. M., Jt., 93, 147 i
_ Research results Small effects, 4-5 _ Underwood, Benton J., 8, 147 Wilson, G. Terrance, 11, 147
abstracting, 41-43 Smart, R. G., 104, 146 __ Uphoff, H. F, 48, 141 , Winer, B., 92, 147 |
correcting, 46-50 Smith, Mary Lee, 4, 10-11, 25, 27, 84, 120, __ Upton, Graham, J. G., 100, 147 Winer method, 92-93
evaluating quality of, 50-51 127, 132-133, 140, 146 | a Wolf, F. M., 11, 148 |
retrieval, 44-46, 106-108 Smith, Paul V., 11, 142 | __ Wolins, Leroy, 50, 148
rho (Spearman rank correlation), 19-20 Snedecor, George W., 6-7, 16, 21, 50, 63, . Variables, single composite, 31-32 i
| Rimland, Bernard, 132, 143 71, 73, 87, 95, 146 _ Viana, Marios A.G., 11, 147 Yule, G. Udny, 100, 148
_ Rogers, Peter L., 111, 145 Snowden, F. J., 142 __ Vikan-Kline, Linda L., 144 Yule’s paradox, 100-101 :
Rosenthal, MaryLu C., 37, 44, 143 Sommer, B., 107, 146 :
Rosenthal, Robert, 4, 6-8, 10-12, 18, 31- Spearman, Charles, 25, 52, 146 a ’ | /
7 32, 34, 40-42, 45-49, 51, 61, 63-65, Spearman-Brown formula, 52-53, 56 _ Wachter, K. W., 11, 147 Zelen, M., 93, 148 Co
_ 67-69, 73-74, 77, 79-80, 82-84, 94, Spearman rank correlation (rho), 19-20 __ Wagner, E. E., 107, 138 2r, 21-24, 63-64, 71-74, 114-115 i
96, 102-105, 107, 110-111, 113-114, Star, S.A. 93, 147 : Zuckerman, Miron, 11, 114, 138, 148 an
_ 116-118, 120, 122-123, 125, 127-128, Steering Committee of the Physicians |
131-133, 135-136, 138, 140, 142-146, Health Study Research Group, 135, i |
_ 148 146 | L
Rosnow, Ralph L., 3, 10-11, 18, 47, 49, 61, Stem-and-leaf plot, 47-49, 111, 124 \
_ 64, 80, 82, 94, 96, 102-104, 111, 118, Sterling, Theodore D., 104, 146 | ; Co
131-132, 135, 142, 145 Stock, William A., 44, 147 /
Rourke, Robert E.K., 58, 142 Stouffer, Samuel A., 93-94, 147 | :
_ Rubin, Donald B., 11, 31-32, 34, 40-41, Stouffer method, 68-70, 93, 113 ' :
_ 46, 51, 63, 65, 67, 73-74, 7, 79-80, Straf, M. L., #1, 147 | |
82-85, 89, 92, 94, 103-104, 107, 113, Strube, Michael J., 11, 34, 147 :
116-117, 122-123, 125, 127-128, 133, Suchman, B. A., 93, 147 | a
136, 145 Sudman, S., 11, 147 | po
_ Rusch, Frank R., 48, 146 Summarizing relationships, 5-7, 9-10 4
_ Ryan, B.J., 48, 141 |
Task persistence study (as example); 12 _ H
_ Sacks, H. S., 107, 137 Taubes, G., 4, 147 . i
Sampling bias, 128-129 Taveggia, T. C., 11, 147 _ !
_ Schill, Thomas R., 144 Test of significance, 14-15 _ |
Schmidt, Frank L., 4, 10-11, 24, 127, 133, Testing mean p, 91-94, 98 _ ‘
_ 141 Testing mean z, 91, 94, 98 [
Scozzaro, M. J., 63, 137 4 (theta), 57 l
i

----- Page 84 (ocr) -----
_ | ABOUT THE AUTHOR |
_ | Robert Rosenthal received his A.B. (1953) and Ph.D. (1956) in psychology

_ _ from UCLA and is a Diplomate in Clinical Psychology. From 1957 to 1962

he taught at the University of North Dakota where he was Director of the

_ Ph.D. program in clinical psychology. He then went to Harvard University,

_ _ where since 1967 he has been Professor-of Social Psychology. Professor

_ Rosenthal’s research has centered for over 30 years on the role of the

__ self-fulfilling prophecy in everyday life and in laboratory situations. Special

_ interests include the effects of teachers’ expectations on students’ intellectual
and physical performance, the effects of experimenters’ expectations on the
_ _ results of their research, and the.effects of healers’ expectations on their |
_ __ patients’ mental and physical health. His interests include (a) the role of

| nonverbal communication in both the mediation of interpersonal expectancy |

_ __ effects and in the relationship between female and male members of small
_ __ work groups and small social groups, (b) the sources of artifact in behavioral | i
/ research, and (c) various quantitative procedures. In the realm of data 1 |
i analysis, his special interests are in analysis of variance, contrast analysis, | |
_ | and meta-analysis. .
_ Professor Rosenthal is a Fellow of the American Association for the
_ __ Advancement of Science and of the American Psychological Society. With
___K. Fode, he received the 1960 Socio-psychological Prize of the American
___ Association for the Advancement of Science and with L. Jacobson, the First |
Prize Cattell Fund Award of the A.P.A. (1967). He was a Senior Fulbright i
| Scholar in the summer of 1972 and a Guggenheim Fellow (1973-1974). In
____1979, the Massachusetts Psychological Association gave him its Distin- f |
guished Career Contribution Award. In 1988, he received the Donald Camp- |
_ ____ bell Award of the Society for Personality and Social Psychology, and in |
____ 1988-1989 he was a Fellow at the Center for Advanced Study in the i
____ Behavioral Sciences. He has lectured widely in the United States and Canada i
| __as well as in Australia, England, Fiji, France, Germany, Israel, Italy, Papua _
_ _____ New Guinea, and Switzerland. He is the author or coauthor of some three L
__ hundred articles and of many books. 7

----- Page 85 (ocr) -----
po

|

_ CPSIA information can be obtained / ,
_ at www.1CGtesting.com

_ Printed in the USA |

FSOWO01n0942011214